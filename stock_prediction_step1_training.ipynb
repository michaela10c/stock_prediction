{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "7447414a-284c-4a75-b9f5-1346823d764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import json\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ac210-ed57-4af5-bae6-bb18c9052ea3",
   "metadata": {},
   "source": [
    "## Data Preparation & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "688825b2-4404-403d-ab53-24f12cacc07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)  # Set seed for numpy\n",
    "    random.seed(seed)  # Set seed for random\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.manual_seed(seed)  # Set seed for PyTorch CPU\n",
    "    \n",
    "    torch.cuda.manual_seed(seed)  # Set seed for PyTorch GPU\n",
    "    torch.cuda.manual_seed_all(seed)  # Set seed for all GPUs\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior for CUDA\n",
    "    torch.backends.cudnn.benchmark = False  # Disable the auto-tuner for GPUs\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "148e7917-605a-4d39-81ef-ce0b63b886a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hyperparameters\n",
    "with open('configs/configs.json', 'r') as file:\n",
    "    hyperparams = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "31e01947-d480-45a5-9807-dd21fa3a2f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove outliers from data\n",
    "# def remove_outliers(data):\n",
    "#     # Convert data to a numpy array if it isn't already\n",
    "#     data = np.array(data)\n",
    "\n",
    "#     # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "#     Q1 = np.percentile(data, 25)\n",
    "#     Q3 = np.percentile(data, 75)\n",
    "\n",
    "#     # Calculate the IQR (Interquartile Range)\n",
    "#     IQR = Q3 - Q1\n",
    "\n",
    "#     # Define the acceptable range for non-outliers\n",
    "#     lower_bound = Q1 - 1.5 * IQR\n",
    "#     upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "#     # Remove outliers\n",
    "#     filtered_data = data[(data >= lower_bound) & (data <= upper_bound)]\n",
    "    \n",
    "#     return filtered_data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "603c0add-4bc9-4f7d-8a0a-8032167a3519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "def load_data(file):\n",
    "    # Load the data\n",
    "    data = pd.read_csv('data/' + file + '.csv')\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.set_index('Date', inplace=True)\n",
    "\n",
    "    # Round to 2 decimal places\n",
    "    data = data.round(2) \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "6561975f-d416-4dca-85d9-403c628095c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3395, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create sequences\n",
    "def create_sequences(data, input_size, num_steps, val_split=0.2, test_split=0.1):\n",
    "    data = [np.array(data[i * input_size: (i + 1) * input_size]) \n",
    "       for i in range(len(data) // input_size)]\n",
    "\n",
    "    # Normalize\n",
    "    data = [data[0] / data[0][0] - 1.0] + [\n",
    "                curr / data[i][-1] - 1.0 for i, curr in enumerate(data[1:])]\n",
    "\n",
    "    print(np.array(data).shape)\n",
    "    \n",
    "    # Split into groups of `num_steps`\n",
    "    X = np.array([data[i: i + num_steps] for i in range(len(data) - num_steps)])\n",
    "    y = np.array([data[i + num_steps] for i in range(len(data) - num_steps)])\n",
    "\n",
    "    # Reshape X to have shape (N, num_steps, input_size)\n",
    "    X = X.reshape(-1, num_steps, input_size)\n",
    "    y = y.reshape(-1, input_size)  # Reshape y to match the output shape\n",
    "\n",
    "    # Split into train, validation, and test sets\n",
    "    total_len = len(X)\n",
    "    test_start = int(total_len * (1 - test_split))\n",
    "    val_start = int(total_len * (1 - test_split - val_split))\n",
    "    \n",
    "    X_train, X_val, X_test = X[:val_start], X[val_start:test_start], X[test_start:]\n",
    "    y_train, y_val, y_test = y[:val_start], y[val_start:test_start], y[test_start:]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# Extract attributes\n",
    "num_steps = hyperparams['num_steps']  # Extract number of steps\n",
    "input_size = hyperparams['input_size']  # Extract input size\n",
    "val_split = hyperparams['val_split']  # Extract validation split\n",
    "test_split = hyperparams['test_split']  # Extract test split\n",
    "\n",
    "data = load_data('SP500')[['Close']]\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = create_sequences(data, input_size, num_steps, val_split, test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "190c9e4f-4467-4d00-8a41-8d65790b9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "assert(not torch.isnan(X_train_tensor).any())\n",
    "assert(not torch.isnan(y_train_tensor).any())\n",
    "assert(not torch.isnan(X_val_tensor).any())\n",
    "assert(not torch.isnan(y_val_tensor).any())\n",
    "assert(not torch.isnan(X_test_tensor).any())\n",
    "assert(not torch.isnan(y_test_tensor).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "6d3c3d65-dd91-46a8-afcc-d4ffe7808a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader instances\n",
    "batch_size = hyperparams['batch_size']\n",
    "num_workers = hyperparams['num_workers']  # Extract number of workers from hyperparameters\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bed851-6643-4cb9-8df6-5c3cd6217810",
   "metadata": {},
   "source": [
    "## Model Definition: LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "23454ef6-a4e3-4598-9e0a-064e72c00b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type = 'lstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "925a4e78-4ccc-48e1-8fd8-c0da3c79cc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hyperparameters\n",
    "def extract_hyperparams(hyperparams, model):\n",
    "    num_units = hyperparams[model][model + '_units']\n",
    "    num_layers = hyperparams[model][model + '_layers']\n",
    "    dropout_rate = hyperparams[model]['dropout_rate']\n",
    "    dense_units = hyperparams[model]['dense_units']\n",
    "    init_learning_rate = hyperparams[model]['init_learning_rate']\n",
    "    learning_rate_decay = hyperparams[model]['learning_rate_decay']\n",
    "    init_epochs = hyperparams[model]['init_epochs']\n",
    "    max_epochs = hyperparams[model]['max_epochs']\n",
    "    use_early_stop = hyperparams[model]['use_early_stop']\n",
    "    early_stop_patience = hyperparams[model]['early_stop_patience']\n",
    "    train_needed = hyperparams[model]['pretrain'] # Whether to train the model\n",
    "\n",
    "    return num_units, num_layers, dropout_rate, dense_units, init_learning_rate, learning_rate_decay, init_epochs, max_epochs, use_early_stop, early_stop_patience, train_needed\n",
    "\n",
    "num_units, num_layers, dropout_rate, dense_units, init_learning_rate, learning_rate_decay, init_epochs, max_epochs, use_early_stop, early_stop_patience, train_needed = extract_hyperparams(hyperparams, cell_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "7bed0f7d-0308-41c6-9a97-44754772156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CustomLSTMCell, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W_i = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_i = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_i = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "\n",
    "        self.W_f = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_f = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_f = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        self.W_c = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_c = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_c = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        self.W_o = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_o = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_o = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        # Layer normalization layers\n",
    "        self.ln_i = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_f = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_c = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_o = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'W_' in name or 'U_' in name:\n",
    "                nn.init.orthogonal_(param)  # Use orthogonal initialization\n",
    "            elif 'b_' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "            # Check for NaN values\n",
    "            assert(not torch.isnan(param).any())\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        i_t = torch.sigmoid(self.ln_i(torch.mm(x, self.W_i) + torch.mm(h, self.U_i) + self.b_i))\n",
    "        f_t = torch.sigmoid(self.ln_f(torch.mm(x, self.W_f) + torch.mm(h, self.U_f) + self.b_f))\n",
    "        g_t = torch.tanh(self.ln_c(torch.mm(x, self.W_c) + torch.mm(h, self.U_c) + self.b_c))\n",
    "        o_t = torch.sigmoid(self.ln_o(torch.mm(x, self.W_o) + torch.mm(h, self.U_o) + self.b_o))\n",
    "\n",
    "        c_t = f_t * c + i_t * g_t\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        \n",
    "        assert(not torch.isnan(h_t).any())\n",
    "        assert(not torch.isnan(c_t).any())\n",
    "    \n",
    "        return h_t, c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "39846679-2ac7-4e08-b102-9457b571e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout_rate, dense_units, cell_type='lstm'):\n",
    "        super(CustomRNNModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "        if cell_type == 'lstm':\n",
    "            self.layers = nn.ModuleList([CustomLSTMCell(input_dim, hidden_dim)])\n",
    "            self.layers.extend([CustomLSTMCell(hidden_dim, hidden_dim) for _ in range(num_layers - 1)])\n",
    "        elif cell_type == 'gru':\n",
    "            self.layers = nn.ModuleList([CustomGRUCell(input_dim, hidden_dim)])\n",
    "            self.layers.extend([CustomGRUCell(hidden_dim, hidden_dim) for _ in range(num_layers - 1)])\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported cell type\")\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden_dim, dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        h_t = [torch.zeros(batch_size, self.hidden_dim, device=x.device) for _ in range(self.num_layers)]\n",
    "        c_t = [torch.zeros(batch_size, self.hidden_dim, device=x.device) for _ in range(self.num_layers)]\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]\n",
    "            for layer in range(self.num_layers):\n",
    "                if self.cell_type == 'lstm':\n",
    "                    h_t[layer], c_t[layer] = self.layers[layer](x_t, h_t[layer], c_t[layer])\n",
    "                else:\n",
    "                    h_t[layer] = self.layers[layer](x_t, h_t[layer])\n",
    "                x_t = h_t[layer]\n",
    "\n",
    "        x = self.dropout(x_t)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "8e23b87e-33fd-4eec-a7e0-a52898067633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss definitions \n",
    "def quantile_loss(outputs, targets, quantile=0.5):\n",
    "    errors = targets - outputs\n",
    "    loss = torch.max((quantile - 1) * errors, quantile * errors)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "class HingeLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HingeLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        return torch.mean(torch.clamp(1 - targets * outputs, min=0))\n",
    "\n",
    "def directional_loss(outputs, targets):\n",
    "    return torch.mean(torch.abs(torch.sign(outputs) - torch.sign(targets)))\n",
    "\n",
    "def choose_loss_function(loss):\n",
    "    if loss == 'huber':  \n",
    "        criterion = nn.SmoothL1Loss() \n",
    "    elif loss == 'mse': \n",
    "        criterion = nn.MSELoss()\n",
    "    elif loss == 'quantile':\n",
    "        criterion = quantile_loss(quantile=quantile)\n",
    "    elif loss == 'hinge':\n",
    "        criterion = HingeLoss()\n",
    "    elif loss == 'directional':\n",
    "        criterion = directional_loss()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported loss function\")\n",
    "\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "5ebe3745-c0a5-4dfe-bb57-a6cb1fd01098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, criterion, init_epochs, num_epochs, init_learning_rate, learning_rate_decay, device, early_stop_patience=None, cell_type='lstm'):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    learning_rates_to_use = [\n",
    "    init_learning_rate * (\n",
    "        learning_rate_decay ** max(float(i + 1 - init_epochs), 0.0)\n",
    "    ) for i in range(num_epochs)]\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        current_lr = learning_rates_to_use[epoch]\n",
    "        optimizer = optim.Adam(model.parameters(), lr=current_lr)\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in tqdm(train_loader):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stop_patience is not None:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                early_stop_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(model.state_dict(), f'models/SP500_{cell_type}_model.pth')\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                if early_stop_counter >= early_stop_patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model, cell_type, device):\n",
    "    model_path = f'models/SP500_{cell_type}_model.pth'\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "448781d8-4161-40bb-a100-1128f3c37436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CustomRNNModel(input_dim=input_size, hidden_dim=num_units, output_dim=input_size, num_layers=num_layers, dropout_rate=dropout_rate, dense_units=dense_units, cell_type=cell_type).to(device)\n",
    "\n",
    "if train_needed:\n",
    "    # loss function\n",
    "    criterion = choose_loss_function('huber')\n",
    "    early_stop_patience = hyperparams[cell_type].get('early_stop_patience', None)\n",
    "    \n",
    "    train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, init_epochs, max_epochs, init_learning_rate, learning_rate_decay, device, early_stop_patience, cell_type=cell_type)\n",
    "    torch.save(model.state_dict(), f'models/SP500_{cell_type}_model.pth')\n",
    "else:\n",
    "    # Load the model\n",
    "    model.load_state_dict(torch.load(f'models/SP500_{cell_type}_model.pth'))\n",
    "    print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "ac35e6ae-5211-4b18-999a-d89c75214404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomRNNModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): CustomLSTMCell(\n",
       "      (ln_i): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_c): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_o): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc1): Linear(in_features=64, out_features=25, bias=True)\n",
       "  (fc2): Linear(in_features=25, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39e9f9a-5242-4951-9c3e-33815964d178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "03b1339e-a298-4d89-a218-96c05dfb9cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.002187528880313039\n"
     ]
    }
   ],
   "source": [
    "# Function to make predictions\n",
    "def make_predictions(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    criterion = choose_loss_function('huber')\n",
    "    test_loss = 0 \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(y_batch.cpu().numpy())\n",
    "            test_loss += criterion(outputs, y_batch)\n",
    "    return np.array(predictions), np.array(actuals), test_loss.item()\n",
    "\n",
    "# Get predictions and actual values\n",
    "predictions, actuals, test_loss = make_predictions(model, test_loader, device)\n",
    "predictions = predictions.flatten()\n",
    "actuals = actuals.flatten()\n",
    "print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6c42e5-4487-4905-bb2d-22d49ae5a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions against the actual values\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(len(actuals))\n",
    "plt.bar(x - 0.2, actuals, label='Actual Prices')\n",
    "plt.bar(x + 0.2, predictions, label='Predicted Prices')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Normalized Price')\n",
    "plt.ylim(-0.1, 0.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116d8e8f-a5be-43e7-9046-15c4cf78e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overfitting/underfitting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c9f834-cf95-4d96-9bdc-764ec4376e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Global classification rate (raise or fall):\", np.mean((predictions > 0) == (actuals > 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31af3c5-7230-4587-b8fc-32e7b68a7358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "914882a4-fa1c-4fcc-90b2-223425d2ea12",
   "metadata": {},
   "source": [
    "## Model Definition: GRU Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3197c243-0f99-4818-8047-58aacf581c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type = 'gru'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f760a21-86b1-4d94-b3fd-f2b2e1223e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hyperparams\n",
    "num_units, num_layers, dropout_rate, dense_units, init_learning_rate, learning_rate_decay, init_epochs, max_epochs, use_early_stop, early_stop_patience, train_needed = extract_hyperparams(hyperparams, cell_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c98952-122b-4d07-a378-4b6b50441a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGRUCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CustomGRUCell, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W_z = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_z = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_z = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "\n",
    "        self.W_r = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_r = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_r = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        self.W_h = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_h = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_h = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        # Layer normalization layers\n",
    "        self.ln_z = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_r = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_h = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'W_' in name or 'U_' in name:\n",
    "                nn.init.xavier_uniform_(param)  # Use orthogonal initialization\n",
    "            elif 'b_' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "            # Check for NaN values\n",
    "            assert(not torch.isnan(param).any())\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        z_t = torch.sigmoid(self.ln_z(torch.mm(x, self.W_z) + torch.mm(h, self.U_z) + self.b_z))\n",
    "        r_t = torch.sigmoid(self.ln_r(torch.mm(x, self.W_r) + torch.mm(h, self.U_r) + self.b_r))\n",
    "        h_hat_t = torch.tanh(self.ln_h(torch.mm(x, self.W_h) + torch.mm(r_t * h, self.U_h) + self.b_h))\n",
    "\n",
    "        h_t = (1 - z_t) * h + z_t * h_hat_t\n",
    "        \n",
    "        assert(not torch.isnan(h_t).any())\n",
    "    \n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8857f191-02f8-490e-aa3f-46fdc8c24a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CustomRNNModel(input_dim=input_size, hidden_dim=num_units, output_dim=input_size, num_layers=num_layers, dropout_rate=dropout_rate, dense_units=dense_units, cell_type=cell_type).to(device)\n",
    "\n",
    "if train_needed:\n",
    "    # loss function\n",
    "    criterion = choose_loss_function('huber')\n",
    "    early_stop_patience = hyperparams[cell_type].get('early_stop_patience', None)\n",
    "    \n",
    "    train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, init_epochs, max_epochs, init_learning_rate, learning_rate_decay, device, early_stop_patience, cell_type=cell_type)\n",
    "    torch.save(model.state_dict(), f'models/SP500_{cell_type}_model.pth')\n",
    "else:\n",
    "    # Load the model\n",
    "    model.load_state_dict(torch.load(f'models/SP500_{cell_type}_model.pth'))\n",
    "    print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290dcc9e-ecd3-4ebd-80ed-efe4a92f3afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaad883-90fd-49f2-b7fd-57f7510cbbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions and actual values\n",
    "predictions, actuals, test_loss = make_predictions(model, test_loader, device)\n",
    "predictions = predictions.flatten()\n",
    "actuals = actuals.flatten()\n",
    "print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ac756a-1ce0-4846-999c-165ab5c283bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions against the actual values\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(len(actuals))\n",
    "plt.bar(x - 0.2, actuals, label='Actual Prices')\n",
    "plt.bar(x + 0.2, predictions, label='Predicted Prices')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Normalized Price')\n",
    "plt.ylim(-0.1, 0.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb49e11-a485-4c64-a223-f10f321fc59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overfitting/underfitting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0941196-b379-471e-8741-4eea2c85fe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Global classification rate (raise or fall):\", np.mean((predictions > 0) == (actuals > 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3292b5d-e5dc-4e19-ae07-233d37db86d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.special import kl_div\n",
    "\n",
    "# # Sample data for demonstration purposes\n",
    "# # In practice, you would use actual activation data from the model\n",
    "# def get_fp32_activation_data():\n",
    "#     # Replace this with actual data collection\n",
    "#     return np.array([0.5, 1.0, 2.0, -1.0, -0.5, 3.0, -3.0, 0.1])\n",
    "\n",
    "# # **1. Calibration: Collect Data and Compute Scale Factor**\n",
    "\n",
    "# # Collect activation data (replace with actual FP32 data)\n",
    "# activation_data_fp32 = get_fp32_activation_data()\n",
    "\n",
    "# # Step 1: Determine the maximum absolute value\n",
    "# max_abs_value = np.max(np.abs(activation_data_fp32))\n",
    "\n",
    "# # Step 2: Compute the scale factor for symmetric quantization\n",
    "# def compute_scale(max_abs_value):\n",
    "#     return max_abs_value / 127  # 127 because 8-bit quantization uses values from -128 to 127\n",
    "\n",
    "# scale = compute_scale(max_abs_value)\n",
    "\n",
    "# # Step 3: Perform preliminary quantization of FP32 data\n",
    "# def preliminary_quantize(fp32_data, scale):\n",
    "#     return np.clip(np.round(fp32_data / scale), -128, 127).astype(np.int8)\n",
    "\n",
    "# activation_data_int8 = preliminary_quantize(activation_data_fp32, scale)\n",
    "\n",
    "# # Step 4: Compute histograms for FP32 and INT8\n",
    "# def compute_histograms(fp32_data, int8_data):\n",
    "#     hist_fp32, _ = np.histogram(fp32_data, bins=2048, range=(-128, 127), density=True)\n",
    "#     hist_int8, _ = np.histogram(int8_data, bins=2048, range=(-128, 127), density=True)\n",
    "#     return hist_fp32, hist_int8\n",
    "\n",
    "# hist_fp32, hist_int8 = compute_histograms(activation_data_fp32, activation_data_int8)\n",
    "\n",
    "# # Step 5: Compute KL divergence\n",
    "# def compute_kl_divergence(hist_fp32, hist_int8):\n",
    "#     # Adding a small constant to avoid log(0)\n",
    "#     return np.sum(kl_div(hist_fp32 + 1e-8, hist_int8 + 1e-8))\n",
    "\n",
    "# kl_divergence = compute_kl_divergence(hist_fp32, hist_int8)\n",
    "# print(f\"KL Divergence: {kl_divergence}\")\n",
    "\n",
    "# # **2. Quantization: Convert FP32 to INT8**\n",
    "\n",
    "# # Final quantization of FP32 data\n",
    "# def quantize_to_int8(fp32_data, scale):\n",
    "#     return np.clip(np.round(fp32_data / scale), -128, 127).astype(np.int8)\n",
    "\n",
    "# activation_data_int8_final = quantize_to_int8(activation_data_fp32, scale)\n",
    "\n",
    "# # **3. INT32 Computations: Perform Layer Operations**\n",
    "\n",
    "# # Example INT32 computation function\n",
    "# def int32_computations(weights, activations, bias):\n",
    "#     # Perform INT32 matrix multiplication and add bias\n",
    "#     int32_result = np.dot(weights, activations) + bias\n",
    "#     return int32_result\n",
    "\n",
    "# # Sample weights and bias for demonstration\n",
    "# weights = np.array([[1, -1], [2, 3]])\n",
    "# bias = np.array([1, -1])\n",
    "\n",
    "# # Perform INT32 computations\n",
    "# int32_result = int32_computations(weights, activation_data_int8_final, bias)\n",
    "# print(f\"INT32 Computation Result: {int32_result}\")\n",
    "\n",
    "# # **4. Re-Quantization: Convert INT32 to INT8**\n",
    "\n",
    "# # Re-quantization process\n",
    "# def requantize(int32_activations, scale, zero_point, bias):\n",
    "#     # Add bias and then requantize\n",
    "#     int32_activations_with_bias = int32_activations + bias\n",
    "#     return np.clip(np.round(int32_activations_with_bias * scale) + zero_point, -128, 127).astype(np.int8)\n",
    "\n",
    "# # Assuming zero_point = 0 for symmetric quantization\n",
    "# zero_point = 0\n",
    "\n",
    "# # Re-quantize INT32 results to INT8\n",
    "# activation_data_int8_requantized = requantize(int32_result, scale, zero_point, bias)\n",
    "# print(f\"Re-Quantized INT8 Data: {activation_data_int8_requantized}\")\n",
    "\n",
    "# # **5. De-Quantization: Convert INT8 Back to FP32**\n",
    "\n",
    "# # De-quantization process\n",
    "# def dequantize_to_fp32(int8_data, scale, zero_point):\n",
    "#     return (int8_data - zero_point) * scale\n",
    "\n",
    "# # Convert INT8 results back to FP32\n",
    "# fp32_reconstructed_data = dequantize_to_fp32(activation_data_int8_requantized, scale, zero_point)\n",
    "# print(f\"De-Quantized FP32 Data: {fp32_reconstructed_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f53dd1-dd61-4de4-83dc-3b468a1634f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
