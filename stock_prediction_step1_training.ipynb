{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7447414a-284c-4a75-b9f5-1346823d764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ac210-ed57-4af5-bae6-bb18c9052ea3",
   "metadata": {},
   "source": [
    "## Data Preparation & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "688825b2-4404-403d-ab53-24f12cacc07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "148e7917-605a-4d39-81ef-ce0b63b886a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "def load_data(file):\n",
    "    # Load the data\n",
    "    data = pd.read_csv('data/' + file + '.csv')\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.set_index('Date', inplace=True)\n",
    "\n",
    "    # round to 2 decimal places\n",
    "    data = data.round(2) \n",
    "\n",
    "    return data\n",
    "\n",
    "data = load_data('SP500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "50b2d188-c818-4359-8b8e-b74361a521c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data (using min-max scaler)\n",
    "def normalize(features):\n",
    "    min_val = np.min(features)\n",
    "    max_val = np.max(features)\n",
    "    return (features - min_val) / (max_val - min_val)\n",
    "    \n",
    "features = np.array(data[['Close']])\n",
    "scaled_data = normalize(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "31e01947-d480-45a5-9807-dd21fa3a2f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hyperparameters\n",
    "with open('configs/configs.json', 'r') as file:\n",
    "    hyperparams = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1af40767-fc7b-4f98-87e2-1ffce90c6377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences\n",
    "def create_sequences(data, input_size, num_steps):\n",
    "    data = [np.array(data[i * input_size: (i + 1) * input_size]) \n",
    "       for i in range(len(data) // input_size)]\n",
    "\n",
    "    # Split into groups of `num_steps`\n",
    "    X = np.array([data[i: i + num_steps] for i in range(len(data) - num_steps)])\n",
    "    y = np.array([data[i + num_steps] for i in range(len(data) - num_steps)])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "num_steps = hyperparams['num_steps'] # Extract number of steps\n",
    "input_size = hyperparams['input_size'] # Extract number of steps\n",
    "test_split = hyperparams['test_split']\n",
    "X, y = create_sequences(scaled_data, input_size, num_steps)\n",
    "\n",
    "split = int((1 - test_split) * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bed851-6643-4cb9-8df6-5c3cd6217810",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "925a4e78-4ccc-48e1-8fd8-c0da3c79cc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hyperparameters\n",
    "def extract_hyperparams(hyperparams, model):\n",
    "    num_units = hyperparams[model][model + '_units']\n",
    "    num_layers = hyperparams[model][model + '_layers']\n",
    "    dropout_rate = hyperparams[model]['dropout_rate']\n",
    "    dense_units = hyperparams[model]['dense_units']\n",
    "    batch_size = hyperparams[model]['batch_size']\n",
    "    max_epochs = hyperparams[model]['max_epochs']\n",
    "    use_early_stop = hyperparams[model]['use_early_stop']\n",
    "    early_stop_patience = hyperparams[model]['early_stop_patience']\n",
    "    train_needed = hyperparams[model]['pretrain'] # Whether to train the model\n",
    "\n",
    "    return num_units, num_layers, dropout_rate, dense_units, batch_size, epochs, use_early_stop, early_stop_patience, train_needed\n",
    "\n",
    "num_units, num_layers, dropout_rate, dense_units, batch_size, max_epochs, use_early_stop, early_stop_patience, train_needed = extract_hyperparams(hyperparams, 'lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed0f7d-0308-41c6-9a97-44754772156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build (or load) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8857f191-02f8-490e-aa3f-46fdc8c24a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.special import kl_div\n",
    "\n",
    "# # Sample data for demonstration purposes\n",
    "# # In practice, you would use actual activation data from the model\n",
    "# def get_fp32_activation_data():\n",
    "#     # Replace this with actual data collection\n",
    "#     return np.array([0.5, 1.0, 2.0, -1.0, -0.5, 3.0, -3.0, 0.1])\n",
    "\n",
    "# # **1. Calibration: Collect Data and Compute Scale Factor**\n",
    "\n",
    "# # Collect activation data (replace with actual FP32 data)\n",
    "# activation_data_fp32 = get_fp32_activation_data()\n",
    "\n",
    "# # Step 1: Determine the maximum absolute value\n",
    "# max_abs_value = np.max(np.abs(activation_data_fp32))\n",
    "\n",
    "# # Step 2: Compute the scale factor for symmetric quantization\n",
    "# def compute_scale(max_abs_value):\n",
    "#     return max_abs_value / 127  # 127 because 8-bit quantization uses values from -128 to 127\n",
    "\n",
    "# scale = compute_scale(max_abs_value)\n",
    "\n",
    "# # Step 3: Perform preliminary quantization of FP32 data\n",
    "# def preliminary_quantize(fp32_data, scale):\n",
    "#     return np.clip(np.round(fp32_data / scale), -128, 127).astype(np.int8)\n",
    "\n",
    "# activation_data_int8 = preliminary_quantize(activation_data_fp32, scale)\n",
    "\n",
    "# # Step 4: Compute histograms for FP32 and INT8\n",
    "# def compute_histograms(fp32_data, int8_data):\n",
    "#     hist_fp32, _ = np.histogram(fp32_data, bins=256, range=(-128, 127), density=True)\n",
    "#     hist_int8, _ = np.histogram(int8_data, bins=256, range=(-128, 127), density=True)\n",
    "#     return hist_fp32, hist_int8\n",
    "\n",
    "# hist_fp32, hist_int8 = compute_histograms(activation_data_fp32, activation_data_int8)\n",
    "\n",
    "# # Step 5: Compute KL divergence\n",
    "# def compute_kl_divergence(hist_fp32, hist_int8):\n",
    "#     # Adding a small constant to avoid log(0)\n",
    "#     return np.sum(kl_div(hist_fp32 + 1e-8, hist_int8 + 1e-8))\n",
    "\n",
    "# kl_divergence = compute_kl_divergence(hist_fp32, hist_int8)\n",
    "# print(f\"KL Divergence: {kl_divergence}\")\n",
    "\n",
    "# # **2. Quantization: Convert FP32 to INT8**\n",
    "\n",
    "# # Final quantization of FP32 data\n",
    "# def quantize_to_int8(fp32_data, scale):\n",
    "#     return np.clip(np.round(fp32_data / scale), -128, 127).astype(np.int8)\n",
    "\n",
    "# activation_data_int8_final = quantize_to_int8(activation_data_fp32, scale)\n",
    "\n",
    "# # **3. INT32 Computations: Perform Layer Operations**\n",
    "\n",
    "# # Example INT32 computation function\n",
    "# def int32_computations(weights, activations, bias):\n",
    "#     # Perform INT32 matrix multiplication and add bias\n",
    "#     int32_result = np.dot(weights, activations) + bias\n",
    "#     return int32_result\n",
    "\n",
    "# # Sample weights and bias for demonstration\n",
    "# weights = np.array([[1, -1], [2, 3]])\n",
    "# bias = np.array([1, -1])\n",
    "\n",
    "# # Perform INT32 computations\n",
    "# int32_result = int32_computations(weights, activation_data_int8_final, bias)\n",
    "# print(f\"INT32 Computation Result: {int32_result}\")\n",
    "\n",
    "# # **4. Re-Quantization: Convert INT32 to INT8**\n",
    "\n",
    "# # Re-quantization process\n",
    "# def requantize(int32_activations, scale, zero_point, bias):\n",
    "#     # Add bias and then requantize\n",
    "#     int32_activations_with_bias = int32_activations + bias\n",
    "#     return np.clip(np.round(int32_activations_with_bias * scale) + zero_point, -128, 127).astype(np.int8)\n",
    "\n",
    "# # Assuming zero_point = 0 for symmetric quantization\n",
    "# zero_point = 0\n",
    "\n",
    "# # Re-quantize INT32 results to INT8\n",
    "# activation_data_int8_requantized = requantize(int32_result, scale, zero_point, bias)\n",
    "# print(f\"Re-Quantized INT8 Data: {activation_data_int8_requantized}\")\n",
    "\n",
    "# # **5. De-Quantization: Convert INT8 Back to FP32**\n",
    "\n",
    "# # De-quantization process\n",
    "# def dequantize_to_fp32(int8_data, scale, zero_point):\n",
    "#     return (int8_data - zero_point) * scale\n",
    "\n",
    "# # Convert INT8 results back to FP32\n",
    "# fp32_reconstructed_data = dequantize_to_fp32(activation_data_int8_requantized, scale, zero_point)\n",
    "# print(f\"De-Quantized FP32 Data: {fp32_reconstructed_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bbc23c-52c1-45a3-a157-a91885ad43d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaad883-90fd-49f2-b7fd-57f7510cbbb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7cfe96-fa3d-48fc-9b58-89e94c89e5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ca9f03-f31e-4960-ab06-5a79524b0770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61455999-322f-4fba-8f21-8dd9a1184399",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4af167e-bec0-45df-9aa1-5b6c65bd6932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hyperparameters\n",
    "num_units, num_layers, dropout_rate, dense_units, batch_size, max_epochs, use_early_stop, early_stop_patience, train_needed = extract_hyperparams(hyperparams, 'gru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7cb49e11-a485-4c64-a223-f10f321fc59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build (or load) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0941196-b379-471e-8741-4eea2c85fe74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ef625c-6263-4b30-a8cd-766496c4d490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97986b0-e170-45bc-aa1c-9a6911ad1540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f806ae-6d5a-46f1-8965-20c06c6e1de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4edf79-e585-46e9-8a3f-ddd4f3e0d988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3292b5d-e5dc-4e19-ae07-233d37db86d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
