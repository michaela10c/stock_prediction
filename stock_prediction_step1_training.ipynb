{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7447414a-284c-4a75-b9f5-1346823d764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error \n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ac210-ed57-4af5-bae6-bb18c9052ea3",
   "metadata": {},
   "source": [
    "## Data Preparation & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "688825b2-4404-403d-ab53-24f12cacc07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)  # Set seed for numpy\n",
    "    random.seed(seed)  # Set seed for random\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.manual_seed(seed)  # Set seed for PyTorch CPU\n",
    "    \n",
    "    torch.cuda.manual_seed(seed)  # Set seed for PyTorch GPU\n",
    "    torch.cuda.manual_seed_all(seed)  # Set seed for all GPUs\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior for CUDA\n",
    "    torch.backends.cudnn.benchmark = False  # Disable the auto-tuner for GPUs\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "148e7917-605a-4d39-81ef-ce0b63b886a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "def load_data(file):\n",
    "    # Load the data\n",
    "    data = pd.read_csv('data/' + file + '.csv')\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.set_index('Date', inplace=True)\n",
    "\n",
    "    # round to 2 decimal places\n",
    "    data = data.round(2) \n",
    "\n",
    "    return data\n",
    "\n",
    "data = load_data('SP500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50b2d188-c818-4359-8b8e-b74361a521c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data (using min-max scaler)\n",
    "def normalize(features):\n",
    "    min_val = np.min(features)\n",
    "    max_val = np.max(features)\n",
    "    return (features - min_val) / (max_val - min_val)\n",
    "    \n",
    "features = np.array(data[['Close']])\n",
    "scaled_data = normalize(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31e01947-d480-45a5-9807-dd21fa3a2f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hyperparameters\n",
    "with open('configs/configs.json', 'r') as file:\n",
    "    hyperparams = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1af40767-fc7b-4f98-87e2-1ffce90c6377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences\n",
    "def create_sequences(data, input_size, num_steps, test_split=0.1):\n",
    "    data = [np.array(data[i * input_size: (i + 1) * input_size]) \n",
    "       for i in range(len(data) // input_size)]\n",
    "\n",
    "    # Split into groups of `num_steps`\n",
    "    X = np.array([data[i: i + num_steps] for i in range(len(data) - num_steps)])\n",
    "    y = np.array([data[i + num_steps] for i in range(len(data) - num_steps)])\n",
    "\n",
    "    # Reshape X to have shape (N, num_steps, input_size)\n",
    "    X = X.reshape(-1, num_steps, input_size)\n",
    "    y = y.reshape(-1, input_size)  # Reshape y to match the output shape\n",
    "\n",
    "    split = int((1 - test_split) * len(X))\n",
    "    X_train, X_test = X[:split], X[split:]\n",
    "    y_train, y_test = y[:split], y[split:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f03fec6d-13eb-4fb2-8149-1355f6621318",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = hyperparams['num_steps'] # Extract number of steps\n",
    "input_size = hyperparams['input_size'] # Extract number of steps\n",
    "test_split = hyperparams['test_split']\n",
    "X_train, y_train, X_test, y_test = create_sequences(scaled_data, input_size, num_steps, test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "190c9e4f-4467-4d00-8a41-8d65790b9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "assert(not torch.isnan(X_train_tensor).any())\n",
    "assert(not torch.isnan(y_train_tensor).any())\n",
    "assert(not torch.isnan(X_test_tensor).any())\n",
    "assert(not torch.isnan(y_test_tensor).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d3c3d65-dd91-46a8-afcc-d4ffe7808a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader instances\n",
    "batch_size = hyperparams['lstm']['batch_size']\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bed851-6643-4cb9-8df6-5c3cd6217810",
   "metadata": {},
   "source": [
    "## Model Definition: LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "925a4e78-4ccc-48e1-8fd8-c0da3c79cc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hyperparameters\n",
    "def extract_hyperparams(hyperparams, model):\n",
    "    num_units = hyperparams[model][model + '_units']\n",
    "    num_layers = hyperparams[model][model + '_layers']\n",
    "    dropout_rate = hyperparams[model]['dropout_rate']\n",
    "    dense_units = hyperparams[model]['dense_units']\n",
    "    batch_size = hyperparams[model]['batch_size']\n",
    "    max_epochs = hyperparams[model]['max_epochs']\n",
    "    use_early_stop = hyperparams[model]['use_early_stop']\n",
    "    early_stop_patience = hyperparams[model]['early_stop_patience']\n",
    "    train_needed = hyperparams[model]['pretrain'] # Whether to train the model\n",
    "\n",
    "    return num_units, num_layers, dropout_rate, dense_units, batch_size, max_epochs, use_early_stop, early_stop_patience, train_needed\n",
    "\n",
    "num_units, num_layers, dropout_rate, dense_units, batch_size, max_epochs, use_early_stop, early_stop_patience, train_needed = extract_hyperparams(hyperparams, 'lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7bed0f7d-0308-41c6-9a97-44754772156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CustomLSTMCell, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W_i = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_i = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_i = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "\n",
    "        self.W_f = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_f = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_f = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        self.W_c = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_c = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_c = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        self.W_o = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_o = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_o = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        # Layer normalization layers\n",
    "        self.ln_i = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_f = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_c = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_o = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'W_' in name or 'U_' in name:\n",
    "                nn.init.orthogonal_(param)  # Use orthogonal initialization\n",
    "            elif 'b_' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "        \n",
    "        # Check for NaN values\n",
    "        for name, param in self.named_parameters():\n",
    "            assert(not torch.isnan(param).any())\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        i_t = torch.sigmoid(self.ln_i(torch.mm(x, self.W_i) + torch.mm(h, self.U_i) + self.b_i))\n",
    "        f_t = torch.sigmoid(self.ln_f(torch.mm(x, self.W_f) + torch.mm(h, self.U_f) + self.b_f))\n",
    "        g_t = torch.tanh(self.ln_c(torch.mm(x, self.W_c) + torch.mm(h, self.U_c) + self.b_c))\n",
    "        o_t = torch.sigmoid(self.ln_o(torch.mm(x, self.W_o) + torch.mm(h, self.U_o) + self.b_o))\n",
    "\n",
    "        c_t = f_t * c + i_t * g_t\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        \n",
    "        assert(not torch.isnan(h_t).any())\n",
    "        assert(not torch.isnan(c_t).any())\n",
    "    \n",
    "        return h_t, c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39846679-2ac7-4e08-b102-9457b571e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout_rate, dense_units, cell_type='lstm'):\n",
    "        super(CustomRNNModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "        if cell_type == 'lstm':\n",
    "            self.layers = nn.ModuleList([CustomLSTMCell(input_dim, hidden_dim)])\n",
    "            self.layers.extend([CustomLSTMCell(hidden_dim, hidden_dim) for _ in range(num_layers - 1)])\n",
    "        elif cell_type == 'gru':\n",
    "            self.layers = nn.ModuleList([CustomGRUCell(input_dim, hidden_dim)])\n",
    "            self.layers.extend([CustomGRUCell(hidden_dim, hidden_dim) for _ in range(num_layers - 1)])\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported cell type\")\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.dense = nn.Linear(hidden_dim, dense_units)  # Dense layer with `hidden_dim` to `dense_units`\n",
    "        self.fc = nn.Linear(dense_units, output_dim)    # Final linear layer with `dense_units` to `output_dim`\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        h = torch.zeros(batch_size, self.hidden_dim).to(x.device)\n",
    "        if self.cell_type == 'lstm':\n",
    "            c = torch.zeros(batch_size, self.hidden_dim).to(x.device)\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]\n",
    "            for layer in self.layers:\n",
    "                if self.cell_type == 'lstm':\n",
    "                    h, c = layer(x_t, h, c)\n",
    "                elif self.cell_type == 'gru':\n",
    "                    h = layer(x_t, h)\n",
    "                x_t = self.dropout(h)\n",
    "\n",
    "        out = self.dense(h)  # Pass through dense layer\n",
    "        out = self.fc(out)  # Pass through final linear layer\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "acf6c481-09c8-48d1-8f67-19abbb18f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "input_dim = X_train_tensor.shape[-1]\n",
    "output_dim = y_train_tensor.shape[-1]\n",
    "\n",
    "model = CustomRNNModel(input_dim=input_dim, hidden_dim=num_units, output_dim=output_dim, num_layers=num_layers, dropout_rate=dropout_rate, dense_units=dense_units, cell_type='lstm')\n",
    "model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = hyperparams['lstm']['learning_rate']\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e23b87e-33fd-4eec-a7e0-a52898067633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, cell_type, train_loader, val_loader, epochs, learning_rate, use_early_stop, early_stop_patience, train_needed):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss_sum = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            if torch.isnan(inputs).any() or torch.isnan(targets).any():\n",
    "                print(\"NaN detected in inputs or targets during training\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                outputs = model(inputs)\n",
    "            except RuntimeError as e:\n",
    "                print(f\"RuntimeError during training: {e}\")\n",
    "                continue\n",
    "            \n",
    "            outputs = outputs.squeeze(-1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            if torch.isnan(loss):\n",
    "                print(\"NaN loss encountered during training\")\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_sum += loss.item() * inputs.size(0)\n",
    "        \n",
    "        avg_train_loss = train_loss_sum / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss_sum = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                if torch.isnan(inputs).any() or torch.isnan(targets).any():\n",
    "                    print(\"NaN detected in inputs or targets during validation\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    outputs = model(inputs)\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"RuntimeError during validation: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                outputs = outputs.squeeze(-1)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                if torch.isnan(loss):\n",
    "                    print(\"NaN validation loss encountered\")\n",
    "                    continue\n",
    "\n",
    "                val_loss_sum += loss.item() * inputs.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss_sum / len(val_loader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        if use_early_stop:\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), f'models/SP500_{cell_type}_model.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= early_stop_patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "\n",
    "    if use_early_stop:\n",
    "        model.load_state_dict(torch.load(f'models/SP500_{cell_type}_model.pth'))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "448781d8-4161-40bb-a100-1128f3c37436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "if train_needed:\n",
    "    # Instantiate the model\n",
    "    model = CustomRNNModel(\n",
    "        input_dim=X_train_tensor.shape[-1],\n",
    "        hidden_dim=num_units,\n",
    "        output_dim=y_train_tensor.shape[-1],\n",
    "        num_layers=num_layers,\n",
    "        dropout_rate=dropout_rate,\n",
    "        dense_units=dense_units,\n",
    "        cell_type='lstm'\n",
    "    )\n",
    "    model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    model = train_model(model, 'lstm', train_loader, val_loader, epochs=max_epochs, learning_rate=learning_rate, use_early_stop=use_early_stop, early_stop_patience=early_stop_patience, train_needed=train_needed)\n",
    "\n",
    "else:\n",
    "    # Load the model\n",
    "    model.load_state_dict(torch.load(f'models/SP500_lstm_model.pth'))\n",
    "    print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "116d8e8f-a5be-43e7-9046-15c4cf78e825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0808\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "model.eval()\n",
    "test_loss_sum = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze(-1)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss_sum += loss.item() * inputs.size(0)\n",
    "\n",
    "avg_test_loss = test_loss_sum / len(test_loader.dataset)\n",
    "print(f'Test Loss: {avg_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31af3c5-7230-4587-b8fc-32e7b68a7358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "914882a4-fa1c-4fcc-90b2-223425d2ea12",
   "metadata": {},
   "source": [
    "## Model Definition: GRU Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f760a21-86b1-4d94-b3fd-f2b2e1223e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hyperparameters\n",
    "num_units, num_layers, dropout_rate, dense_units, batch_size, max_epochs, use_early_stop, early_stop_patience, train_needed = extract_hyperparams(hyperparams, 'gru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c98952-122b-4d07-a378-4b6b50441a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build (or load) model\n",
    "class CustomGRUCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CustomGRUCell, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.W_z = nn.Parameter(torch.randn(input_dim, hidden_dim))  # Input-to-hidden weight for update gate\n",
    "        self.U_z = nn.Parameter(torch.randn(hidden_dim, hidden_dim))  # Hidden-to-hidden weight for update gate\n",
    "        self.b_z = nn.Parameter(torch.zeros(hidden_dim))  # Bias for update gate\n",
    "        \n",
    "        self.W_r = nn.Parameter(torch.randn(input_dim, hidden_dim))  # Input-to-hidden weight for reset gate\n",
    "        self.U_r = nn.Parameter(torch.randn(hidden_dim, hidden_dim))  # Hidden-to-hidden weight for reset gate\n",
    "        self.b_r = nn.Parameter(torch.zeros(hidden_dim))  # Bias for reset gate\n",
    "        \n",
    "        self.W_h = nn.Parameter(torch.randn(input_dim, hidden_dim))  # Input-to-hidden weight for candidate hidden state\n",
    "        self.U_h = nn.Parameter(torch.randn(hidden_dim, hidden_dim))  # Hidden-to-hidden weight for candidate hidden state\n",
    "        self.b_h = nn.Parameter(torch.zeros(hidden_dim))  # Bias for candidate hidden state\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param, a=0, mode='fan_in', nonlinearity='tanh')\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        z_t = torch.sigmoid(torch.mm(x, self.W_z) + torch.mm(h, self.U_z) + self.b_z)\n",
    "        r_t = torch.sigmoid(torch.mm(x, self.W_r) + torch.mm(h, self.U_r) + self.b_r)\n",
    "        h_hat_t = torch.tanh(torch.mm(x, self.W_h) + torch.mm(r_t * h, self.U_h) + self.b_h)\n",
    "        h_t = (1 - z_t) * h + z_t * h_hat_t\n",
    "        \n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9211600-6f1c-4b5e-acd0-13d49e8f9990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "input_dim = X_train_tensor.shape[-1]\n",
    "output_dim = y_train_tensor.shape[-1]\n",
    "\n",
    "model = CustomRNNModel(input_dim=input_dim, hidden_dim=num_units, output_dim=output_dim, num_layers=num_layers, dropout_rate=dropout_rate, cell_type='gru')\n",
    "model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = hyperparams['gru']['learning_rate']\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3d2ef2-1582-45f8-ade4-1a5bc00a1a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "model.eval()\n",
    "test_loss_sum = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze(-1)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss_sum += loss.item() * inputs.size(0)\n",
    "\n",
    "avg_test_loss = test_loss_sum / len(test_loader.dataset)\n",
    "print(f'Test Loss: {avg_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8464e369-b9e7-4f87-896e-b0d78bf1d6db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74062759-5308-47f4-a88a-41346d64e901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8857f191-02f8-490e-aa3f-46fdc8c24a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_needed:\n",
    "    # Train the model\n",
    "    model = train_model(model, 'gru', train_loader, val_loader, epochs=max_epochs, learning_rate=learning_rate, use_early_stop=use_early_stop, early_stop_patience=early_stop_patience, train_needed=train_needed)\n",
    "else:\n",
    "    # Load the model\n",
    "    model.load_state_dict(torch.load(f'models/SP500_gru_model.pth'))\n",
    "    print(\"Model loaded successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaad883-90fd-49f2-b7fd-57f7510cbbb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7cfe96-fa3d-48fc-9b58-89e94c89e5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ca9f03-f31e-4960-ab06-5a79524b0770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb49e11-a485-4c64-a223-f10f321fc59e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0941196-b379-471e-8741-4eea2c85fe74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ef625c-6263-4b30-a8cd-766496c4d490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97986b0-e170-45bc-aa1c-9a6911ad1540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f806ae-6d5a-46f1-8965-20c06c6e1de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4edf79-e585-46e9-8a3f-ddd4f3e0d988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3292b5d-e5dc-4e19-ae07-233d37db86d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.special import kl_div\n",
    "\n",
    "# # Sample data for demonstration purposes\n",
    "# # In practice, you would use actual activation data from the model\n",
    "# def get_fp32_activation_data():\n",
    "#     # Replace this with actual data collection\n",
    "#     return np.array([0.5, 1.0, 2.0, -1.0, -0.5, 3.0, -3.0, 0.1])\n",
    "\n",
    "# # **1. Calibration: Collect Data and Compute Scale Factor**\n",
    "\n",
    "# # Collect activation data (replace with actual FP32 data)\n",
    "# activation_data_fp32 = get_fp32_activation_data()\n",
    "\n",
    "# # Step 1: Determine the maximum absolute value\n",
    "# max_abs_value = np.max(np.abs(activation_data_fp32))\n",
    "\n",
    "# # Step 2: Compute the scale factor for symmetric quantization\n",
    "# def compute_scale(max_abs_value):\n",
    "#     return max_abs_value / 127  # 127 because 8-bit quantization uses values from -128 to 127\n",
    "\n",
    "# scale = compute_scale(max_abs_value)\n",
    "\n",
    "# # Step 3: Perform preliminary quantization of FP32 data\n",
    "# def preliminary_quantize(fp32_data, scale):\n",
    "#     return np.clip(np.round(fp32_data / scale), -128, 127).astype(np.int8)\n",
    "\n",
    "# activation_data_int8 = preliminary_quantize(activation_data_fp32, scale)\n",
    "\n",
    "# # Step 4: Compute histograms for FP32 and INT8\n",
    "# def compute_histograms(fp32_data, int8_data):\n",
    "#     hist_fp32, _ = np.histogram(fp32_data, bins=2048, range=(-128, 127), density=True)\n",
    "#     hist_int8, _ = np.histogram(int8_data, bins=2048, range=(-128, 127), density=True)\n",
    "#     return hist_fp32, hist_int8\n",
    "\n",
    "# hist_fp32, hist_int8 = compute_histograms(activation_data_fp32, activation_data_int8)\n",
    "\n",
    "# # Step 5: Compute KL divergence\n",
    "# def compute_kl_divergence(hist_fp32, hist_int8):\n",
    "#     # Adding a small constant to avoid log(0)\n",
    "#     return np.sum(kl_div(hist_fp32 + 1e-8, hist_int8 + 1e-8))\n",
    "\n",
    "# kl_divergence = compute_kl_divergence(hist_fp32, hist_int8)\n",
    "# print(f\"KL Divergence: {kl_divergence}\")\n",
    "\n",
    "# # **2. Quantization: Convert FP32 to INT8**\n",
    "\n",
    "# # Final quantization of FP32 data\n",
    "# def quantize_to_int8(fp32_data, scale):\n",
    "#     return np.clip(np.round(fp32_data / scale), -128, 127).astype(np.int8)\n",
    "\n",
    "# activation_data_int8_final = quantize_to_int8(activation_data_fp32, scale)\n",
    "\n",
    "# # **3. INT32 Computations: Perform Layer Operations**\n",
    "\n",
    "# # Example INT32 computation function\n",
    "# def int32_computations(weights, activations, bias):\n",
    "#     # Perform INT32 matrix multiplication and add bias\n",
    "#     int32_result = np.dot(weights, activations) + bias\n",
    "#     return int32_result\n",
    "\n",
    "# # Sample weights and bias for demonstration\n",
    "# weights = np.array([[1, -1], [2, 3]])\n",
    "# bias = np.array([1, -1])\n",
    "\n",
    "# # Perform INT32 computations\n",
    "# int32_result = int32_computations(weights, activation_data_int8_final, bias)\n",
    "# print(f\"INT32 Computation Result: {int32_result}\")\n",
    "\n",
    "# # **4. Re-Quantization: Convert INT32 to INT8**\n",
    "\n",
    "# # Re-quantization process\n",
    "# def requantize(int32_activations, scale, zero_point, bias):\n",
    "#     # Add bias and then requantize\n",
    "#     int32_activations_with_bias = int32_activations + bias\n",
    "#     return np.clip(np.round(int32_activations_with_bias * scale) + zero_point, -128, 127).astype(np.int8)\n",
    "\n",
    "# # Assuming zero_point = 0 for symmetric quantization\n",
    "# zero_point = 0\n",
    "\n",
    "# # Re-quantize INT32 results to INT8\n",
    "# activation_data_int8_requantized = requantize(int32_result, scale, zero_point, bias)\n",
    "# print(f\"Re-Quantized INT8 Data: {activation_data_int8_requantized}\")\n",
    "\n",
    "# # **5. De-Quantization: Convert INT8 Back to FP32**\n",
    "\n",
    "# # De-quantization process\n",
    "# def dequantize_to_fp32(int8_data, scale, zero_point):\n",
    "#     return (int8_data - zero_point) * scale\n",
    "\n",
    "# # Convert INT8 results back to FP32\n",
    "# fp32_reconstructed_data = dequantize_to_fp32(activation_data_int8_requantized, scale, zero_point)\n",
    "# print(f\"De-Quantized FP32 Data: {fp32_reconstructed_data}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
