{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "4CATJiF_A1VI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import json\n",
    "import seaborn as sns\n",
    "import time\n",
    "import itertools\n",
    "import cProfile\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import entropy\n",
    "from scipy.interpolate import interp1d\n",
    "from TimeSeriesDataset import TimeSeriesDataset\n",
    "import logging\n",
    "import os\n",
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXcNnnW_C3XC"
   },
   "source": [
    "## Data Preparation & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "UOXSnPeqEO7q"
   },
   "outputs": [],
   "source": [
    "def load_configs(filename):\n",
    "    with open(f'configs/{filename}.json', 'r') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "nxRE-Z-hBRWh"
   },
   "outputs": [],
   "source": [
    "# Plot functions\n",
    "def plot_last_prices(last_prices, normalized=True):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(last_prices, label='Normalized Prices')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Normalized Price' if normalized else 'Price')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.title('Relative Change Rates of Close Prices' if normalized else 'Close Prices')\n",
    "    plt.show()\n",
    "\n",
    "def plot_volume(data, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(data['datetime'], data['volume'])\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Trade Volume')\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "def plot_prices_with_ma(data, period, period_labels):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(data.index, data['last_price'], label='Last Price')\n",
    "    for i in range(len(period)):\n",
    "      t = period[i]\n",
    "      period_label = period_labels[i]\n",
    "      plt.plot(data.index, data[f'ma({t})'], label=f'{period_label} MA')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Last Price')\n",
    "    plt.title('Prices with Moving Average')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_prices_with_macd(data, short_period, long_period, signal_period):\n",
    "    for i in range(len(short_period)):\n",
    "      plt.figure(figsize=(10, 5))\n",
    "      a = short_period[i]\n",
    "      b = long_period[i]\n",
    "      c = signal_period[i]\n",
    "      plt.plot(data.index, data[f'macd_line({a},{b},{c})'], label=f'MACD')\n",
    "      plt.plot(data.index, data[f'signal_line({a},{b},{c})'], label=f'Signal')\n",
    "      plt.plot(data.index, data[f'macd_histogram({a},{b},{c})'], label=f'Histogram')\n",
    "      plt.xlabel('Date')\n",
    "      plt.ylabel('Convergence/Divergence')\n",
    "      plt.title(f\"MACD({a},{b},{c})\")\n",
    "      plt.legend()\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "XkiCBffzERr9"
   },
   "outputs": [],
   "source": [
    "# Moving Average (MA)\n",
    "def moving_average(last_prices, num_periods):\n",
    "    ma = last_prices.rolling(window=num_periods, min_periods=1).mean()\n",
    "    # Forward fill the first num_periods-1 NaN values with the first non-NaN value\n",
    "    ma.ffill(inplace=True)\n",
    "    return ma\n",
    "\n",
    "def calculate_ema(data, period):\n",
    "    alpha = 2 / (period + 1)\n",
    "    ema = [data.iloc[0]]  # EMA starts with the first data point\n",
    "\n",
    "    for price in data.iloc[1:]:\n",
    "        ema.append(alpha * price + (1 - alpha) * ema[-1])\n",
    "\n",
    "    return pd.Series(ema, index=data.index)\n",
    "\n",
    "# Moving Average Convergence/Divergence (MACD)\n",
    "def calculate_macd(data, short_period, long_period, signal_period):\n",
    "    short_ema = calculate_ema(data, short_period)\n",
    "    long_ema = calculate_ema(data, long_period)\n",
    "\n",
    "    macd_line = short_ema - long_ema\n",
    "    signal_line = calculate_ema(macd_line, signal_period)\n",
    "    macd_histogram = macd_line - signal_line\n",
    "\n",
    "    return macd_line, signal_line, macd_histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kq3mxEWoEUt-"
   },
   "outputs": [],
   "source": [
    "def filter_data_by_intervals(data, intervals):\n",
    "    # Initialize a mask with False values\n",
    "    interval_mask = pd.Series([False] * len(data))\n",
    "\n",
    "    # Iterate over intervals and apply masks\n",
    "    for start, end in intervals:\n",
    "        start_time = pd.to_datetime(start).time()\n",
    "        end_time = pd.to_datetime(end).time()\n",
    "\n",
    "        # Create masks for start and end times\n",
    "        start_time_mask = (data['datetime'].dt.time >= start_time)\n",
    "        end_time_mask = (data['datetime'].dt.time <= end_time)\n",
    "\n",
    "        # Combine masks based on interval crossing midnight or not\n",
    "        if start_time <= end_time:\n",
    "            interval_mask |= (start_time_mask & end_time_mask)\n",
    "        else:\n",
    "            interval_mask |= (start_time_mask | end_time_mask)\n",
    "\n",
    "    # Apply the final mask to filter the data\n",
    "    data_filtered = data[interval_mask]\n",
    "    return data_filtered\n",
    "\n",
    "def assert_time_intervals(df, intervals):\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    times = df.index.time\n",
    "\n",
    "    time_intervals = [(pd.Timestamp(start).time(), pd.Timestamp(end).time()) for start, end in intervals]\n",
    "\n",
    "    def is_within_intervals(t):\n",
    "        return any(start <= t <= end if start <= end else start <= t or t <= end for start, end in time_intervals)\n",
    "\n",
    "    outside_intervals = ~np.vectorize(is_within_intervals)(times)\n",
    "\n",
    "    if outside_intervals.any():\n",
    "        print(\"There are times outside the specified intervals:\")\n",
    "        print(df[outside_intervals])\n",
    "    else:\n",
    "        print(\"All times are within the specified intervals.\")\n",
    "\n",
    "    assert(not outside_intervals.any())\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def extract_main_contract(data_filtered, window=1000, min_periods=1, quantile=0.80):\n",
    "    # Compute the rolling mean of volume\n",
    "    data_filtered.loc[:, 'volume_rolling'] = data_filtered['volume'].rolling(window=window, min_periods=min_periods).mean()\n",
    "\n",
    "    # Calculate the volume threshold based on the quantile\n",
    "    volume_threshold = data_filtered['volume_rolling'].quantile(quantile)\n",
    "\n",
    "    # Identify high volume segments\n",
    "    data_filtered.loc[:, 'high_volume'] = data_filtered['volume_rolling'] > volume_threshold\n",
    "\n",
    "    # Segment identification by cumulative sum of changes in high_volume status\n",
    "    data_filtered.loc[:, 'segment'] = data_filtered['high_volume'].ne(data_filtered['high_volume'].shift()).cumsum()\n",
    "\n",
    "    # Filter the high volume segments and fill NaN values\n",
    "    high_volume_segments = data_filtered[data_filtered['high_volume']]\n",
    "    high_volume_segments.loc[:, 'volume'] = high_volume_segments['volume'].ffill()\n",
    "\n",
    "    # Drop the temporary columns\n",
    "    high_volume_segments.drop(columns=['high_volume', 'segment', 'volume_rolling'], inplace=True)\n",
    "\n",
    "    return high_volume_segments\n",
    "\n",
    "\n",
    "def normalize_prices(data_array, intervals, timestamps, num_steps, input_size, last_price_index=0):\n",
    "    # Convert timestamps to time objects\n",
    "    timestamp_times = timestamps.time\n",
    "\n",
    "    # Create an empty array to collect normalized prices\n",
    "    normalized_last_price = np.full(len(data_array), np.nan)  # Use NaN to identify unassigned values\n",
    "\n",
    "    for start, end in intervals:\n",
    "        start_time = pd.Timestamp(start).time()\n",
    "        end_time = pd.Timestamp(end).time()\n",
    "\n",
    "        # Create masks for the interval\n",
    "        start_mask = timestamp_times >= start_time\n",
    "        end_mask = timestamp_times <= end_time\n",
    "        if start_time <= end_time:\n",
    "            interval_mask = start_mask & end_mask\n",
    "        else:\n",
    "            interval_mask = start_mask | end_mask\n",
    "\n",
    "        # Filter data by interval\n",
    "        interval_data = data_array[interval_mask]\n",
    "        clear_gpu_cache()\n",
    "        if len(interval_data) == 0:\n",
    "            continue\n",
    "\n",
    "        num_windows = (len(interval_data) + input_size - 1) // input_size\n",
    "        windows = np.array_split(interval_data, num_windows)\n",
    "\n",
    "        # Create array to hold normalized values for the current interval\n",
    "        interval_normalized_last_price = np.full(len(interval_data), np.nan)\n",
    "\n",
    "        start_idx = 0\n",
    "        for window_data in windows:\n",
    "            if len(window_data) == 0:\n",
    "                continue\n",
    "\n",
    "            if start_idx == 0:\n",
    "                window_first_price = window_data[0, last_price_index]\n",
    "                values = window_data[:, last_price_index] / window_first_price - 1.0\n",
    "            else:\n",
    "                window_last_price = window_data[-1, last_price_index]\n",
    "                values = window_data[:, last_price_index] / window_last_price - 1.0\n",
    "\n",
    "            end_idx = start_idx + len(window_data)\n",
    "            interval_normalized_last_price[start_idx:end_idx] = values\n",
    "            start_idx = end_idx\n",
    "\n",
    "        normalized_last_price[interval_mask] = interval_normalized_last_price\n",
    "\n",
    "    # Set to the data array\n",
    "    data_array[:, last_price_index] = normalized_last_price\n",
    "\n",
    "    # Process all outliers - impute with its previous non-outlying value\n",
    "    postprocess_outliers(data_array)\n",
    "\n",
    "    # Check for NaN values\n",
    "    if np.isnan(data_array[:, last_price_index]).any():\n",
    "        raise ValueError(\"Data contains NaN values after normalization. Please check the normalization process.\")\n",
    "\n",
    "    return data_array\n",
    "\n",
    "\n",
    "def postprocess_outliers(data, threshold=0.5):\n",
    "    outlier_indices = np.where(np.abs(data[:, 0]) > threshold)[0]\n",
    "\n",
    "    for idx in outlier_indices:\n",
    "        previous_value_idx = idx - 1\n",
    "        while data[previous_value_idx, 0] > threshold and previous_value_idx > 0:\n",
    "            previous_value_idx -= 1\n",
    "        data[idx, 0] = data[previous_value_idx, 0]\n",
    "\n",
    "def roll_data(data_array, num_steps, input_size):\n",
    "    # Roll data to reshape it into the 4D shape (N, num_steps, input_size, # features)\n",
    "    data_array = [np.array(data_array[i * input_size: (i + 1) * input_size])\n",
    "                  for i in range(len(data_array) // input_size)]\n",
    "    data_array = np.stack(data_array)\n",
    "    return data_array\n",
    "\n",
    "\n",
    "# Generator function\n",
    "def data_generator(data_array, indices, num_steps, batch_size, last_price_index=0):\n",
    "    total_len = len(indices)\n",
    "    for start_idx in range(0, total_len, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, total_len)\n",
    "        batch_indices = indices[start_idx:end_idx]\n",
    "        X_batch = np.array([data_array[i: i + num_steps] for i in batch_indices])\n",
    "        y_batch = data_array[batch_indices + num_steps, :, last_price_index]\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "\n",
    "# Train-test split function\n",
    "def train_test_split(data_array, num_steps, input_size, val_split, test_split, last_price_index=0):\n",
    "    # Calculate the total number of samples\n",
    "    total_len = len(data_array) - num_steps\n",
    "\n",
    "    X = np.empty((total_len, num_steps, input_size, data_array.shape[-1] - 1), dtype=np.float32)\n",
    "    y = np.empty((total_len, num_steps), dtype=np.float32)\n",
    "\n",
    "    for i in range(total_len):\n",
    "        X[i] = data_array[i:i + num_steps, :, 1:]  # Exclude the last_price column (column 0)\n",
    "        y[i] = data_array[i:i + num_steps, :, last_price_index]  # Store the last_price values\n",
    "\n",
    "    # Check the shapes of X and y\n",
    "    num_features = data_array.shape[-1] - 1  # Exclude the last_price column\n",
    "    assert X.shape == (total_len, num_steps, input_size, num_features), f\"X shape mismatch: {X.shape}\"\n",
    "    assert y.shape == (total_len, num_steps), f\"y shape mismatch: {y.shape}\"\n",
    "    assert len(X) == len(y), \"Number of samples in X and y must be equal\"\n",
    "\n",
    "    # Determine the split indices\n",
    "    test_start = int(total_len * (1 - test_split))\n",
    "    val_start = int(total_len * (1 - test_split - val_split))\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_val, X_test = X[:val_start], X[val_start:test_start], X[test_start:]\n",
    "    y_train, y_val, y_test = y[:val_start], y[val_start:test_start], y[test_start:]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "def normalize_data(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    normalized_df = pd.DataFrame(normalized_data, index=data.index, columns=data.columns)\n",
    "    return normalized_df\n",
    "\n",
    "# Create dataloader instances\n",
    "def create_dataloader_instances(dataset, val_split, test_split, batch_size, num_workers=8):\n",
    "    dataset_size = len(dataset)\n",
    "    test_size = int(test_split * dataset_size)\n",
    "    val_size = int(val_split * (dataset_size - test_size))\n",
    "    train_size = dataset_size - val_size - test_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Create forward fill bid custom\n",
    "def forward_fill_bid_custom(data, price_cols):\n",
    "    # Forward fill NaN values across rows for specified columns\n",
    "    data[price_cols] = data[price_cols].ffill(axis=1)\n",
    "\n",
    "    # Forward fill NaN values across columns for specified columns\n",
    "    data[price_cols] = data[price_cols].ffill(axis=0)\n",
    "\n",
    "    return data\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Create sequences based on the extended_segment\n",
    "def create_sequences(extended_segment, num_steps, input_size, normalize, val_split=0.1, test_split=0.1, batch_size=64):\n",
    "    # Ensure no NaN values\n",
    "    assert(np.isnan(extended_segment).sum().sum() == 0)\n",
    "\n",
    "    # Setup\n",
    "    timestamps = extended_segment.index\n",
    "    extended_segment = extended_segment.to_numpy()\n",
    "    last_price_index = 0\n",
    "\n",
    "    # Prevent a division by 0 by imputing 0s to a very small number\n",
    "    extended_segment[:, last_price_index] = np.where(extended_segment[:, last_price_index] == 0, 1e-6, extended_segment[:, last_price_index])\n",
    "\n",
    "    # Plot the prices before normalization\n",
    "    plot_last_prices(extended_segment[:, last_price_index], normalized=False)\n",
    "\n",
    "    # Normalize prices - retrieve relative change rates\n",
    "    if normalize:\n",
    "        extended_segment = normalize_prices(extended_segment, intervals, timestamps, num_steps, input_size, last_price_index)\n",
    "\n",
    "    clear_gpu_cache()\n",
    "\n",
    "    # Plot the prices after normalization\n",
    "    plot_last_prices(extended_segment[:, last_price_index])\n",
    "\n",
    "    # Roll data for RNN\n",
    "    extended_segment = roll_data(extended_segment, num_steps, input_size)\n",
    "    clear_gpu_cache()\n",
    "\n",
    "    # Create dataset and dataloader instances\n",
    "    dataset = TimeSeriesDataset(extended_segment, num_steps, last_price_index)\n",
    "    clear_gpu_cache()\n",
    "    train_loader, val_loader, test_loader = create_dataloader_instances(dataset, val_split=val_split, test_split=test_split, batch_size=batch_size)\n",
    "    clear_gpu_cache()\n",
    "    \n",
    "    # # Remove all rows with exceptionally small bid ask spread\n",
    "    # extended_segment = extended_segment[:, ]\n",
    "\n",
    "    return extended_segment, train_loader, val_loader, test_loader, dataset.num_features\n",
    "\n",
    "\n",
    "def add_derived_features(extended_segment):\n",
    "    # Forward fill nan values in bid_prices (NOTE: This may not reflect the changes in market!)\n",
    "    bid_price_cols = ['bid_price1', 'bid_price2', 'bid_price3', 'bid_price4', 'bid_price5']\n",
    "    ask_price_cols = ['ask_price1', 'ask_price2', 'ask_price3', 'ask_price4', 'ask_price5']\n",
    "    extended_segment = forward_fill_bid_custom(extended_segment, bid_price_cols)\n",
    "\n",
    "    # Bid & Ask Volumes\n",
    "    bid_volume_cols = ['bid_volume1', 'bid_volume2', 'bid_volume3', 'bid_volume4', 'bid_volume5']\n",
    "    ask_volume_cols = ['ask_volume1', 'ask_volume2', 'ask_volume3', 'ask_volume4', 'ask_volume5']\n",
    "\n",
    "    # Calculate Bid-Ask Spread\n",
    "    extended_segment.loc[:, 'bid_ask_spread'] = extended_segment['ask_price1'] - extended_segment['bid_price1']\n",
    "\n",
    "    # Calculate Market Depth (total bid and ask volumes)\n",
    "    extended_segment.loc[:, 'total_bid_volume'] = extended_segment[bid_volume_cols].sum(axis=1)\n",
    "    extended_segment.loc[:, 'total_ask_volume'] = extended_segment[ask_volume_cols].sum(axis=1)\n",
    "\n",
    "    # Calculate Order Imbalance Ratio\n",
    "    extended_segment.loc[:, 'order_imbalance_ratio'] = (extended_segment['total_bid_volume'] - extended_segment['total_ask_volume']) / (extended_segment['total_bid_volume'] + extended_segment['total_ask_volume'])\n",
    "\n",
    "    # Calculate Volume Order Imbalance\n",
    "    delta_bid_vol = extended_segment[bid_volume_cols].diff().fillna(0)\n",
    "    delta_ask_vol = extended_segment[ask_volume_cols].diff().fillna(0)\n",
    "    delta_bid_price = extended_segment[bid_price_cols].diff().fillna(0)\n",
    "    delta_ask_price = extended_segment[ask_price_cols].diff().fillna(0)\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        bid_vol_col = f'bid_volume{i}'\n",
    "        ask_vol_col = f'ask_volume{i}'\n",
    "        bid_price_col = f'bid_price{i}'\n",
    "        ask_price_col = f'ask_price{i}'\n",
    "\n",
    "        # Clip delta_bid_vol to bid_volume on a rise\n",
    "        delta_bid_vol[bid_vol_col] = np.where(delta_bid_price[bid_price_col] > 0,\n",
    "                                              np.minimum(delta_bid_vol[bid_vol_col], extended_segment[bid_vol_col]),\n",
    "                                              delta_bid_vol[bid_vol_col])\n",
    "\n",
    "        # Clip delta_ask_vol to ask_volume on a fall\n",
    "        delta_ask_vol[ask_vol_col] = np.where(delta_ask_price[ask_price_col] < 0,\n",
    "                                              np.minimum(delta_ask_vol[ask_vol_col], extended_segment[ask_vol_col]),\n",
    "                                              delta_ask_vol[ask_vol_col])\n",
    "\n",
    "    extended_segment.loc[:, 'volume_order_imbalance'] = delta_bid_vol.sum(axis=1) - delta_ask_vol.sum(axis=1)\n",
    "\n",
    "    # Calculate Mid-Price Basis\n",
    "    extended_segment.loc[:, 'mid_price'] = (extended_segment.loc[:, 'bid_price1'] + extended_segment.loc[:, 'ask_price1']) / 2\n",
    "\n",
    "    # Calculate average trade price if it doesn't exist\n",
    "    if 'average_trade_price' not in extended_segment.columns:\n",
    "        extended_segment['average_trade_price'] = np.where(\n",
    "            extended_segment['volume'].diff() != 0,\n",
    "            (extended_segment['amount'].diff() / extended_segment['volume'].diff()).fillna(0),\n",
    "            extended_segment['mid_price']\n",
    "        )\n",
    "    else:\n",
    "        extended_segment['average_trade_price'] = np.where(\n",
    "            extended_segment['volume'].diff() != 0,\n",
    "            (extended_segment['amount'].diff() / extended_segment['volume'].diff()).fillna(0),\n",
    "            extended_segment['average_trade_price'].shift(1).fillna(0)\n",
    "        )\n",
    "\n",
    "    extended_segment['mid_price_basis'] = extended_segment['average_trade_price'] - extended_segment['mid_price']\n",
    "\n",
    "    # Drop intermediary derived feature columns\n",
    "    intermediary_columns = [\n",
    "        'total_bid_volume', 'total_ask_volume', 'mid_price', 'average_trade_price'\n",
    "    ]\n",
    "    extended_segment = extended_segment.drop(columns=intermediary_columns)\n",
    "\n",
    "    # Ensure there are no NaN values\n",
    "    assert not extended_segment.isnull().values.any(), \"There are NaN values in the derived features\"\n",
    "    assert(np.isnan(extended_segment).sum().sum() == 0)\n",
    "\n",
    "\n",
    "def add_factors(extended_segment, short_period, long_period, signal_period, period):\n",
    "    # Compute Factors: MA & MACD\n",
    "    for i in range(len(period)):\n",
    "      # Calculate MA\n",
    "      t = period[i]\n",
    "      extended_segment[f'ma({t})'] = moving_average(extended_segment['last_price'], t)\n",
    "      extended_segment[f'ma({t})'] = extended_segment[f'ma({t})'].fillna(method='ffill')\n",
    "\n",
    "      # Calculate MACD\n",
    "      a = short_period[i]\n",
    "      b = long_period[i]\n",
    "      c = signal_period[i]\n",
    "      macd_line, signal_line, macd_histogram = calculate_macd(extended_segment['last_price'], a, b, c)\n",
    "      extended_segment[f'macd_line({a},{b},{c})'] = macd_line\n",
    "      extended_segment[f'signal_line({a},{b},{c})'] = signal_line\n",
    "      extended_segment[f'macd_histogram({a},{b},{c})'] = macd_histogram\n",
    "\n",
    "    print(extended_segment.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DVuKsKrMC7iP",
    "outputId": "907ed828-ba3b-453e-9a0f-f94d442eb1c5"
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "dataset_filename = 'INE.sc2010'\n",
    "intervals = [\n",
    "    ('21:00:00', '02:30:00'),\n",
    "    ('09:00:00', '10:15:00'),\n",
    "    ('10:30:00', '11:30:00'),\n",
    "    ('13:30:00', '15:00:00')\n",
    "]\n",
    "configs_filename = 'configs'\n",
    "\n",
    "# Load configs file\n",
    "configs = load_configs(configs_filename)\n",
    "clear_gpu_cache()\n",
    "\n",
    "# Read data\n",
    "# data = pd.read_csv('data/' + dataset_filename + '.csv')\n",
    "data = pd.read_csv(f'data/{dataset_filename}.csv')\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "\n",
    "# Set seed\n",
    "set_seed(42)\n",
    "\n",
    "with cProfile.Profile() as pr:\n",
    "    # Only include data within the specified intervals\n",
    "    data_filtered = filter_data_by_intervals(data, intervals)\n",
    "    # pr.print_stats()\n",
    "\n",
    "    # Make sure there are no values outside the given time intervals\n",
    "    assert_time_intervals(data_filtered, intervals)\n",
    "    # pr.print_stats()\n",
    "\n",
    "    # Plot trading volume data\n",
    "    plot_volume(data_filtered, 'Daily Trading Volume of SC2010 Stocks')\n",
    "\n",
    "    # Extract the main contract\n",
    "    # data_filtered = data_filtered.copy() # Avoid SettingWithCopyWarning\n",
    "    # pr.print_stats()\n",
    "    data_filtered.iloc[:, 1:] = data_filtered.iloc[:, 1:].astype(np.float32)\n",
    "    # pr.print_stats()\n",
    "    extended_segment = extract_main_contract(data_filtered)\n",
    "    # pr.print_stats()\n",
    "\n",
    "    # Plot main contract segment\n",
    "    plot_volume(extended_segment, 'Trading Volume of Main Contract Segment')\n",
    "    # pr.print_stats()\n",
    "\n",
    "    # Set index of the resulting dataframe\n",
    "    extended_segment.set_index('datetime', inplace=True)\n",
    "    # pr.print_stats()\n",
    "\n",
    "    # Make sure there are no values outside the given time intervals\n",
    "    assert_time_intervals(extended_segment, intervals)\n",
    "    # pr.print_stats()\n",
    "\n",
    "    # Extract needed hyperparams\n",
    "    input_size = configs['input_size']\n",
    "    num_steps = configs['num_steps']\n",
    "    normalize = configs['normalize']\n",
    "    batch_size = configs['batch_size']\n",
    "\n",
    "    # Add derived features\n",
    "    add_derived_features(extended_segment)\n",
    "\n",
    "    # Add factors\n",
    "    period_labels = np.array(['2.5s', '5s', '7.5s', '10s'])\n",
    "    period = np.array([5, 10, 15, 20])\n",
    "    short_period = period\n",
    "    long_period = period + 10\n",
    "    signal_period = 2 * period\n",
    "\n",
    "    add_factors(extended_segment, short_period, long_period, signal_period, period)\n",
    "    # pr.print_stats()\n",
    "\n",
    "    # Plot closing prices with factors\n",
    "    plot_prices_with_ma(extended_segment, period, period_labels)\n",
    "    plot_prices_with_macd(extended_segment, short_period, long_period, signal_period)\n",
    "\n",
    "    # Normalize data\n",
    "    extended_segment = normalize_data(extended_segment)\n",
    "    assert(extended_segment.isna().sum().sum() == 0)\n",
    "\n",
    "    # Create sequences based on the extended_segment\n",
    "    extended_segment, train_loader, val_loader, test_loader, num_features = create_sequences(extended_segment, num_steps, input_size, normalize, val_split=0.1, test_split=0.1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYSFG2CvRdl3"
   },
   "source": [
    "## RNN Model Definition \n",
    "\n",
    "Include quantized components when Quantized-Aware Training (QAT) enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97w6HrsKqROK"
   },
   "outputs": [],
   "source": [
    "def convert_to_labels(val, flat_gap=0.01):\n",
    "    return 2 if val > flat_gap else 0 if val < -flat_gap else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6Jc4p_Rqguf"
   },
   "outputs": [],
   "source": [
    "def classification_accuracy(y_pred, y_true):\n",
    "    y_pred_labels = torch.tensor([convert_to_labels(y) for y in torch.flatten(y_pred)])\n",
    "    y_true_labels = torch.tensor([convert_to_labels(y) for y in torch.flatten(y_true)])\n",
    "    return torch.sum(y_pred_labels == y_true_labels) / len(y_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to fake quantize input to specified bit-width with stochastic rounding\n",
    "def fake_quantize(x, scale, zero_point, num_bits=8, stochastic=False):\n",
    "    qmin, qmax = -(1 << (num_bits - 1)), (1 << (num_bits - 1)) - 1\n",
    "    x = x.to(torch.float32)\n",
    "    qx = x / scale + zero_point\n",
    "\n",
    "    if stochastic:\n",
    "        qx = torch.floor(qx + torch.rand_like(qx))\n",
    "    else:\n",
    "        qx = torch.round(qx)\n",
    "    \n",
    "    qx = qx.clamp(qmin, qmax)\n",
    "    return qx * scale - zero_point\n",
    "\n",
    "def analyze_output_distribution(outputs, title=\"Output Distribution\"):\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    if np.isnan(outputs).any():\n",
    "        print(f\"Warning: NaN values detected in {title}\")\n",
    "        return\n",
    "    plt.hist(outputs.flatten(), bins=100)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "# Per-channel quantization function\n",
    "def fake_quantize_per_channel(tensor, scales, zero_points, axis=0, num_bits=8, stochastic=False):\n",
    "    qmin, qmax = -(1 << (num_bits - 1)), (1 << (num_bits - 1)) - 1\n",
    "    scales = scales.unsqueeze(axis).expand_as(tensor)\n",
    "    zero_points = zero_points.unsqueeze(axis).expand_as(tensor)\n",
    "    quantized_tensor = tensor / scales + zero_points\n",
    "    \n",
    "    if stochastic:\n",
    "        quantized_tensor = torch.floor(quantized_tensor + torch.rand_like(quantized_tensor))\n",
    "    else:\n",
    "        quantized_tensor = torch.round(quantized_tensor)\n",
    "\n",
    "    quantized_tensor = quantized_tensor.clamp(qmin, qmax)\n",
    "    return quantized_tensor * scales - zero_points\n",
    "\n",
    "# Dynamic adjustment of scale and zero-point per channel based on output distribution\n",
    "def update_scale_and_zero_point_per_channel(tensor, axis=0, num_bits=8):\n",
    "    qmin, qmax = -(1 << (num_bits - 1)), (1 << (num_bits - 1)) - 1\n",
    "    tensor_min = tensor.min(dim=axis, keepdim=True).values\n",
    "    tensor_max = tensor.max(dim=axis, keepdim=True).values\n",
    "\n",
    "    scale = (tensor_max - tensor_min) / (qmax - qmin)\n",
    "    scale = torch.where(scale == 0, torch.ones_like(scale), scale)\n",
    "    zero_point = torch.round(qmin - tensor_min / scale).int().clamp(qmin, qmax)\n",
    "\n",
    "    return scale.squeeze(dim=axis), zero_point.squeeze(dim=axis)\n",
    "\n",
    "# Update scale and zero-point with proper bit-width support\n",
    "def update_scale_and_zero_point(x, num_bits=8):\n",
    "    qmin, qmax = -(1 << (num_bits - 1)), (1 << (num_bits - 1)) - 1\n",
    "\n",
    "    min_val, max_val = x.min().item(), x.max().item()\n",
    "\n",
    "    if min_val == max_val:\n",
    "        scale = 1.0\n",
    "        zero_point = 0\n",
    "    else:\n",
    "        scale = (max_val - min_val) / (qmax - qmin)\n",
    "        zero_point = qmin - min_val / scale\n",
    "        zero_point = torch.round(torch.tensor(zero_point)).int().clamp(qmin, qmax)\n",
    "    \n",
    "    return scale, zero_point\n",
    "\n",
    "# Function to dynamically update scale and zero-point per channel\n",
    "def update_scale_and_zero_point_per_channel(tensor, axis=0, num_bits=8):\n",
    "    qmin, qmax = -(1 << (num_bits - 1)), (1 << (num_bits - 1)) - 1\n",
    "    tensor_min = tensor.min(dim=axis, keepdim=True).values\n",
    "    tensor_max = tensor.max(dim=axis, keepdim=True).values\n",
    "\n",
    "    scale = (tensor_max - tensor_min) / (qmax - qmin)\n",
    "    scale = torch.where(scale == 0, torch.ones_like(scale), scale)\n",
    "    zero_point = torch.round(qmin - tensor_min / scale).int().clamp(qmin, qmax)\n",
    "\n",
    "    return scale.squeeze(dim=axis), zero_point.squeeze(dim=axis)\n",
    "\n",
    "# Quantize tensor with specified bit-width and stochastic rounding\n",
    "def quantize_tensor(tensor, scale, zero_point, num_bits=8, stochastic=False):\n",
    "    qmin, qmax = -(1 << (num_bits - 1)), (1 << (num_bits - 1)) - 1\n",
    "    q_tensor = tensor / scale + zero_point\n",
    "\n",
    "    if stochastic:\n",
    "        q_tensor = torch.floor(q_tensor + torch.rand_like(q_tensor))\n",
    "    else:\n",
    "        q_tensor = torch.round(q_tensor)\n",
    "\n",
    "    q_tensor_new = torch.clamp(q_tensor, qmin, qmax)\n",
    "    return q_tensor_new\n",
    "\n",
    "# Quantize tensor per channel with specified bit-width and stochastic rounding\n",
    "def quantize_per_channel(tensor, axis=0, num_bits=8, stochastic=False):\n",
    "    scales, zero_points = update_scale_and_zero_point_per_channel(tensor, axis=axis, num_bits=num_bits)\n",
    "    quantized_tensor = fake_quantize_per_channel(tensor, scales, zero_points, axis=axis, num_bits=num_bits, stochastic=stochastic)\n",
    "    return quantized_tensor, scales, zero_points\n",
    "\n",
    "# Quantize-and-dequantize operation for analysis and calibration\n",
    "def quantize_and_dequantize(x, scale, zero_point, num_bits=8, stochastic=False):\n",
    "    q_tensor = fake_quantize(x, scale, zero_point, num_bits=num_bits, stochastic=stochastic)\n",
    "    deq_tensor = dequantize_tensor(q_tensor, scale, zero_point)\n",
    "    return deq_tensor\n",
    "\n",
    "# Per-layer quantization analysis with dynamic scale adjustment\n",
    "def per_layer_analysis(layers, inputs):\n",
    "    for i, layer in enumerate(layers):\n",
    "        with torch.no_grad():\n",
    "            outputs = layer(inputs)\n",
    "            analyze_output_distribution(outputs, f\"Layer {i+1} Output Distribution\")\n",
    "\n",
    "            # Update scales and zero-points dynamically during analysis\n",
    "            scale, zero_point = update_scale_and_zero_point(outputs, num_bits=8)\n",
    "            outputs = quantize_and_dequantize(outputs, scale, zero_point)\n",
    "\n",
    "            inputs = outputs\n",
    "\n",
    "# Dequantize tensor back to float\n",
    "def dequantize_tensor(q_tensor, scale, zero_point):\n",
    "    if not isinstance(scale, torch.Tensor):\n",
    "        scale = torch.tensor(scale, dtype=torch.float32, device=q_tensor.device)\n",
    "    if not isinstance(zero_point, torch.Tensor):\n",
    "        zero_point = torch.tensor(zero_point, dtype=torch.float32, device=q_tensor.device)\n",
    "    return (q_tensor.to(torch.float32) - zero_point) * scale\n",
    "\n",
    "# Export model parameters with bit-width support\n",
    "def export_model_parameters(model, filepath):\n",
    "    model_params = {}\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.dim() > 0:  # Ensure we're not trying to quantize a 0-dim tensor\n",
    "            q_tensor, scales, zero_points = quantize_per_channel(param, num_bits=model.num_bits)\n",
    "            model_params[name] = {\n",
    "                \"weights\": q_tensor.tolist(),\n",
    "                \"scales\": scales.tolist(),\n",
    "                \"zero_points\": zero_points.tolist()\n",
    "            }\n",
    "        else:\n",
    "            # For 0-dim tensors, handle separately\n",
    "            model_params[name] = {\n",
    "                \"weights\": [param.item()],\n",
    "                \"scales\": [1.0],\n",
    "                \"zero_points\": [0]\n",
    "            }\n",
    "\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(model_params, f, indent=4)\n",
    "\n",
    "# Log tensor information for debugging\n",
    "def log_tensor_info(tensor, name):\n",
    "    print(f\"{name} shape: {tensor.shape}\")\n",
    "    print(f\"{name} min: {tensor.min()}, max: {tensor.max()}\")\n",
    "    print(f\"{name} mean: {tensor.to(torch.float32).mean()}\")\n",
    "    assert not torch.isnan(tensor).any(), f\"{name} contains NaN values!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom LayerNorm for internal normalization in the LSTMCell\n",
    "class CustomLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(CustomLayerNorm, self).__init__()\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = 1e-5\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        return (x - mean) / torch.sqrt(var + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeQuantizationFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, scale, zero_point, num_bits):\n",
    "        # Forward pass remains the same\n",
    "        ctx.save_for_backward(x, scale, zero_point)\n",
    "        ctx.num_bits = num_bits\n",
    "        qmin, qmax = -(1 << (num_bits - 1)), (1 << (num_bits - 1)) - 1\n",
    "        x_q = torch.round(x / scale + zero_point)\n",
    "        x_q_ = torch.clamp(x_q, qmin, qmax)\n",
    "        x_q_new = (x_q_ - zero_point) * scale\n",
    "        return x_q_new\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, scale, zero_point = ctx.saved_tensors\n",
    "        num_bits = ctx.num_bits\n",
    "        qmin, qmax = -(1 << (num_bits - 1)), (1 << (num_bits - 1)) - 1\n",
    "        x_q = torch.round(x / scale + zero_point)\n",
    "        x_q_clamped = torch.clamp(x_q, qmin, qmax)\n",
    "        grad_input = grad_output / scale\n",
    "        grad_scale = grad_output * (x_q_clamped - zero_point).sum() / scale\n",
    "        grad_zero_point = -grad_output * (x_q_clamped - zero_point).sum()\n",
    "        return grad_input, grad_scale, grad_zero_point, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTpoDSrwJxeA"
   },
   "outputs": [],
   "source": [
    "# Custom LSTM Cell with quantization\n",
    "class CustomLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout_rate=0.0, quantize=False, num_bits=8, stochastic=False):\n",
    "        super(CustomLSTMCell, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.quantize = quantize\n",
    "        self.num_bits = num_bits\n",
    "        self.stochastic = stochastic\n",
    "\n",
    "        self.W_i = nn.Parameter(torch.randn(hidden_dim, input_dim))\n",
    "        self.U_i = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
    "        self.b_i = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.W_f = nn.Parameter(torch.randn(hidden_dim, input_dim))\n",
    "        self.U_f = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
    "        self.b_f = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.W_c = nn.Parameter(torch.randn(hidden_dim, input_dim))\n",
    "        self.U_c = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
    "        self.b_c = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.W_o = nn.Parameter(torch.randn(hidden_dim, input_dim))\n",
    "        self.U_o = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
    "        self.b_o = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.batch_norm = CustomLayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'W_' in name or 'U_' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'b_' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "    def quantize_and_dequantize(self, x, scale, zero_point):\n",
    "        return quantize_and_dequantize(x, scale, zero_point, self.num_bits, stochastic=self.stochastic)\n",
    "\n",
    "    def quantize_weights(self, W):\n",
    "        scale, zero_point = update_scale_and_zero_point_per_channel(W, axis=0, num_bits=self.num_bits)\n",
    "        W_q = fake_quantize_per_channel(W, scale, zero_point, axis=0, num_bits=self.num_bits, stochastic=self.stochastic)\n",
    "        return W_q, scale, zero_point\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        if self.quantize:\n",
    "            # Quantize input x\n",
    "            scale_x, zero_point_x = update_scale_and_zero_point(x, self.num_bits)\n",
    "            x_q = self.quantize_and_dequantize(x, scale_x, zero_point_x)\n",
    "\n",
    "            # Quantize weights\n",
    "            W_i_q, scale_w_i, zero_point_w_i = self.quantize_weights(self.W_i)\n",
    "            U_i_q, scale_u_i, zero_point_u_i = self.quantize_weights(self.U_i)\n",
    "            b_i_q = fake_quantize(self.b_i, scale_w_i.mean(), zero_point_w_i.float().mean(), num_bits=self.num_bits, stochastic=self.stochastic)\n",
    "\n",
    "            W_f_q, scale_w_f, zero_point_w_f = self.quantize_weights(self.W_f)\n",
    "            U_f_q, scale_u_f, zero_point_u_f = self.quantize_weights(self.U_f)\n",
    "            b_f_q = fake_quantize(self.b_f, scale_w_f.mean(), zero_point_w_f.float().mean(), num_bits=self.num_bits, stochastic=self.stochastic)\n",
    "\n",
    "            W_o_q, scale_w_o, zero_point_w_o = self.quantize_weights(self.W_o)\n",
    "            U_o_q, scale_u_o, zero_point_u_o = self.quantize_weights(self.U_o)\n",
    "            b_o_q = fake_quantize(self.b_o, scale_w_o.mean(), zero_point_w_o.float().mean(), num_bits=self.num_bits, stochastic=self.stochastic)\n",
    "\n",
    "            W_c_q, scale_w_c, zero_point_w_c = self.quantize_weights(self.W_c)\n",
    "            U_c_q, scale_u_c, zero_point_u_c = self.quantize_weights(self.U_c)\n",
    "            b_c_q = fake_quantize(self.b_c, scale_w_c.mean(), zero_point_w_c.float().mean(), num_bits=self.num_bits, stochastic=self.stochastic)\n",
    "\n",
    "            # Compute gates\n",
    "            i_t = torch.sigmoid(self.dropout(torch.mm(x_q, W_i_q.float().t()) + torch.mm(h, U_i_q.float().t()) + b_i_q))\n",
    "            f_t = torch.sigmoid(self.dropout(torch.mm(x_q, W_f_q.float().t()) + torch.mm(h, U_f_q.float().t()) + b_f_q))\n",
    "            o_t = torch.sigmoid(self.dropout(torch.mm(x_q, W_o_q.float().t()) + torch.mm(h, U_o_q.float().t()) + b_o_q))\n",
    "            c_hat_t = torch.tanh(self.dropout(torch.mm(x_q, W_c_q.float().t()) + torch.mm(h, U_c_q.float().t()) + b_c_q))\n",
    "\n",
    "        else:\n",
    "            # Without quantization\n",
    "            i_t = torch.sigmoid(self.dropout(torch.mm(x, self.W_i.t()) + torch.mm(h, self.U_i.t()) + self.b_i))\n",
    "            f_t = torch.sigmoid(self.dropout(torch.mm(x, self.W_f.t()) + torch.mm(h, self.U_f.t()) + self.b_f))\n",
    "            o_t = torch.sigmoid(self.dropout(torch.mm(x, self.W_o.t()) + torch.mm(h, self.U_o.t()) + self.b_o))\n",
    "            c_hat_t = torch.tanh(self.dropout(torch.mm(x, self.W_c.t()) + torch.mm(h, self.U_c.t()) + self.b_c))\n",
    "\n",
    "        # Compute the new cell state\n",
    "        c_t = f_t * c + i_t * c_hat_t\n",
    "\n",
    "        # Apply LayerNorm to c_t\n",
    "        c_t = self.batch_norm(c_t)\n",
    "\n",
    "        # Compute the new hidden state\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "        return h_t, c_t\n",
    "\n",
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout_rate=0.0, quantize=False, num_bits=8, stochastic=False):\n",
    "        super(CustomLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.quantize = quantize\n",
    "        self.num_bits = num_bits\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            CustomLSTMCell(input_dim if i == 0 else hidden_dim, hidden_dim, dropout_rate, quantize, num_bits, stochastic=stochastic)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        hidden_states = [self.init_hidden(batch_size) for _ in range(self.num_layers)]\n",
    "    \n",
    "        outputs = []\n",
    "    \n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :] \n",
    "            # Pass through each layer\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                h, c = hidden_states[i]\n",
    "                # For the first layer, use x_t; for subsequent layers, use h from the last\n",
    "                x_t, new_c = layer(x_t, h, c)\n",
    "                \n",
    "                # # Apply per-layer analysis after each layer's output\n",
    "                # analyze_output_distribution(x_t, f\"Layer {i+1} at time step {t+1} Output Distribution\")\n",
    "                \n",
    "                hidden_states[i] = (x_t, new_c)\n",
    "            \n",
    "            outputs.append(x_t.unsqueeze(1))\n",
    "    \n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h = torch.zeros(batch_size, self.hidden_dim).to(next(self.parameters()).device)\n",
    "        c = torch.zeros(batch_size, self.hidden_dim).to(next(self.parameters()).device)\n",
    "        return h, c\n",
    "\n",
    "    def set_quantization_bits(self, num_bits):\n",
    "        self.num_bits = num_bits\n",
    "        for layer in self.layers:\n",
    "            layer.num_bits = num_bits\n",
    "    \n",
    "    def set_quantization(self, quantize):\n",
    "        self.quantize = quantize\n",
    "        for layer in self.layers:\n",
    "            layer.quantize = quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckdaPSaxJxhM"
   },
   "outputs": [],
   "source": [
    "# Custom GRU Cell with quantization support\n",
    "class CustomGRUCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout_rate=0.1, quantize=False, num_bits=8, stochastic=False):\n",
    "        super(CustomGRUCell, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.quantize = quantize\n",
    "        self.num_bits = num_bits\n",
    "        self.stochastic = stochastic\n",
    "\n",
    "        self.W_z = nn.Parameter(torch.randn(hidden_dim, input_dim))\n",
    "        self.U_z = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
    "        self.b_z = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.W_r = nn.Parameter(torch.randn(hidden_dim, input_dim))\n",
    "        self.U_r = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
    "        self.b_r = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.W_h = nn.Parameter(torch.randn(hidden_dim, input_dim))\n",
    "        self.U_h = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
    "        self.b_h = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.batch_norm = CustomLayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'W_' in name or 'U_' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'b_' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "    \n",
    "    def quantize_and_dequantize(self, x, scale, zero_point):\n",
    "        return quantize_and_dequantize(x, scale, zero_point, self.num_bits, stochastic=self.stochastic)\n",
    "\n",
    "    def quantize_weights(self, W):\n",
    "        scale, zero_point = update_scale_and_zero_point_per_channel(W, axis=0, num_bits=self.num_bits)\n",
    "        W_q = fake_quantize_per_channel(W, scale, zero_point, axis=0, num_bits=self.num_bits, stochastic=self.stochastic)\n",
    "        return W_q, scale, zero_point\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        if self.quantize:\n",
    "            # Quantize input x\n",
    "            scale_x, zero_point_x = update_scale_and_zero_point(x, self.num_bits)\n",
    "            x_q = self.quantize_and_dequantize(x, scale_x, zero_point_x)\n",
    "\n",
    "            # Quantize weights\n",
    "            W_z_q, scale_w_z, zero_point_w_z = self.quantize_weights(self.W_z)\n",
    "            U_z_q, scale_u_z, zero_point_u_z = self.quantize_weights(self.U_z)\n",
    "            b_z_q = fake_quantize(self.b_z, scale_w_z.mean(), zero_point_w_z.float().mean(), num_bits=self.num_bits, stochastic=self.stochastic)\n",
    "            \n",
    "            W_r_q, scale_w_r, zero_point_w_r = self.quantize_weights(self.W_r)\n",
    "            U_r_q, scale_u_r, zero_point_u_r = self.quantize_weights(self.U_r)\n",
    "            b_r_q = fake_quantize(self.b_r, scale_w_r.mean(), zero_point_w_r.float().mean(), num_bits=self.num_bits, stochastic=self.stochastic)\n",
    "\n",
    "            W_h_q, scale_w_h, zero_point_w_h = self.quantize_weights(self.W_h)\n",
    "            U_h_q, scale_u_h, zero_point_u_h = self.quantize_weights(self.U_h)\n",
    "            b_h_q = fake_quantize(self.b_h, scale_w_h.mean(), zero_point_w_h.float().mean(), num_bits=self.num_bits, stochastic=self.stochastic)\n",
    "\n",
    "            # Compute gates\n",
    "            z_t = torch.sigmoid(torch.mm(x_q, W_z_q.float().t()) + torch.mm(h, U_z_q.float().t()) + b_z_q)\n",
    "            r_t = torch.sigmoid(torch.mm(x_q, W_r_q.float().t()) + torch.mm(h, U_r_q.float().t()) + b_r_q)\n",
    "            h_tilda = torch.tanh(torch.mm(x_q, W_h_q.float().t()) + r_t * torch.mm(h, U_h_q.float().t()) + b_h_q)\n",
    "        else:\n",
    "            # Without quantization\n",
    "            z_t = torch.sigmoid(torch.mm(x, self.W_z.t()) + torch.mm(h, self.U_z.t()) + self.b_z)\n",
    "            r_t = torch.sigmoid(torch.mm(x, self.W_r.t()) + torch.mm(h, self.U_r.t()) + self.b_r)\n",
    "            h_tilda = torch.tanh(torch.mm(x, self.W_h.t()) + r_t * torch.mm(h, self.U_h.t()) + self.b_h)\n",
    "        \n",
    "        # Compute the new hidden state\n",
    "        h_next = (1 - z_t) * h + z_t * h_tilda\n",
    "\n",
    "        # Apply LayerNorm to h_next\n",
    "        h_next = self.batch_norm(h_next)\n",
    "        \n",
    "        return h_next\n",
    "\n",
    "class CustomGRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout_rate=0.0, quantize=False, num_bits=8, stochastic=False):\n",
    "        super(CustomGRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.quantize = quantize\n",
    "        self.num_bits = num_bits\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            CustomGRUCell(input_dim if i == 0 else hidden_dim, hidden_dim, dropout_rate, quantize, num_bits, stochastic=stochastic)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        hidden_states = [self.init_hidden(batch_size) for _ in range(self.num_layers)]\n",
    "    \n",
    "        outputs = []\n",
    "    \n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :] \n",
    "            # Pass through each layer\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                h = hidden_states[i]\n",
    "                # For the first layer, use x_t; for subsequent layers, use h from the last\n",
    "                x_t = layer(x_t, h)\n",
    "                hidden_states[i] = x_t\n",
    "            # Append output from last layer\n",
    "            outputs.append(x_t)\n",
    "    \n",
    "        # Stack outputs to get the final shape (batch_size, num_steps, hidden_dim)\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_dim)\n",
    "\n",
    "    def set_quantization_bits(self, num_bits):\n",
    "        self.num_bits = num_bits\n",
    "        for layer in self.layers:\n",
    "            layer.num_bits = num_bits\n",
    "\n",
    "    def set_quantization(self, quantize):\n",
    "        self.quantize = quantize\n",
    "        for layer in self.layers:\n",
    "            layer.quantize = quantize\n",
    "\n",
    "\n",
    "class CustomGRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout_rate=0.0, quantize=False, num_bits=8, stochastic=False):\n",
    "        super(CustomGRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.quantize = quantize\n",
    "        self.num_bits = num_bits\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            CustomGRUCell(input_dim if i == 0 else hidden_dim, hidden_dim, dropout_rate, quantize, num_bits, stochastic=stochastic)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        hidden_states = [self.init_hidden(batch_size) for _ in range(self.num_layers)]\n",
    "    \n",
    "        outputs = []\n",
    "    \n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :] \n",
    "            # Pass through each layer\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                h = hidden_states[i]\n",
    "                # For the first layer, use x_t; for subsequent layers, use h from the last\n",
    "                x_t = layer(x_t, h)\n",
    "                hidden_states[i] = x_t\n",
    "            # Append output from last layer\n",
    "            outputs.append(x_t)\n",
    "    \n",
    "        # Stack outputs to get the final shape (batch_size, num_steps, hidden_dim)\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_dim)\n",
    "\n",
    "    def set_quantization_bits(self, num_bits):\n",
    "        self.num_bits = num_bits\n",
    "        for layer in self.layers:\n",
    "            layer.num_bits = num_bits\n",
    "\n",
    "    def set_quantization(self, quantize):\n",
    "        self.quantize = quantize\n",
    "        for layer in self.layers:\n",
    "            layer.quantize = quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, quantize=False, num_bits=8):\n",
    "        super(CustomLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.quantize = quantize\n",
    "        self.num_bits = num_bits\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.quantized = False\n",
    "\n",
    "        self.init_weights()\n",
    "        self.scale_w = None\n",
    "        self.zero_point_w = None\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            nn.init.constant_(self.bias, 0.0)\n",
    "    \n",
    "    def quantize_and_dequantize(self, x, scale, zero_point):\n",
    "        if scale is None or zero_point is None:\n",
    "            raise ValueError(\"Quantization parameters (scale and zero_point) must be set before quantization.\")\n",
    "\n",
    "        if not isinstance(scale, torch.Tensor):\n",
    "            scale = torch.tensor(scale, dtype=torch.float, device=x.device, requires_grad=False)\n",
    "        if not isinstance(zero_point, torch.Tensor):\n",
    "            zero_point = torch.tensor(zero_point, dtype=torch.float, device=x.device, requires_grad=False)\n",
    "            \n",
    "        return FakeQuantizationFunction.apply(x, scale, zero_point, self.num_bits)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape x to 2D: (batch_size * num_steps, input_size)\n",
    "        batch_size, num_steps, _ = x.size()\n",
    "        x = x.view(-1, self.in_features)\n",
    "\n",
    "        if self.quantize:\n",
    "            # Quantize weights and perform matrix multiplication\n",
    "            quantized_weight = self.quantize_and_dequantize(self.weight, self.scale_w, self.zero_point_w)\n",
    "            output = torch.mm(x, quantized_weight.t())\n",
    "        else:\n",
    "            # Use the non-quantized weights\n",
    "            output = torch.mm(x, self.weight.t())\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "\n",
    "        # Reshape output back to 3D: (batch_size, num_steps, out_features)\n",
    "        output = output.view(batch_size, num_steps, self.out_features)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def set_quantization_bits(self, num_bits):\n",
    "        self.num_bits = num_bits  # Set bit precision dynamically\n",
    "    \n",
    "    def set_quantization(self, quantize):\n",
    "        self.quantize = quantize\n",
    "        if self.quantize:\n",
    "            # Initialize quantization parameters (for example, you may set them to specific values or compute them)\n",
    "            self.scale_w = torch.tensor(1.0)  # Set a default value or compute the scale\n",
    "            self.zero_point_w = torch.tensor(0)  # Set a default value or compute the zero point\n",
    "        else:\n",
    "            # Reset quantization parameters when not quantizing\n",
    "            self.scale_w, self.zero_point_w = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNNModel(nn.Module):\n",
    "    def __init__(self, num_steps=20, input_size=10, hidden_units=128, num_layers=3, dropout_rate=0.1, quantize=False, rnn_type='lstm', num_bits=8, stochastic=False):\n",
    "        super(CustomRNNModel, self).__init__()\n",
    "        self.quantize = quantize\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_bits = num_bits\n",
    "        \n",
    "        # Select RNN type\n",
    "        if rnn_type == 'lstm':\n",
    "            self.rnn = CustomLSTM(input_size * num_features, hidden_units, num_layers, dropout_rate=dropout_rate, quantize=quantize, num_bits=num_bits, stochastic=stochastic)\n",
    "        elif rnn_type == 'gru':\n",
    "            self.rnn = CustomGRU(input_size * num_features, hidden_units, num_layers, dropout_rate=dropout_rate, quantize=quantize, num_bits=num_bits)\n",
    "        \n",
    "        self.fc = CustomLinear(hidden_units, input_size, quantize=quantize, num_bits=num_bits)  # CustomLinear layer with quantization\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "        # Assign names to RNN layers for easy quantization handling\n",
    "        for layer_idx, layer in enumerate(self.rnn.layers):\n",
    "            for name, param in layer.named_parameters():\n",
    "                name = name.replace('.', '_')\n",
    "                self.register_buffer(f'rnn{layer_idx}_{name}', param)\n",
    "\n",
    "        # Assign names to linear layers\n",
    "        for name, param in self.fc.named_parameters():\n",
    "            name = name.replace('.', '_')\n",
    "            self.register_buffer(f'fc_{name}', param)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'fc' in name:\n",
    "                if len(param.shape) >= 2:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "            elif 'b' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "    def set_quantization_bits(self, num_bits):\n",
    "        self.quantization_bits = num_bits\n",
    "        self.rnn.set_quantization_bits(num_bits)\n",
    "        self.fc.set_quantization_bits(num_bits)\n",
    "\n",
    "    def quantize(self, x):\n",
    "        scale = 2 ** (self.quantization_bits - 1) - 1\n",
    "        return torch.round(x * scale) / scale\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reshape input to batch_size, num_steps, input_size\n",
    "        batch_size, num_steps, input_size, num_features = x.shape\n",
    "        x = torch.reshape(x, (batch_size, num_steps, input_size * num_features))\n",
    "        \n",
    "        # Pass input through RNN\n",
    "        rnn_output = self.rnn(x)\n",
    "        \n",
    "        # Apply the fully connected layer on the output of the last time step\n",
    "        output = self.fc(rnn_output)\n",
    "\n",
    "        output = torch.reshape(output, (batch_size, num_steps, input_size))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def set_quantization(self, quantize):\n",
    "        self.quantize = quantize\n",
    "        self.rnn.set_quantization(quantize)\n",
    "        self.fc.set_quantization(quantize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stepwise_quantization_schedule(epoch):\n",
    "#     if epoch < 10:\n",
    "#         return 16\n",
    "#     elif epoch < 20:\n",
    "#         return 12\n",
    "#     else:\n",
    "#         return 8\n",
    "\n",
    "# def linear_decay_quantization_schedule(epoch, total_epochs):\n",
    "#     return max(8, 16 - (8 * epoch / total_epochs))\n",
    "\n",
    "# def adaptive_quantization_schedule(epoch, validation_loss, threshold=2e-5):\n",
    "#     if epoch < 10:\n",
    "#         return 16\n",
    "#     elif validation_loss > threshold:  # Define your own threshold\n",
    "#         return 12\n",
    "#     else:\n",
    "#         return 8\n",
    "\n",
    "# def dynamic_quantization_schedule(epoch, total_epochs):\n",
    "#     if epoch < 5:\n",
    "#         return 16\n",
    "#     elif epoch < 20:\n",
    "#         return max(8, 16 - (8 * (epoch - 5) / (total_epochs - 5)))\n",
    "#     else:\n",
    "#         # Optional: adjust based on final performance if needed\n",
    "#         return 8\n",
    "\n",
    "# def exponential_quantization_schedule(epoch, total_epochs):\n",
    "#     decay_rate = 0.1  # Adjust this based on how quickly you want to decay\n",
    "#     min_bits = 8\n",
    "#     max_bits = 16\n",
    "#     return max(min_bits, max_bits * math.exp(-decay_rate * epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_pred, y_true, y_pred_quantized, criterion):\n",
    "    # Calculate standard loss\n",
    "    standard_loss = criterion(y_pred, y_true)\n",
    "    \n",
    "    # Calculate quantization loss\n",
    "    quantization_loss = criterion(y_pred_quantized, y_true)\n",
    "\n",
    "    # Calculate difference between quantized vs non-quantized predictions\n",
    "    quantization_diff = criterion(y_pred_quantized, y_pred)\n",
    "\n",
    "    # Calculate quantization percent error\n",
    "    quant_percent_error = torch.mean(torch.abs((y_pred - y_pred_quantized) / (y_pred + 1e-8)))\n",
    "                    \n",
    "    # Calculate total loss as a weighted sum of standard and quantization losses\n",
    "    total_loss = standard_loss + quantization_loss + quantization_diff \n",
    "\n",
    "    # Return the total loss and the updated alpha\n",
    "    return total_loss, standard_loss, quantization_loss, quantization_diff, quant_percent_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"Save the training model's state as a checkpoint.\"\"\"\n",
    "    print(\"Saving checkpoint...\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"Load a training checkpoint if it exists.\"\"\"\n",
    "    if os.path.isfile(filename):\n",
    "        print(f\"Loading checkpoint '{filename}'\")\n",
    "        checkpoint = torch.load(filename)\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        current_bits = checkpoint['current_bits']\n",
    "        patience = checkpoint['patience']\n",
    "        sparsity_factor = checkpoint.get('sparsity_factor', 0.1) \n",
    "        total_sparsity = checkpoint.get('total_sparsity', 0.0)    \n",
    "        nan_epochs = checkpoint.get('nan_epochs', 0)  \n",
    "        pruning_stopped = checkpoint.get('pruning_stopped', False)  \n",
    "        print(f\"Checkpoint loaded successfully from '{filename}' at epoch {start_epoch}, with {current_bits} bits and sparsity factor {sparsity_factor}\")\n",
    "        return start_epoch, best_val_loss, current_bits, patience, sparsity_factor, total_sparsity, nan_epochs, pruning_stopped\n",
    "    else:\n",
    "        print(f\"No checkpoint found at '{filename}'. Starting from scratch.\")\n",
    "        return 0, float('inf'), None, 0, 0.1, 0.0, 0, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7x14GluDn5Yt"
   },
   "outputs": [],
   "source": [
    "def plot_output_distributions(y_pred, y_pred_quantized, title=\"Output Distribution Comparison\"):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(y_pred, color=\"blue\", label=\"Non-Quantized\", kde=True)\n",
    "    sns.histplot(y_pred_quantized, color=\"red\", label=\"Quantized\", kde=True)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_confusion_matrix(y_true, y_pred, y_pred_quantized):\n",
    "    cm_non_quantized = confusion_matrix(y_true, y_pred.argmax(axis=1))\n",
    "    cm_quantized = confusion_matrix(y_true, y_pred_quantized.argmax(axis=1))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(cm_non_quantized, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(\"Non-Quantized Model\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(cm_quantized, annot=True, fmt=\"d\", cmap=\"Reds\")\n",
    "    plt.title(\"Quantized Model\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gradient_based_pruning(model, sparsity=0.3):\n",
    "    \"\"\"\n",
    "    Apply gradient-based pruning to the weights of LSTM/GRU layers.\n",
    "    :param model: the CustomRNNModel object (LSTM/GRU model)\n",
    "    :param sparsity: the sparsity level to apply (0.3 means 30% of the weights will be pruned)\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, CustomLSTMCell):\n",
    "            for weight_name in ['W_i', 'W_f', 'W_c', 'W_o']:\n",
    "                weight_tensor = getattr(module, weight_name)\n",
    "                \n",
    "                if weight_tensor.requires_grad:\n",
    "                    weight_tensor.retain_grad()\n",
    "\n",
    "                if weight_tensor.grad is not None:\n",
    "                    gradients = torch.abs(weight_tensor.grad)\n",
    "                    threshold = torch.quantile(gradients, sparsity)\n",
    "                    mask = gradients > threshold\n",
    "\n",
    "                    # Apply mask manually to prune weights\n",
    "                    pruned_weight_tensor = weight_tensor * mask.float()\n",
    "\n",
    "                    # Assign pruned weights back to the model\n",
    "                    setattr(module, weight_name, torch.nn.Parameter(pruned_weight_tensor))\n",
    "\n",
    "        elif isinstance(module, CustomGRUCell):\n",
    "            for weight_name in ['W_r', 'W_z', 'W_h']:\n",
    "                weight_tensor = getattr(module, weight_name)\n",
    "\n",
    "                if weight_tensor.requires_grad:\n",
    "                    weight_tensor.retain_grad()\n",
    "\n",
    "                if weight_tensor.grad is not None:\n",
    "                    gradients = torch.abs(weight_tensor.grad)\n",
    "                    threshold = torch.quantile(gradients, sparsity)\n",
    "                    mask = gradients > threshold\n",
    "\n",
    "                    pruned_weight_tensor = weight_tensor * mask.float()\n",
    "                    setattr(module, weight_name, torch.nn.Parameter(pruned_weight_tensor))\n",
    "\n",
    "def apply_magnitude_based_pruning(model, sparsity=0.3):\n",
    "    \"\"\"\n",
    "    Apply magnitude-based pruning to the weights of LSTM/GRU layers.\n",
    "    :param model: the CustomRNNModel object (LSTM/GRU model)\n",
    "    :param sparsity: the sparsity level to apply (0.3 means 30% of the weights will be pruned)\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, CustomLSTMCell):\n",
    "            for weight_name in ['W_i', 'W_f', 'W_c', 'W_o']:\n",
    "                weights = torch.abs(getattr(module, weight_name))\n",
    "                threshold = torch.quantile(weights, sparsity)\n",
    "                mask = weights > threshold\n",
    "                \n",
    "                pruned_weight_tensor = getattr(module, weight_name) * mask.float()\n",
    "                setattr(module, weight_name, torch.nn.Parameter(pruned_weight_tensor))\n",
    "\n",
    "        elif isinstance(module, CustomGRUCell):\n",
    "            for weight_name in ['W_r', 'W_z', 'W_h']:\n",
    "                weights = torch.abs(getattr(module, weight_name))\n",
    "                threshold = torch.quantile(weights, sparsity)\n",
    "                mask = weights > threshold\n",
    "                \n",
    "                pruned_weight_tensor = getattr(module, weight_name) * mask.float()\n",
    "                setattr(module, weight_name, torch.nn.Parameter(pruned_weight_tensor))\n",
    "\n",
    "\n",
    "def apply_combined_pruning(model, grad_sparsity=0.05, mag_sparsity=0.05, sparsity_dict=None, current_sparsity_factor=1.0, target_sparsity=0.6, min_remaining_percentage=39.0):\n",
    "    \"\"\"\n",
    "    Apply gradient-based, magnitude-based, and layer-specific pruning in combination.\n",
    "    The current_sparsity_factor controls the dynamic increase in pruning as training progresses.\n",
    "    Pruning stops once the model reaches close to 32% remaining weights.\n",
    "    \"\"\"\n",
    "    grad_sparsity *= current_sparsity_factor\n",
    "    mag_sparsity *= current_sparsity_factor\n",
    "\n",
    "    # Calculate current model sparsity\n",
    "    actual_sparsity, percent_remaining = calculate_sparsity(model)\n",
    "\n",
    "    # If model has reached or exceeded the minimum remaining percentage, stop pruning\n",
    "    if percent_remaining <= min_remaining_percentage:\n",
    "        print(f\"Reached minimum remaining percentage of {min_remaining_percentage}%. Stopping further pruning.\")\n",
    "        return actual_sparsity, percent_remaining  # Return without further pruning\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, CustomLSTMCell) or isinstance(module, CustomGRUCell):\n",
    "            # Gradient-based pruning\n",
    "            apply_gradient_based_pruning(module, grad_sparsity)\n",
    "\n",
    "            # Magnitude-based pruning\n",
    "            for weight_name in ['W_i', 'W_f', 'W_c', 'W_o'] if isinstance(module, CustomLSTMCell) else ['W_r', 'W_z', 'W_h']:\n",
    "                weights = torch.abs(getattr(module, weight_name))\n",
    "                threshold = torch.quantile(weights, mag_sparsity)\n",
    "                mask = weights > threshold\n",
    "                \n",
    "                pruned_weight_tensor = getattr(module, weight_name) * mask.float()\n",
    "                setattr(module, weight_name, torch.nn.Parameter(pruned_weight_tensor))\n",
    "\n",
    "    # Recalculate the actual sparsity and return the updated values\n",
    "    actual_sparsity, percent_remaining = calculate_sparsity(model)\n",
    "    return actual_sparsity, percent_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sparsity(model):\n",
    "    total_weights = 0\n",
    "    non_zero_weights = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name or 'W_' in name:\n",
    "            total_weights += param.numel()\n",
    "            non_zero_weights += (param != 0).sum().item()  # Check for actual zeros after pruning\n",
    "\n",
    "    sparsity = 1 - (non_zero_weights / total_weights)\n",
    "    remaining_percentage = 100 * (non_zero_weights / total_weights)\n",
    "\n",
    "    print(f'Total Weights: {total_weights}')\n",
    "    print(f'Non-zero Weights: {non_zero_weights}')\n",
    "    print(f'Sparsity (pruned): {sparsity * 100:.2f}%')\n",
    "    print(f'Model remaining: {remaining_percentage:.2f}%')\n",
    "    \n",
    "    return sparsity, remaining_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, config, device, model_path, rnn_type='lstm', \n",
    "                print_freq=10, start_bits=8, end_bits=8, checkpoint_file='checkpoint.pth.tar', \n",
    "                prune=True, grad_sparsity=0.05, mag_sparsity=0.05, target_sparsity=0.6, \n",
    "                sparsity_dict=None, prune_interval=1, min_remaining_percentage=39, max_sparsity_increment=0.01, prune_start_epoch=5):\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['init_learning_rate'], weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=config['learning_rate_decay'])\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    alpha = torch.tensor(1.0)\n",
    "    \n",
    "    print(f\"Max Sparsity Increment: {max_sparsity_increment}\")  \n",
    "\n",
    "    # Load checkpoint\n",
    "    start_epoch, best_val_loss, current_bits, patience, nan_epochs, current_sparsity_factor, total_sparsity, pruning_stopped = load_checkpoint(model, optimizer, scheduler, filename=checkpoint_file)\n",
    "    \n",
    "    if current_bits is None:\n",
    "        current_bits = start_bits\n",
    "\n",
    "    while current_bits >= end_bits:\n",
    "        print(f\"Training with {current_bits} bits\")\n",
    "\n",
    "        model.set_quantization_bits(current_bits)\n",
    "\n",
    "        for epoch in tqdm(range(start_epoch, config['max_epochs']), leave=True):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            nan_detected = False\n",
    "\n",
    "            for i, (x_batch, y_batch) in tqdm(enumerate(train_loader), leave=True):\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    y_pred = model(x_batch)\n",
    "                    model.set_quantization(True)\n",
    "                    y_pred_quantized = model(x_batch)\n",
    "                    model.set_quantization(False)\n",
    "                    \n",
    "                    # Calculate losses\n",
    "                    total_loss, standard_loss, quantization_loss, quantization_diff, quant_percent_error = custom_loss(\n",
    "                        y_pred, y_batch, y_pred_quantized, criterion\n",
    "                    )\n",
    "                \n",
    "                # Handle NaN values and halt pruning\n",
    "                if torch.isnan(total_loss) or torch.isnan(quant_percent_error):\n",
    "                    print(\"NaN detected! Halting pruning increment.\")\n",
    "                    nan_detected = True\n",
    "                    nan_epochs += 1\n",
    "                    break\n",
    "\n",
    "                # Handle NaN in gradients and skip this iteration\n",
    "                for param in model.parameters():\n",
    "                    if param.grad is not None:\n",
    "                        if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "                            print(\"NaN or Inf found in gradients! Skipping this step.\")\n",
    "                            nan_detected = True\n",
    "                            nan_epochs += 1\n",
    "                            continue\n",
    "                        # Ensure pruned weights remain zero\n",
    "                        mask = param == 0\n",
    "                        param.grad[mask] = 0\n",
    "\n",
    "                # Apply backpropagation and updates\n",
    "                scaler.scale(total_loss).backward(retain_graph=True)\n",
    "\n",
    "                if prune and not pruning_stopped and epoch >= prune_start_epoch:\n",
    "                    # Check the sparsity before every step\n",
    "                    actual_sparsity, percent_remaining = calculate_sparsity(model)\n",
    "                    print(f\"Checking sparsity at Step [{i+1}/{len(train_loader)}]\")\n",
    "                    print(f\"Current model sparsity: {actual_sparsity * 100:.2f}%, Remaining: {percent_remaining:.2f}%\")\n",
    "\n",
    "                    # If model has reached or exceeded the minimum remaining percentage, stop pruning\n",
    "                    if percent_remaining <= min_remaining_percentage:\n",
    "                        print(f\"Reached minimum remaining percentage of {min_remaining_percentage}%. Stopping further pruning.\")\n",
    "                        pruning_stopped = True  # Stop any future pruning\n",
    "                    else:\n",
    "                        print(f\"Pruning step. Current sparsity factor: {current_sparsity_factor}\")\n",
    "                        actual_sparsity, percent_remaining = apply_combined_pruning(\n",
    "                            model, grad_sparsity=grad_sparsity, mag_sparsity=mag_sparsity,\n",
    "                            sparsity_dict=sparsity_dict, current_sparsity_factor=current_sparsity_factor\n",
    "                        )\n",
    "                        print(f\"After pruning, Actual Sparsity: {actual_sparsity * 100:.2f}%, Remaining: {percent_remaining:.2f}%\")\n",
    "                        \n",
    "                        # Increment the sparsity factor for the next pruning step\n",
    "                        current_sparsity_factor += max_sparsity_increment\n",
    "                        total_sparsity += current_sparsity_factor\n",
    "                        print(f\"Updated Current Sparsity Factor: {current_sparsity_factor}\")\n",
    "                        print(f\"Updated Total Sparsity: {total_sparsity}\")\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                train_loss += total_loss.item()\n",
    "            \n",
    "                if i % print_freq == 0:                    \n",
    "                    print(f'Epoch [{epoch + 1}/{config[\"max_epochs\"]}], Step [{i + 1}/{len(train_loader)}], Loss: {total_loss.item()}')\n",
    "                    print(f'Alpha: {alpha.item()}')\n",
    "                    print(f'Sparsity factor: {current_sparsity_factor}')\n",
    "                    print(f'Total sparsity: {total_sparsity}')\n",
    "                    print(f'NaN epochs: {nan_epochs}')\n",
    "\n",
    "                    # Recalculate the sparsity after each iteration\n",
    "                    actual_sparsity, percent_remaining = calculate_sparsity(model)\n",
    "                    print(f\"Actual model sparsity: {actual_sparsity * 100:.2f}%\")\n",
    "                    print(f\"Percentage remaining: {percent_remaining:.2f}%\")\n",
    "                    \n",
    "                    print(f'Patience: {patience}')\n",
    "                    print('Training Loss Standard:', standard_loss.item())\n",
    "                    print('Training Loss Quantized:', quantization_loss.item())\n",
    "                    print('Training Quantization Diff:', quantization_diff.item())\n",
    "                    print(f'Training Quant Percent Diff in Outputs: {100 * quant_percent_error.item()}%')\n",
    "                    print(f'Training Percent Diff in Loss: {100 * (quantization_loss - standard_loss) / (standard_loss + 1e-8).item()}%')\n",
    "                    print('Training classification accuracy standard:', classification_accuracy(y_pred, y_batch).item())\n",
    "                    print('Training classification accuracy quantized:', classification_accuracy(y_pred_quantized, y_batch).item())\n",
    "\n",
    "            if nan_detected:\n",
    "                current_sparsity_factor = max(0.0, current_sparsity_factor - max_sparsity_increment / 2)  # Reduce sparsity increment if NaNs occur\n",
    "\n",
    "            # Validation and checkpoint logic\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i, (x_batch, y_batch) in tqdm(enumerate(val_loader), position=0, leave=True):\n",
    "                    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        y_pred = model(x_batch)\n",
    "                        model.set_quantization(True)\n",
    "                        y_pred_quantized = model(x_batch)\n",
    "                        model.set_quantization(False)\n",
    "\n",
    "                        total_loss, standard_loss, quantization_loss, quantization_diff, quant_percent_error = custom_loss(\n",
    "                            y_pred, y_batch, y_pred_quantized, criterion\n",
    "                        )\n",
    "                    \n",
    "                    val_loss += total_loss.item()\n",
    "\n",
    "                    if i % print_freq == 0:\n",
    "                        print(f'Epoch [{epoch + 1}/{config[\"max_epochs\"]}], Step [{i + 1}/{len(val_loader)}], Loss: {total_loss.item()}')\n",
    "                        print(f'Alpha: {alpha.item()}')\n",
    "                        print(f'Sparsity factor: {current_sparsity_factor}')\n",
    "                        print(f'Total sparsity: {total_sparsity}')\n",
    "                        print(f'NaN epochs: {nan_epochs}')\n",
    "                        print(f'Patience: {patience}')\n",
    "                        print('Validation Loss Standard:', standard_loss.item())\n",
    "                        print('Validation Loss Quantized:', quantization_loss.item())\n",
    "                        print('Validation Quantization Diff:', quantization_diff.item())\n",
    "                        print(f'Validation Quant Percent Diff in Outputs: {100 * quant_percent_error.item()}%')\n",
    "                        print(f'Validation Percent Diff in Loss: {100 * (quantization_loss - standard_loss) / (standard_loss + 1e-8).item()}%')\n",
    "                        print('Validation classification accuracy standard:', classification_accuracy(y_pred, y_batch).item())\n",
    "                        print('Validation classification accuracy quantized:', classification_accuracy(y_pred_quantized, y_batch).item())\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            print(f'Epoch [{epoch + 1}/{config[\"max_epochs\"]}], Validation Loss: {val_loss}')\n",
    "\n",
    "            # Save checkpoint if validation improves\n",
    "            is_best = val_loss < best_val_loss\n",
    "            if is_best:\n",
    "                best_val_loss = val_loss\n",
    "                patience = 0\n",
    "                print('Saving model with best validation loss...')\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= config['early_stop_patience']:\n",
    "                    print(\"Early stopping due to lack of improvement.\")\n",
    "                    break\n",
    "            \n",
    "            # Save the current training state and pruning progress\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'current_bits': current_bits,\n",
    "                'patience': patience,\n",
    "                'sparsity_factor': current_sparsity_factor,  # Save current sparsity factor\n",
    "                'total_sparsity': total_sparsity,            # Save total sparsity applied\n",
    "                'nan_epochs': nan_epochs,                    # Save nan_epoch count\n",
    "                'pruning_stopped': pruning_stopped           # Save the pruning stopped flag\n",
    "            }, filename=checkpoint_file)\n",
    "            \n",
    "            scheduler.step()\n",
    "\n",
    "        current_bits -= 1\n",
    "        start_epoch = 0\n",
    "\n",
    "    print('Training complete.')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noAjxW3sppPC"
   },
   "outputs": [],
   "source": [
    "def load_model(model, model_path, device):\n",
    "    # Load the original state dictionary from disk\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFfMKnlIPjmo"
   },
   "outputs": [],
   "source": [
    "def convert_tensor_to_sparse(tensor):\n",
    "    \"\"\"\n",
    "    Converts a dense tensor to a sparse tensor by retaining only non-zero values.\n",
    "    :param tensor: The dense tensor to be converted.\n",
    "    :return: A sparse version of the tensor.\n",
    "    \"\"\"\n",
    "    if tensor.is_cuda:\n",
    "        tensor = tensor.cpu()  # Convert to CPU if it's on GPU (sparse tensors are CPU-based)\n",
    "    \n",
    "    sparse_tensor = tensor.to_sparse()\n",
    "    return sparse_tensor\n",
    "\n",
    "def convert_model_to_sparse(model):\n",
    "    \"\"\"\n",
    "    Converts the pruned model's dense weights to sparse format to save memory and improve efficiency.\n",
    "    :param model: The pruned model with dense weight matrices.\n",
    "    :return: The same model with sparse weights for the pruned layers.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, CustomLSTMCell) or isinstance(module, CustomGRUCell):\n",
    "            # Convert pruned weights to sparse format\n",
    "            for weight_name in ['W_i', 'W_f', 'W_c', 'W_o']:\n",
    "                dense_weight = getattr(module, weight_name)\n",
    "                if hasattr(module, f'{weight_name}_mask'):\n",
    "                    sparse_weight = convert_tensor_to_sparse(dense_weight)\n",
    "                    # Replace the original weight with the sparse tensor\n",
    "                    setattr(module, weight_name, sparse_weight)\n",
    "                    # Optionally remove the mask to avoid confusion\n",
    "                    delattr(module, f'{weight_name}_mask')\n",
    "                    \n",
    "            if isinstance(module, CustomGRUCell):\n",
    "                for weight_name in ['W_r', 'W_z', 'W_h']:\n",
    "                    dense_weight = getattr(module, weight_name)\n",
    "                    if hasattr(module, f'{weight_name}_mask'):\n",
    "                        sparse_weight = convert_tensor_to_sparse(dense_weight)\n",
    "                        setattr(module, weight_name, sparse_weight)\n",
    "                        delattr(module, f'{weight_name}_mask')\n",
    "\n",
    "    print(\"Model successfully converted to sparse format.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_or_load_model(configs, train_loader, val_loader, rnn_type, device, model_path, prune=False, grad_sparsity=0.05, mag_sparsity=0.05, target_sparsity=0.7, prune_interval=1, min_remaining_percentage=32, prune_start_epoch=5, checkpoint_file='checkpoint.pth.tar'):\n",
    "    # Define model & load into device\n",
    "    model = CustomRNNModel(rnn_type=rnn_type, num_steps=configs['num_steps'], input_size=configs['input_size'], hidden_units=configs[rnn_type][f'{rnn_type}_units'], num_layers=configs[rnn_type][f'{rnn_type}_layers'], dropout_rate=configs[rnn_type]['dropout_rate'], stochastic=True)\n",
    "    model.to(device)\n",
    "\n",
    "    # Train the model and save checkpoints\n",
    "    if configs[rnn_type]['pretrain']:\n",
    "        # Keep a copy of whether we originally set to prune the model\n",
    "        prune_orig = prune\n",
    "\n",
    "        # Apply magnitude-based and layer-specific pruning before training\n",
    "        if prune:\n",
    "            sparsity_dict = {'rnn.layers.0': 0.2, 'rnn.layers.1': 0.35, 'rnn.layers.2': 0.45}  # Layer-specific sparsity levels\n",
    "        \n",
    "        model = train_model(model, train_loader, val_loader, configs[rnn_type], device, model_path, prune=prune, grad_sparsity=grad_sparsity, mag_sparsity=mag_sparsity, target_sparsity=target_sparsity, prune_interval=prune_interval, min_remaining_percentage=min_remaining_percentage, sparsity_dict=sparsity_dict, prune_start_epoch=prune_start_epoch, rnn_type=rnn_type, checkpoint_file=checkpoint_file)\n",
    "        \n",
    "        # Convert pruned model to sparse format after training, but keep sparsity.\n",
    "        if prune_orig:\n",
    "            convert_model_to_sparse(model)\n",
    "\n",
    "    # Load the model from the latest checkpoint\n",
    "    model = load_model(model, model_path, device)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1-42q_WQxoq"
   },
   "outputs": [],
   "source": [
    "def mean_percent_diff(quant_outputs, noquant_outputs):\n",
    "    total_abs_diff = 0.0\n",
    "    total_elements = 0\n",
    "    \n",
    "    # Iterate through each batch in the list\n",
    "    for i in range(len(noquant_outputs)):\n",
    "        noquant = noquant_outputs[i]\n",
    "        quant = quant_outputs[i]\n",
    "        \n",
    "        # Ensure shapes match\n",
    "        if noquant.shape != quant.shape:\n",
    "            raise ValueError(f\"Shape mismatch: {noquant.shape} vs {quant.shape}\")\n",
    "        \n",
    "        # Calculate absolute differences\n",
    "        abs_diff = np.abs(noquant - quant)\n",
    "        \n",
    "        # Calculate percent differences\n",
    "        percent_diff = (abs_diff / (np.abs(noquant) + 1e-8)) * 100\n",
    "        \n",
    "        # Aggregate differences\n",
    "        total_abs_diff += np.sum(percent_diff)\n",
    "        total_elements += np.size(percent_diff)\n",
    "    \n",
    "    # Calculate mean percent difference\n",
    "    percent_diff = total_abs_diff / total_elements   \n",
    "    return percent_diff\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, test_loader, device, quantize=False):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Apply quantization if specified\n",
    "    model.set_quantization(quantize)\n",
    "\n",
    "    test_loss = 0.0\n",
    "    test_accuracy = 0.0\n",
    "    \n",
    "    preds = []\n",
    "\n",
    "    # Disable gradient computation during evaluation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over test set data\n",
    "        for i, (x_batch, y_batch) in tqdm(enumerate(test_loader), position=0, leave=True):\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(x_batch)\n",
    "            loss = nn.MSELoss()(y_pred, y_batch)\n",
    "            preds.append(y_pred.cpu().numpy())  # Ensure data is on CPU for numpy operations\n",
    "            test_loss += loss.item()\n",
    "            test_accuracy += classification_accuracy(y_pred, y_batch).item()\n",
    "    \n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_accuracy = test_accuracy / len(test_loader)\n",
    "\n",
    "    print(\"Test Loss:\", test_loss)\n",
    "    print('Test Classification Accuracy:', test_accuracy)\n",
    "    \n",
    "    return preds, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtfCK8cen5dZ"
   },
   "outputs": [],
   "source": [
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset configs\n",
    "configs = load_configs('configs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXCWMY1ERPb4"
   },
   "source": [
    "## Training + Evaluation Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM (without pruning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rnn_type = 'lstm'\n",
    "model_path = f'models/SC2010_{rnn_type}_model_QAT.pth'\n",
    "checkpoint_path = f'checkpoints/SC2010_{rnn_type}_checkpoint_QAT.pth'\n",
    "model_params_path = f'models/SC2010_quantized_{rnn_type}_parameters_QAT.json'\n",
    "\n",
    "# Training\n",
    "model_lstm = train_or_load_model(configs, train_loader, val_loader, rnn_type, device, model_path, checkpoint_file=checkpoint_path)\n",
    "\n",
    "# Export model parameters to JSON file\n",
    "export_model_parameters(model_lstm, model_params_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "95eca35a46a14aa4abf7c91b1ccee065",
      "01289697a46a4749808002f2a6eaaf73",
      "9ff6b3cf75b746f5a11dc7a7eadc2009",
      "e86b3864d880446081ae53689493f24e",
      "c275aca6f2c84125923cbd7246786078",
      "b1f95063a92b48e0b5ca86e7a6716236",
      "abf1a93ee34843578018f72eeb64a9ea",
      "18429f5771a24ae1a72150c16e7f97d3",
      "a583468ab883499e9d8cc9dadac9c597",
      "497e7dd3069a4b5dbc883e4695332963",
      "5ab32519140341eebad1e15228deb3e2",
      "6b7700e80d854914ad4f75c34e338d3e",
      "77b06e361d74438fad7a17c8f34c0aa1",
      "c2ba33ed9a314536bffb427470ee1957",
      "b15dd251352142d39fb0c1a46cf9b388",
      "2ee881decbf04d1cbc933c20ab8bc714",
      "e879cb6793ff45e4886866587f71db62",
      "08f0ea7c403040968f079e0e694286b7",
      "04aac2c8d09544f88e1b50f7ac048ca7",
      "22f0476c8b2f48a8868bd2216ed4d7cf",
      "abc2dd3740504f31bcc05d4ca007e658",
      "d0ab580d14614370b83ce50e456b1e75"
     ]
    },
    "id": "O3dmAEtuQ9l5",
    "outputId": "b4634db3-939c-4a67-c6f8-8975af1ed448"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf7475907844e6c8e5feda9106d263f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.961668281832577e-05\n",
      "Test Classification Accuracy: 0.9756905862263271\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "test_preds_lstm_noquant, test_loss_lstm_noquant, test_accuracy_lstm_noquant = evaluate_model(model_lstm, test_loader, device, quantize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a725c0feaf41cba1c04609e2a1446c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.9414796042838134e-05\n",
      "Test Classification Accuracy: 0.9756949033055987\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "test_preds_lstm_quant, test_loss_lstm_quant, test_accuracy_lstm_quant = evaluate_model(model_lstm, test_loader, device, quantize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM (with pruning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_type = 'lstm'\n",
    "model_path = f'models/SC2010_{rnn_type}_model_QAT_sparsity.pth'\n",
    "checkpoint_path = f'checkpoints/SC2010_{rnn_type}_checkpoint_QAT_sparsity.pth'\n",
    "model_params_path = f'models/SC2010_quantized_{rnn_type}_parameters_QAT_sparsity.json'\n",
    "\n",
    "# Training\n",
    "configs[rnn_type]['pretrain'] = True\n",
    "model_lstm_sparsify = train_or_load_model(configs, train_loader, val_loader, rnn_type, device, model_path, prune=True, grad_sparsity=0.05, mag_sparsity=0.05, target_sparsity=0.6, prune_interval=1, min_remaining_percentage=39, prune_start_epoch=5, checkpoint_file=checkpoint_path)\n",
    "configs[rnn_type]['pretrain'] = False\n",
    "\n",
    "# Export model parameters to JSON file\n",
    "export_model_parameters(model_lstm_sparsify, model_params_path)\n",
    "\n",
    "# Calculate sparsity\n",
    "sparsity, remaining_percentage = calculate_sparsity(model_lstm_sparsify)\n",
    "print(f'Final model sparsity: {sparsity * 100:.2f}%')\n",
    "print(f'Model is {remaining_percentage:.2f}% of its original size after pruning.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "test_preds_lstm_noquant_sparsify, test_loss_lstm_noquant_sparsify, test_accuracy_lstm_noquant_sparsify = evaluate_model(model_lstm_sparsify, test_loader, device, quantize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/michaelzhou/miniforge3/envs/tf_m1/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/michaelzhou/miniforge3/envs/tf_m1/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/michaelzhou/miniforge3/envs/tf_m1/lib/python3.8/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/michaelzhou/miniforge3/envs/tf_m1/lib/python3.8/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/michaelzhou/miniforge3/envs/tf_m1/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/michaelzhou/miniforge3/envs/tf_m1/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/michaelzhou/miniforge3/envs/tf_m1/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/michaelzhou/miniforge3/envs/tf_m1/lib/python3.8/asyncio/base_events.py\", line 1823, in _run_once\n",
      "    event_list = self._selector.select(timeout)\n",
      "  File \"/Users/michaelzhou/miniforge3/envs/tf_m1/lib/python3.8/selectors.py\", line 558, in select\n",
      "    kev_list = self._selector.control(None, max_ev, timeout)\n",
      "  File \"/Users/michaelzhou/miniforge3/envs/tf_m1/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 21993) is killed by signal: Interrupt: 2. \n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "test_preds_lstm_quant_sparsify, test_loss_lstm_quant_sparsify, test_accuracy_lstm_quant_sparsify = evaluate_model(model_lstm_sparsify, test_loader, device, quantize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bliBob-oRT-A"
   },
   "source": [
    "### GRU (without pruning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rnn_type = 'gru'\n",
    "model_path = f'models/SC2010_{rnn_type}_model_QAT.pth'\n",
    "checkpoint_path = f'checkpoints/SC2010_{rnn_type}_checkpoint_QAT.pth'\n",
    "model_params_path = f'models/SC2010_quantized_{rnn_type}_parameters_QAT.json'\n",
    "\n",
    "# Training\n",
    "model_gru = train_or_load_model(configs, train_loader, val_loader, rnn_type, device, model_path, checkpoint_file=checkpoint_path)\n",
    "\n",
    "# Export model parameters to JSON file\n",
    "export_model_parameters(model_gru, model_params_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "yItI-2tcPEZf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e3a572874f483581749c106318eee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.150141413689458e-05\n",
      "Test Classification Accuracy: 0.9751477667263576\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "test_preds_gru_noquant, test_loss_gru_noquant, test_accuracy_gru_noquant = evaluate_model(model_gru, test_loader, device, quantize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a39eed663bd4fe0bbe126df3c3f401f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.941503926770695e-05\n",
      "Test Classification Accuracy: 0.9756949033055987\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "test_preds_gru_quant, test_loss_gru_quant, test_accuracy_gru_quant = evaluate_model(model_gru, test_loader, device, quantize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU (with pruning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Percent Difference between outputs (non-quantized vs quantized) GRU: 127.71%\n"
     ]
    }
   ],
   "source": [
    "rnn_type = 'gru'\n",
    "model_path = f'models/SC2010_{rnn_type}_model_QAT_sparsity.pth'\n",
    "checkpoint_path = f'checkpoints/SC2010_{rnn_type}_checkpoint_QAT_sparsity.pth'\n",
    "model_params_path = f'models/SC2010_quantized_{rnn_type}_parameters_QAT_sparsity.json'\n",
    "\n",
    "# Training\n",
    "configs[rnn_type]['pretrain'] = True\n",
    "model_gru_sparsify = train_or_load_model(configs, train_loader, val_loader, rnn_type, device, model_path, prune=True, grad_sparsity=0.05, mag_sparsity=0.05, target_sparsity=0.6, prune_interval=1, min_remaining_percentage=39, checkpoint_file=checkpoint_path)\n",
    "configs[rnn_type]['pretrain'] = False\n",
    "\n",
    "# Export model parameters to JSON file\n",
    "export_model_parameters(model_gru_sparsify, model_params_path)\n",
    "\n",
    "# Calculate sparsity\n",
    "sparsity, remaining_percentage = calculate_sparsity(model_gru_sparsify)\n",
    "print(f'Final model sparsity: {sparsity * 100:.2f}%')\n",
    "print(f'Model is {remaining_percentage:.2f}% of its original size after pruning.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "test_preds_gru_noquant_sparsify, test_loss_gru_noquant_sparsify, test_accuracy_gru_noquant_sparsify = evaluate_model(model_gru_sparsify, test_loader, device, quantize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "test_preds_gru_quant_sparsify, test_loss_gru_quant_sparsify, test_accuracy_gru_quant_sparsify = evaluate_model(model_gru_sparsify, test_loader, device, quantize=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01289697a46a4749808002f2a6eaaf73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1f95063a92b48e0b5ca86e7a6716236",
      "placeholder": "",
      "style": "IPY_MODEL_abf1a93ee34843578018f72eeb64a9ea",
      "value": "0%"
     }
    },
    "04aac2c8d09544f88e1b50f7ac048ca7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "08f0ea7c403040968f079e0e694286b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "18429f5771a24ae1a72150c16e7f97d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22f0476c8b2f48a8868bd2216ed4d7cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2ee881decbf04d1cbc933c20ab8bc714": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "497e7dd3069a4b5dbc883e4695332963": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ab32519140341eebad1e15228deb3e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6b7700e80d854914ad4f75c34e338d3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_77b06e361d74438fad7a17c8f34c0aa1",
       "IPY_MODEL_c2ba33ed9a314536bffb427470ee1957",
       "IPY_MODEL_b15dd251352142d39fb0c1a46cf9b388"
      ],
      "layout": "IPY_MODEL_2ee881decbf04d1cbc933c20ab8bc714"
     }
    },
    "77b06e361d74438fad7a17c8f34c0aa1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e879cb6793ff45e4886866587f71db62",
      "placeholder": "",
      "style": "IPY_MODEL_08f0ea7c403040968f079e0e694286b7",
      "value": ""
     }
    },
    "95eca35a46a14aa4abf7c91b1ccee065": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_01289697a46a4749808002f2a6eaaf73",
       "IPY_MODEL_9ff6b3cf75b746f5a11dc7a7eadc2009",
       "IPY_MODEL_e86b3864d880446081ae53689493f24e"
      ],
      "layout": "IPY_MODEL_c275aca6f2c84125923cbd7246786078"
     }
    },
    "9ff6b3cf75b746f5a11dc7a7eadc2009": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18429f5771a24ae1a72150c16e7f97d3",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a583468ab883499e9d8cc9dadac9c597",
      "value": 0
     }
    },
    "a583468ab883499e9d8cc9dadac9c597": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "abc2dd3740504f31bcc05d4ca007e658": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abf1a93ee34843578018f72eeb64a9ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b15dd251352142d39fb0c1a46cf9b388": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_abc2dd3740504f31bcc05d4ca007e658",
      "placeholder": "",
      "style": "IPY_MODEL_d0ab580d14614370b83ce50e456b1e75",
      "value": "0/?[00:00&lt;?,?it/s]"
     }
    },
    "b1f95063a92b48e0b5ca86e7a6716236": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c275aca6f2c84125923cbd7246786078": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2ba33ed9a314536bffb427470ee1957": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04aac2c8d09544f88e1b50f7ac048ca7",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_22f0476c8b2f48a8868bd2216ed4d7cf",
      "value": 0
     }
    },
    "d0ab580d14614370b83ce50e456b1e75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e86b3864d880446081ae53689493f24e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_497e7dd3069a4b5dbc883e4695332963",
      "placeholder": "",
      "style": "IPY_MODEL_5ab32519140341eebad1e15228deb3e2",
      "value": "0/50[00:00&lt;?,?it/s]"
     }
    },
    "e879cb6793ff45e4886866587f71db62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
