{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "9d843354-e8d6-40c7-99ae-8c4e21d4016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aea98f5-1b54-4ee2-bb03-40146c87e8af",
   "metadata": {},
   "source": [
    "## Model Training + Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "944434c1-b00e-45cb-948b-edee1c4b6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(CustomLinear, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        \n",
    "        # Initialize weights and bias using Xavier/Glorot initialization\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.constant_(self.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x, self.weight.t()) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "b9fa2a21-0e33-4643-b0c9-259479854354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple model with a single fully connected layer for regression\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = CustomLinear(10, 1)  \n",
    "\n",
    "        # Assign names to linear layers\n",
    "        for name, param in self.fc.named_parameters():\n",
    "            self.register_buffer(f'fc_{name}', param)\n",
    "\n",
    "        # Initialize weights and bias using Xavier/Glorot initialization\n",
    "        nn.init.xavier_uniform_(self.fc.weight.data)\n",
    "        nn.init.constant_(self.fc.bias.data, 0.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "1d4c3b42-296f-4ba4-8608-f8afaa41ceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "def generate_synthetic_data(num_samples, input_size):\n",
    "    inputs = torch.randn(num_samples, input_size)\n",
    "    targets = torch.randint(0, 5, (num_samples,))\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "dc7682b1-30f6-4085-9358-e8026a76ade9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model:\n",
      "SimpleModel(\n",
      "  (fc): CustomLinear()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = SimpleModel()\n",
    "print(\"Original Model:\")\n",
    "print(model)\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = generate_synthetic_data(100000, 10)\n",
    "dataset = TensorDataset(X, y)\n",
    "data_loader = DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "f2bfbdf2-25cb-4742-9456-3d5b35f1a238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9r/mtq2vc395jd2kf7n3d7l_b9r0000gn/T/ipykernel_3996/2676978537.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
      "/var/folders/9r/mtq2vc395jd2kf7n3d7l_b9r0000gn/T/ipykernel_3996/2676978537.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
      "/var/folders/9r/mtq2vc395jd2kf7n3d7l_b9r0000gn/T/ipykernel_3996/2676978537.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
      "/var/folders/9r/mtq2vc395jd2kf7n3d7l_b9r0000gn/T/ipykernel_3996/2676978537.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create TensorDataset objects for train and test data\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoader objects for train and test datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "9b2a40ea-b4b4-48c9-ae2d-46bb603b4d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_model(model, data_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    with torch.no_grad():\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0.0\n",
    "            for inputs, targets in data_loader:\n",
    "                optimizer.zero_grad()  # Zero the gradients\n",
    "    \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "    \n",
    "                # Backward pass and optimization\n",
    "                loss.requires_grad = True\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                total_loss += loss.item()\n",
    "    \n",
    "            avg_loss = total_loss / len(data_loader)\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "3ab9c6c2-4cf5-4d56-bb64-5654fb45673b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 8.6553\n",
      "Epoch [2/50], Loss: 8.6505\n",
      "Epoch [3/50], Loss: 8.6477\n",
      "Epoch [4/50], Loss: 8.6503\n",
      "Epoch [5/50], Loss: 8.6474\n",
      "Epoch [6/50], Loss: 8.6535\n",
      "Epoch [7/50], Loss: 8.6494\n",
      "Epoch [8/50], Loss: 8.6494\n",
      "Epoch [9/50], Loss: 8.6536\n",
      "Epoch [10/50], Loss: 8.6471\n",
      "Epoch [11/50], Loss: 8.6462\n",
      "Epoch [12/50], Loss: 8.6487\n",
      "Epoch [13/50], Loss: 8.6475\n",
      "Epoch [14/50], Loss: 8.6518\n",
      "Epoch [15/50], Loss: 8.6502\n",
      "Epoch [16/50], Loss: 8.6545\n",
      "Epoch [17/50], Loss: 8.6555\n",
      "Epoch [18/50], Loss: 8.6471\n",
      "Epoch [19/50], Loss: 8.6533\n",
      "Epoch [20/50], Loss: 8.6571\n",
      "Epoch [21/50], Loss: 8.6499\n",
      "Epoch [22/50], Loss: 8.6506\n",
      "Epoch [23/50], Loss: 8.6481\n",
      "Epoch [24/50], Loss: 8.6537\n",
      "Epoch [25/50], Loss: 8.6519\n",
      "Epoch [26/50], Loss: 8.6510\n",
      "Epoch [27/50], Loss: 8.6491\n",
      "Epoch [28/50], Loss: 8.6475\n",
      "Epoch [29/50], Loss: 8.6483\n",
      "Epoch [30/50], Loss: 8.6524\n",
      "Epoch [31/50], Loss: 8.6499\n",
      "Epoch [32/50], Loss: 8.6501\n",
      "Epoch [33/50], Loss: 8.6530\n",
      "Epoch [34/50], Loss: 8.6506\n",
      "Epoch [35/50], Loss: 8.6454\n",
      "Epoch [36/50], Loss: 8.6464\n",
      "Epoch [37/50], Loss: 8.6521\n",
      "Epoch [38/50], Loss: 8.6443\n",
      "Epoch [39/50], Loss: 8.6496\n",
      "Epoch [40/50], Loss: 8.6464\n",
      "Epoch [41/50], Loss: 8.6466\n",
      "Epoch [42/50], Loss: 8.6557\n",
      "Epoch [43/50], Loss: 8.6490\n",
      "Epoch [44/50], Loss: 8.6543\n",
      "Epoch [45/50], Loss: 8.6533\n",
      "Epoch [46/50], Loss: 8.6502\n",
      "Epoch [47/50], Loss: 8.6479\n",
      "Epoch [48/50], Loss: 8.6484\n",
      "Epoch [49/50], Loss: 8.6510\n",
      "Epoch [50/50], Loss: 8.6484\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer for regression\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "9dce0a6a-8c6a-4578-933e-989fbaf5dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in data_loader:\n",
    "            outputs = model(data)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"Test Loss (MSE): {avg_loss:.4f}\")\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "7f0e7be7-cefa-44f4-83aa-ffa418092991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss (MSE): 8.6669\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "test_loss_fc_noquant = evaluate_model(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d1dc2c-fb7a-4db2-8813-3cd3d41e396f",
   "metadata": {},
   "source": [
    "## Add PTSQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "c2cf8142-2961-441b-bb03-793336aad668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a subset of indices to use for calibration\n",
    "def get_calibration_subset_indices(dataset, subset_size=1000):\n",
    "    \"\"\"Select a subset of indices for calibration.\"\"\"\n",
    "    # Randomly sample indices\n",
    "    indices = np.random.choice(len(dataset), size=subset_size, replace=False)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "00da5800-504c-4bc8-ab3d-aec3ea79ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get calibration subset indices\n",
    "calibration_indices = get_calibration_subset_indices(train_dataset, subset_size=int(len(train_dataset) * 0.1))\n",
    "\n",
    "# Create a subset of the dataset\n",
    "calibration_subset = Subset(train_dataset, calibration_indices)\n",
    "\n",
    "# Create a DataLoader for the calibration subset\n",
    "calibration_loader = DataLoader(calibration_subset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "366fea5a-33ad-454f-ba45-ed147f9bf555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_activations(model, dataloader, device):\n",
    "    activations = {}\n",
    "\n",
    "    def forward_hook(module, input, output):\n",
    "        if module is model.fc:\n",
    "            module_name = \"fc_output\"\n",
    "            if isinstance(output, tuple):\n",
    "                for i, out in enumerate(output):\n",
    "                    if isinstance(out, torch.Tensor):\n",
    "                        activations.setdefault(f\"{module_name}_{i}\", []).append(out.cpu().numpy())\n",
    "            elif isinstance(output, torch.Tensor):\n",
    "                activations.setdefault(module_name, []).append(output.cpu().numpy())\n",
    "\n",
    "    # Register hooks only for the fully connected layer\n",
    "    hook = model.fc.register_forward_hook(forward_hook)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, _ in dataloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            model(x_batch)\n",
    "\n",
    "    # Remove the hook\n",
    "    hook.remove()\n",
    "\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "b5673862-713f-4cd4-98e9-73884693f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_arrays(arrays, target_shape):\n",
    "    padded_arrays = []\n",
    "    for arr in arrays:\n",
    "        pad_width = [(0, max(0, target - dim)) for dim, target in zip(arr.shape, target_shape)]\n",
    "        padded_array = np.pad(arr, pad_width, mode='constant', constant_values=0)\n",
    "        padded_arrays.append(padded_array)\n",
    "    return padded_arrays\n",
    "\n",
    "def compute_histogram(activations, num_bins=2048):\n",
    "    histograms = {}\n",
    "    for layer_type, outputs in activations.items():\n",
    "        target_shape = tuple(max(s) for s in zip(*[output.shape for output in outputs]))\n",
    "        padded_outputs = pad_arrays(outputs, target_shape)\n",
    "        all_outputs = np.concatenate(padded_outputs, axis=0)\n",
    "        histograms[layer_type] = np.histogram(all_outputs, bins=num_bins, range=(all_outputs.min(), all_outputs.max()))\n",
    "    return histograms\n",
    "\n",
    "def compute_scale_from_range(min_value, max_value, num_levels=256):\n",
    "    scale = (max_value - min_value) / (num_levels - 1)\n",
    "    return scale\n",
    "\n",
    "def compute_optimal_scale(histogram, num_levels=256):\n",
    "    counts, bin_edges = histogram\n",
    "    \n",
    "    # Compute scale based on min and max bin edges\n",
    "    min_value = bin_edges[0]\n",
    "    max_value = bin_edges[-1]\n",
    "    \n",
    "    scale = (max_value - min_value) / (num_levels - 1)\n",
    "    \n",
    "    return scale\n",
    "\n",
    "# Modify the histogram processing to calculate scales\n",
    "def compute_scales_from_histograms(histograms, num_levels=256):\n",
    "    scales = {}\n",
    "    \n",
    "    for layer, histogram in histograms.items():\n",
    "        scale = compute_optimal_scale(histogram, num_levels)\n",
    "        scales[layer] = scale\n",
    "        \n",
    "        # Optional: Compute zero point if needed\n",
    "        min_value = histogram[1][0]  # Minimum bin edge\n",
    "        zero_point = round(-min_value / scale)\n",
    "        scales[f'{layer}_zero_point'] = zero_point\n",
    "    \n",
    "    return scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "4ed19fd1-67b4-4fad-8d9d-d28095b83261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histograms: {'fc_output': (array([1, 0, 0, ..., 0, 0, 1]), array([-5.823876 , -5.818256 , -5.812636 , ...,  5.6748137,  5.6804338,\n",
      "        5.6860538], dtype=float32))}\n",
      "Scales: {'fc_output': 0.04513697904698989}\n"
     ]
    }
   ],
   "source": [
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "activations = collect_activations(model, calibration_loader, device)\n",
    "histograms = compute_histogram(activations)\n",
    "\n",
    "# Debugging output for histograms\n",
    "print(f'Histograms: {histograms}')\n",
    "\n",
    "scales = {layer: compute_optimal_scale(histogram) for layer, histogram in histograms.items()}\n",
    "print(f'Scales: {scales}')\n",
    "# activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "72707ec1-d8e5-461f-9998-297d266ea7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_tensor(tensor, scale, zero_point=0, symmetric=False):\n",
    "    if symmetric:\n",
    "        return (tensor / scale).round().clamp(-128, 127).to(torch.int8)\n",
    "    else:\n",
    "        return ((tensor / scale) + zero_point).round().clamp(0, 255).to(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "e4de9e8d-5b87-4725-8ba7-8a3307c4ecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int32_computation(weights, biases, inputs):\n",
    "    \"\"\"Perform INT32 computations for recurrent/linear cells.\"\"\"\n",
    "    return torch.matmul(inputs, weights) + biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "73d20f05-1f61-4189-9f54-459506290bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def requantize_tensor(int32_tensor, output_scale):\n",
    "    \"\"\"Requantize INT32 results back to INT8.\"\"\"\n",
    "    return (int32_tensor.float() / output_scale).round().clamp(-128, 127).to(torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "e9dd7fa7-e01b-44aa-bb28-608db4bc109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dequantize_tensor(tensor, scale, zero_point=0, symmetric=False):\n",
    "    if symmetric:\n",
    "        return tensor.float() * scale\n",
    "    else:\n",
    "        return (tensor.to(torch.float32) - zero_point) * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "ac8f62f6-cff8-48c0-bb13-0e5a481fcdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(original, dequantized):\n",
    "    return np.mean((original - dequantized) ** 2)\n",
    "\n",
    "def print_intermediate_results(x, x_quantized, x_dequantized):\n",
    "    print(\"Original tensor:\", x.mean().item(), x.std().item())\n",
    "    print(\"Quantized tensor:\", x_quantized.cpu().numpy().mean().item(), x_quantized.cpu().numpy().std().item())\n",
    "    print(\"Dequantized tensor:\", x_dequantized.cpu().numpy().mean().item(), x_dequantized.cpu().numpy().std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "3122771d-3887-4776-9ecc-096435877a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantized_forward(model, x, scales, zero_point=0):\n",
    "    x_quantized = quantize_tensor(x, scales['fc_output'], zero_point)\n",
    "    fc_weight_quantized = quantize_tensor(model.fc.weight.data, scales['fc_output'], zero_point)\n",
    "    fc_biases_quantized = quantize_tensor(model.fc.bias.data, scales['fc_output'], zero_point)\n",
    "    dense_out_int32 = int32_computation(\n",
    "        fc_weight_quantized.to(torch.int32).T,\n",
    "        fc_biases_quantized.to(torch.int32),\n",
    "        x_quantized.to(torch.int32)\n",
    "    )\n",
    "    dense_out = requantize_tensor(dense_out_int32, scales['fc_output'])\n",
    "    return dense_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "9966c249-56e7-4be7-b468-16dd4cbb32e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e869251905445296cd8b986be7fe04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Test Loss Quantized: 15.825647464823723\n",
      "Avg Test Loss Dequantized: 15.825647464823723\n",
      "% diff: 0.0 \n",
      "\n",
      "0.0% difference between quantized & non-quantized loss\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_with_quantization(model, test_loader, scales, zero_point=0):\n",
    "    \"\"\"Evaluate model with quantization.\"\"\"\n",
    "    test_loss_quant = 0.0\n",
    "    test_loss_dequant = 0.0\n",
    "    total = 0 \n",
    "    \n",
    "    criterion = nn.MSELoss()  # Adjust this criterion if needed\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in tqdm(test_loader, position=0, leave=True):\n",
    "            # Move to device\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            # Forward pass with quantization\n",
    "            y_pred_quant = quantized_forward(model, X_batch, scales, zero_point)\n",
    "            \n",
    "            # Dequantize the predictions\n",
    "            y_pred_dequant = dequantize_tensor(y_pred_quant, scales['fc_output'])\n",
    "            \n",
    "            # Calculate MSE between original and dequantized predictions\n",
    "            y_pred_quant_np = y_pred_quant.cpu().numpy()\n",
    "            y_pred_dequant_np = y_pred_dequant.cpu().numpy()\n",
    "            # mse_error = compute_mse(y_pred_quant_np, y_pred_dequant_np)\n",
    "            # print(f'Mean Squared Error between quantized and dequantized tensor: {mse_error}')\n",
    "            \n",
    "            # Loss for quantized predictions\n",
    "            y_pred_quant_dequant = dequantize_tensor(y_pred_quant, scales['fc_output'])\n",
    "            loss_quant = criterion(y_pred_quant_dequant, y_batch)\n",
    "            test_loss_quant += loss_quant.item()\n",
    "            \n",
    "            # Loss for dequantized predictions\n",
    "            loss_dequant = criterion(y_pred_dequant, y_batch)\n",
    "            test_loss_dequant += loss_dequant.item()\n",
    "            \n",
    "            total += y_batch.size(0)\n",
    "    \n",
    "    avg_test_loss_quant = test_loss_quant / len(test_loader)\n",
    "    avg_test_loss_dequant = test_loss_dequant / len(test_loader)\n",
    "\n",
    "    print('Avg Test Loss Quantized:', avg_test_loss_quant)\n",
    "    print('Avg Test Loss Dequantized:', avg_test_loss_dequant)\n",
    "    print('% diff:', 100 * abs(avg_test_loss_dequant - avg_test_loss_quant) / avg_test_loss_dequant, '\\n')\n",
    "    \n",
    "    return avg_test_loss_quant, avg_test_loss_dequant\n",
    "\n",
    "# Evaluate the model\n",
    "avg_test_loss_quant, avg_test_loss_noquant = evaluate_model_with_quantization(model, data_loader, scales)\n",
    "percent_diff_in_test_loss_fc = 100 * abs(avg_test_loss_quant - avg_test_loss_noquant) / avg_test_loss_noquant\n",
    "print(f'{percent_diff_in_test_loss_fc}% difference between quantized & non-quantized loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc6f7f-8421-492b-8989-e44a56178601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
