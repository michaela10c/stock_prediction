{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "4CATJiF_A1VI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import json\n",
    "import seaborn as sns\n",
    "import time\n",
    "import itertools\n",
    "import cProfile\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import entropy\n",
    "from scipy.interpolate import interp1d\n",
    "from TimeSeriesDataset import TimeSeriesDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXcNnnW_C3XC"
   },
   "source": [
    "## Data Preparation & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "UOXSnPeqEO7q"
   },
   "outputs": [],
   "source": [
    "def load_configs(filename):\n",
    "    with open(f'configs/{filename}.json', 'r') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "nxRE-Z-hBRWh"
   },
   "outputs": [],
   "source": [
    "# Plot functions\n",
    "def plot_last_prices(last_prices, normalized=True):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(last_prices, label='Normalized Prices')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Normalized Price' if normalized else 'Price')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.title('Relative Change Rates of Close Prices' if normalized else 'Close Prices')\n",
    "    plt.show()\n",
    "\n",
    "def plot_volume(data, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(data['datetime'], data['volume'])\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Trade Volume')\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "def plot_prices_with_ma(data, period, period_labels):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(data.index, data['last_price'], label='Last Price')\n",
    "    for i in range(len(period)):\n",
    "      t = period[i]\n",
    "      period_label = period_labels[i]\n",
    "      plt.plot(data.index, data[f'ma({t})'], label=f'{period_label} MA')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Last Price')\n",
    "    plt.title('Prices with Moving Average')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_prices_with_macd(data, short_period, long_period, signal_period):\n",
    "    for i in range(len(short_period)):\n",
    "      plt.figure(figsize=(10, 5))\n",
    "      a = short_period[i]\n",
    "      b = long_period[i]\n",
    "      c = signal_period[i]\n",
    "      plt.plot(data.index, data[f'macd_line({a},{b},{c})'], label=f'MACD')\n",
    "      plt.plot(data.index, data[f'signal_line({a},{b},{c})'], label=f'Signal')\n",
    "      plt.plot(data.index, data[f'macd_histogram({a},{b},{c})'], label=f'Histogram')\n",
    "      plt.xlabel('Date')\n",
    "      plt.ylabel('Convergence/Divergence')\n",
    "      plt.title(f\"MACD({a},{b},{c})\")\n",
    "      plt.legend()\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "XkiCBffzERr9"
   },
   "outputs": [],
   "source": [
    "# Moving Average (MA)\n",
    "def moving_average(last_prices, num_periods):\n",
    "    ma = last_prices.rolling(window=num_periods, min_periods=1).mean()\n",
    "    # Forward fill the first num_periods-1 NaN values with the first non-NaN value\n",
    "    ma.ffill(inplace=True)\n",
    "    return ma\n",
    "\n",
    "def calculate_ema(data, period):\n",
    "    alpha = 2 / (period + 1)\n",
    "    ema = [data.iloc[0]]  # EMA starts with the first data point\n",
    "\n",
    "    for price in data.iloc[1:]:\n",
    "        ema.append(alpha * price + (1 - alpha) * ema[-1])\n",
    "\n",
    "    return pd.Series(ema, index=data.index)\n",
    "\n",
    "# Moving Average Convergence/Divergence (MACD)\n",
    "def calculate_macd(data, short_period, long_period, signal_period):\n",
    "    short_ema = calculate_ema(data, short_period)\n",
    "    long_ema = calculate_ema(data, long_period)\n",
    "\n",
    "    macd_line = short_ema - long_ema\n",
    "    signal_line = calculate_ema(macd_line, signal_period)\n",
    "    macd_histogram = macd_line - signal_line\n",
    "\n",
    "    return macd_line, signal_line, macd_histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "kq3mxEWoEUt-"
   },
   "outputs": [],
   "source": [
    "def filter_data_by_intervals(data, intervals):\n",
    "    # Initialize a mask with False values\n",
    "    interval_mask = pd.Series([False] * len(data))\n",
    "\n",
    "    # Iterate over intervals and apply masks\n",
    "    for start, end in intervals:\n",
    "        start_time = pd.to_datetime(start).time()\n",
    "        end_time = pd.to_datetime(end).time()\n",
    "\n",
    "        # Create masks for start and end times\n",
    "        start_time_mask = (data['datetime'].dt.time >= start_time)\n",
    "        end_time_mask = (data['datetime'].dt.time <= end_time)\n",
    "\n",
    "        # Combine masks based on interval crossing midnight or not\n",
    "        if start_time <= end_time:\n",
    "            interval_mask |= (start_time_mask & end_time_mask)\n",
    "        else:\n",
    "            interval_mask |= (start_time_mask | end_time_mask)\n",
    "\n",
    "    # Apply the final mask to filter the data\n",
    "    data_filtered = data[interval_mask]\n",
    "    return data_filtered\n",
    "\n",
    "def assert_time_intervals(df, intervals):\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    times = df.index.time\n",
    "\n",
    "    time_intervals = [(pd.Timestamp(start).time(), pd.Timestamp(end).time()) for start, end in intervals]\n",
    "\n",
    "    def is_within_intervals(t):\n",
    "        return any(start <= t <= end if start <= end else start <= t or t <= end for start, end in time_intervals)\n",
    "\n",
    "    outside_intervals = ~np.vectorize(is_within_intervals)(times)\n",
    "\n",
    "    if outside_intervals.any():\n",
    "        print(\"There are times outside the specified intervals:\")\n",
    "        print(df[outside_intervals])\n",
    "    else:\n",
    "        print(\"All times are within the specified intervals.\")\n",
    "\n",
    "    assert(not outside_intervals.any())\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def extract_main_contract(data_filtered, window=1000, min_periods=1, quantile=0.80):\n",
    "    # Compute the rolling mean of volume\n",
    "    data_filtered.loc[:, 'volume_rolling'] = data_filtered['volume'].rolling(window=window, min_periods=min_periods).mean()\n",
    "\n",
    "    # Calculate the volume threshold based on the quantile\n",
    "    volume_threshold = data_filtered['volume_rolling'].quantile(quantile)\n",
    "\n",
    "    # Identify high volume segments\n",
    "    data_filtered.loc[:, 'high_volume'] = data_filtered['volume_rolling'] > volume_threshold\n",
    "\n",
    "    # Segment identification by cumulative sum of changes in high_volume status\n",
    "    data_filtered.loc[:, 'segment'] = data_filtered['high_volume'].ne(data_filtered['high_volume'].shift()).cumsum()\n",
    "\n",
    "    # Filter the high volume segments and fill NaN values\n",
    "    high_volume_segments = data_filtered[data_filtered['high_volume']]\n",
    "    high_volume_segments.loc[:, 'volume'] = high_volume_segments['volume'].ffill()\n",
    "\n",
    "    # Drop the temporary columns\n",
    "    high_volume_segments.drop(columns=['high_volume', 'segment', 'volume_rolling'], inplace=True)\n",
    "\n",
    "    return high_volume_segments\n",
    "\n",
    "\n",
    "def normalize_prices(data_array, intervals, timestamps, num_steps, input_size, last_price_index=0):\n",
    "    # Convert timestamps to time objects\n",
    "    timestamp_times = timestamps.time\n",
    "\n",
    "    # Create an empty array to collect normalized prices\n",
    "    normalized_last_price = np.full(len(data_array), np.nan)  # Use NaN to identify unassigned values\n",
    "\n",
    "    for start, end in intervals:\n",
    "        start_time = pd.Timestamp(start).time()\n",
    "        end_time = pd.Timestamp(end).time()\n",
    "\n",
    "        # Create masks for the interval\n",
    "        start_mask = timestamp_times >= start_time\n",
    "        end_mask = timestamp_times <= end_time\n",
    "        if start_time <= end_time:\n",
    "            interval_mask = start_mask & end_mask\n",
    "        else:\n",
    "            interval_mask = start_mask | end_mask\n",
    "\n",
    "        # Filter data by interval\n",
    "        interval_data = data_array[interval_mask]\n",
    "        clear_gpu_cache()\n",
    "        if len(interval_data) == 0:\n",
    "            continue\n",
    "\n",
    "        num_windows = (len(interval_data) + input_size - 1) // input_size\n",
    "        windows = np.array_split(interval_data, num_windows)\n",
    "\n",
    "        # Create array to hold normalized values for the current interval\n",
    "        interval_normalized_last_price = np.full(len(interval_data), np.nan)\n",
    "\n",
    "        start_idx = 0\n",
    "        for window_data in windows:\n",
    "            if len(window_data) == 0:\n",
    "                continue\n",
    "\n",
    "            if start_idx == 0:\n",
    "                window_first_price = window_data[0, last_price_index]\n",
    "                values = window_data[:, last_price_index] / window_first_price - 1.0\n",
    "            else:\n",
    "                window_last_price = window_data[-1, last_price_index]\n",
    "                values = window_data[:, last_price_index] / window_last_price - 1.0\n",
    "\n",
    "            end_idx = start_idx + len(window_data)\n",
    "            interval_normalized_last_price[start_idx:end_idx] = values\n",
    "            start_idx = end_idx\n",
    "\n",
    "        normalized_last_price[interval_mask] = interval_normalized_last_price\n",
    "\n",
    "    # Set to the data array\n",
    "    data_array[:, last_price_index] = normalized_last_price\n",
    "\n",
    "    # Process all outliers - impute with its previous non-outlying value\n",
    "    postprocess_outliers(data_array)\n",
    "\n",
    "    # Check for NaN values\n",
    "    if np.isnan(data_array[:, last_price_index]).any():\n",
    "        raise ValueError(\"Data contains NaN values after normalization. Please check the normalization process.\")\n",
    "\n",
    "    return data_array\n",
    "\n",
    "\n",
    "def postprocess_outliers(data, threshold=0.5):\n",
    "    outlier_indices = np.where(np.abs(data[:, 0]) > threshold)[0]\n",
    "\n",
    "    for idx in outlier_indices:\n",
    "        previous_value_idx = idx - 1\n",
    "        while data[previous_value_idx, 0] > threshold and previous_value_idx > 0:\n",
    "            previous_value_idx -= 1\n",
    "        data[idx, 0] = data[previous_value_idx, 0]\n",
    "\n",
    "def roll_data(data_array, num_steps, input_size):\n",
    "    # Roll data to reshape it into the 4D shape (N, num_steps, input_size, # features)\n",
    "    data_array = [np.array(data_array[i * input_size: (i + 1) * input_size])\n",
    "                  for i in range(len(data_array) // input_size)]\n",
    "    data_array = np.stack(data_array)\n",
    "    return data_array\n",
    "\n",
    "\n",
    "# Generator function\n",
    "def data_generator(data_array, indices, num_steps, batch_size, last_price_index=0):\n",
    "    total_len = len(indices)\n",
    "    for start_idx in range(0, total_len, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, total_len)\n",
    "        batch_indices = indices[start_idx:end_idx]\n",
    "        X_batch = np.array([data_array[i: i + num_steps] for i in batch_indices])\n",
    "        y_batch = data_array[batch_indices + num_steps, :, last_price_index]\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "\n",
    "# Train-test split function\n",
    "def train_test_split(data_array, num_steps, input_size, val_split, test_split, last_price_index=0):\n",
    "    # Calculate the total number of samples\n",
    "    total_len = len(data_array) - num_steps\n",
    "\n",
    "    X = np.empty((total_len, num_steps, input_size, data_array.shape[-1] - 1), dtype=np.float32)\n",
    "    y = np.empty((total_len, num_steps), dtype=np.float32)\n",
    "\n",
    "    for i in range(total_len):\n",
    "        X[i] = data_array[i:i + num_steps, :, 1:]  # Exclude the last_price column (column 0)\n",
    "        y[i] = data_array[i:i + num_steps, :, last_price_index]  # Store the last_price values\n",
    "\n",
    "    # Check the shapes of X and y\n",
    "    num_features = data_array.shape[-1] - 1  # Exclude the last_price column\n",
    "    assert X.shape == (total_len, num_steps, input_size, num_features), f\"X shape mismatch: {X.shape}\"\n",
    "    assert y.shape == (total_len, num_steps), f\"y shape mismatch: {y.shape}\"\n",
    "    assert len(X) == len(y), \"Number of samples in X and y must be equal\"\n",
    "\n",
    "    # Determine the split indices\n",
    "    test_start = int(total_len * (1 - test_split))\n",
    "    val_start = int(total_len * (1 - test_split - val_split))\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_val, X_test = X[:val_start], X[val_start:test_start], X[test_start:]\n",
    "    y_train, y_val, y_test = y[:val_start], y[val_start:test_start], y[test_start:]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "def normalize_data(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    normalized_df = pd.DataFrame(normalized_data, index=data.index, columns=data.columns)\n",
    "    return normalized_df\n",
    "\n",
    "# Create dataloader instances\n",
    "def create_dataloader_instances(dataset, val_split, test_split, batch_size, num_workers=8):\n",
    "    dataset_size = len(dataset)\n",
    "    test_size = int(test_split * dataset_size)\n",
    "    val_size = int(val_split * (dataset_size - test_size))\n",
    "    train_size = dataset_size - val_size - test_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Create forward fill bid custom\n",
    "def forward_fill_bid_custom(data, price_cols):\n",
    "    # Forward fill NaN values across rows for specified columns\n",
    "    data[price_cols] = data[price_cols].ffill(axis=1)\n",
    "\n",
    "    # Forward fill NaN values across columns for specified columns\n",
    "    data[price_cols] = data[price_cols].ffill(axis=0)\n",
    "\n",
    "    return data\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Create sequences based on the extended_segment\n",
    "def create_sequences(extended_segment, num_steps, input_size, normalize, val_split=0.1, test_split=0.1, batch_size=64):\n",
    "    # Ensure no NaN values\n",
    "    assert(np.isnan(extended_segment).sum().sum() == 0)\n",
    "\n",
    "    # Setup\n",
    "    timestamps = extended_segment.index\n",
    "    extended_segment = extended_segment.to_numpy()\n",
    "    last_price_index = 0\n",
    "\n",
    "    # Prevent a division by 0 by imputing 0s to a very small number\n",
    "    extended_segment[:, last_price_index] = np.where(extended_segment[:, last_price_index] == 0, 1e-6, extended_segment[:, last_price_index])\n",
    "\n",
    "    # Plot the prices before normalization\n",
    "    plot_last_prices(extended_segment[:, last_price_index], normalized=False)\n",
    "\n",
    "    # Normalize prices - retrieve relative change rates\n",
    "    if normalize:\n",
    "        extended_segment = normalize_prices(extended_segment, intervals, timestamps, num_steps, input_size, last_price_index)\n",
    "\n",
    "    clear_gpu_cache()\n",
    "\n",
    "    # Plot the prices after normalization\n",
    "    plot_last_prices(extended_segment[:, last_price_index])\n",
    "\n",
    "    # Roll data for RNN\n",
    "    extended_segment = roll_data(extended_segment, num_steps, input_size)\n",
    "    clear_gpu_cache()\n",
    "\n",
    "    # Create dataset and dataloader instances\n",
    "    dataset = TimeSeriesDataset(extended_segment, num_steps, last_price_index)\n",
    "    clear_gpu_cache()\n",
    "    train_loader, val_loader, test_loader = create_dataloader_instances(dataset, val_split=val_split, test_split=test_split, batch_size=batch_size)\n",
    "    clear_gpu_cache()\n",
    "    \n",
    "    # # Remove all rows with exceptionally small bid ask spread\n",
    "    # extended_segment = extended_segment[:, ]\n",
    "\n",
    "    return extended_segment, train_loader, val_loader, test_loader, dataset.num_features\n",
    "\n",
    "\n",
    "def add_derived_features(extended_segment):\n",
    "    # Forward fill nan values in bid_prices (NOTE: This may not reflect the changes in market!)\n",
    "    bid_price_cols = ['bid_price1', 'bid_price2', 'bid_price3', 'bid_price4', 'bid_price5']\n",
    "    ask_price_cols = ['ask_price1', 'ask_price2', 'ask_price3', 'ask_price4', 'ask_price5']\n",
    "    extended_segment = forward_fill_bid_custom(extended_segment, bid_price_cols)\n",
    "\n",
    "    # Bid & Ask Volumes\n",
    "    bid_volume_cols = ['bid_volume1', 'bid_volume2', 'bid_volume3', 'bid_volume4', 'bid_volume5']\n",
    "    ask_volume_cols = ['ask_volume1', 'ask_volume2', 'ask_volume3', 'ask_volume4', 'ask_volume5']\n",
    "\n",
    "    # Calculate Bid-Ask Spread\n",
    "    extended_segment.loc[:, 'bid_ask_spread'] = extended_segment['ask_price1'] - extended_segment['bid_price1']\n",
    "\n",
    "    # Calculate Market Depth (total bid and ask volumes)\n",
    "    extended_segment.loc[:, 'total_bid_volume'] = extended_segment[bid_volume_cols].sum(axis=1)\n",
    "    extended_segment.loc[:, 'total_ask_volume'] = extended_segment[ask_volume_cols].sum(axis=1)\n",
    "\n",
    "    # Calculate Order Imbalance Ratio\n",
    "    extended_segment.loc[:, 'order_imbalance_ratio'] = (extended_segment['total_bid_volume'] - extended_segment['total_ask_volume']) / (extended_segment['total_bid_volume'] + extended_segment['total_ask_volume'])\n",
    "\n",
    "    # Calculate Volume Order Imbalance\n",
    "    delta_bid_vol = extended_segment[bid_volume_cols].diff().fillna(0)\n",
    "    delta_ask_vol = extended_segment[ask_volume_cols].diff().fillna(0)\n",
    "    delta_bid_price = extended_segment[bid_price_cols].diff().fillna(0)\n",
    "    delta_ask_price = extended_segment[ask_price_cols].diff().fillna(0)\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        bid_vol_col = f'bid_volume{i}'\n",
    "        ask_vol_col = f'ask_volume{i}'\n",
    "        bid_price_col = f'bid_price{i}'\n",
    "        ask_price_col = f'ask_price{i}'\n",
    "\n",
    "        # Clip delta_bid_vol to bid_volume on a rise\n",
    "        delta_bid_vol[bid_vol_col] = np.where(delta_bid_price[bid_price_col] > 0,\n",
    "                                              np.minimum(delta_bid_vol[bid_vol_col], extended_segment[bid_vol_col]),\n",
    "                                              delta_bid_vol[bid_vol_col])\n",
    "\n",
    "        # Clip delta_ask_vol to ask_volume on a fall\n",
    "        delta_ask_vol[ask_vol_col] = np.where(delta_ask_price[ask_price_col] < 0,\n",
    "                                              np.minimum(delta_ask_vol[ask_vol_col], extended_segment[ask_vol_col]),\n",
    "                                              delta_ask_vol[ask_vol_col])\n",
    "\n",
    "    extended_segment.loc[:, 'volume_order_imbalance'] = delta_bid_vol.sum(axis=1) - delta_ask_vol.sum(axis=1)\n",
    "\n",
    "    # Calculate Mid-Price Basis\n",
    "    extended_segment.loc[:, 'mid_price'] = (extended_segment.loc[:, 'bid_price1'] + extended_segment.loc[:, 'ask_price1']) / 2\n",
    "\n",
    "    # Calculate average trade price if it doesn't exist\n",
    "    if 'average_trade_price' not in extended_segment.columns:\n",
    "        extended_segment['average_trade_price'] = np.where(\n",
    "            extended_segment['volume'].diff() != 0,\n",
    "            (extended_segment['amount'].diff() / extended_segment['volume'].diff()).fillna(0),\n",
    "            extended_segment['mid_price']\n",
    "        )\n",
    "    else:\n",
    "        extended_segment['average_trade_price'] = np.where(\n",
    "            extended_segment['volume'].diff() != 0,\n",
    "            (extended_segment['amount'].diff() / extended_segment['volume'].diff()).fillna(0),\n",
    "            extended_segment['average_trade_price'].shift(1).fillna(0)\n",
    "        )\n",
    "\n",
    "    extended_segment['mid_price_basis'] = extended_segment['average_trade_price'] - extended_segment['mid_price']\n",
    "\n",
    "    # Drop intermediary derived feature columns\n",
    "    intermediary_columns = [\n",
    "        'total_bid_volume', 'total_ask_volume', 'mid_price', 'average_trade_price'\n",
    "    ]\n",
    "    extended_segment = extended_segment.drop(columns=intermediary_columns)\n",
    "\n",
    "    # Ensure there are no NaN values\n",
    "    assert not extended_segment.isnull().values.any(), \"There are NaN values in the derived features\"\n",
    "    assert(np.isnan(extended_segment).sum().sum() == 0)\n",
    "\n",
    "\n",
    "def add_factors(extended_segment, short_period, long_period, signal_period, period):\n",
    "    # Compute Factors: MA & MACD\n",
    "    for i in range(len(period)):\n",
    "      # Calculate MA\n",
    "      t = period[i]\n",
    "      extended_segment[f'ma({t})'] = moving_average(extended_segment['last_price'], t)\n",
    "      extended_segment[f'ma({t})'] = extended_segment[f'ma({t})'].fillna(method='ffill')\n",
    "\n",
    "      # Calculate MACD\n",
    "      a = short_period[i]\n",
    "      b = long_period[i]\n",
    "      c = signal_period[i]\n",
    "      macd_line, signal_line, macd_histogram = calculate_macd(extended_segment['last_price'], a, b, c)\n",
    "      extended_segment[f'macd_line({a},{b},{c})'] = macd_line\n",
    "      extended_segment[f'signal_line({a},{b},{c})'] = signal_line\n",
    "      extended_segment[f'macd_histogram({a},{b},{c})'] = macd_histogram\n",
    "\n",
    "    print(extended_segment.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DVuKsKrMC7iP",
    "outputId": "907ed828-ba3b-453e-9a0f-f94d442eb1c5"
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "dataset_filename = 'INE.sc2010'\n",
    "intervals = [\n",
    "    ('21:00:00', '02:30:00'),\n",
    "    ('09:00:00', '10:15:00'),\n",
    "    ('10:30:00', '11:30:00'),\n",
    "    ('13:30:00', '15:00:00')\n",
    "]\n",
    "configs_filename = 'configs'\n",
    "\n",
    "# Load configs file\n",
    "configs = load_configs(configs_filename)\n",
    "clear_gpu_cache()\n",
    "\n",
    "# Read data\n",
    "# data = pd.read_csv('data/' + dataset_filename + '.csv')\n",
    "data = pd.read_csv(f'data/{dataset_filename}.csv')\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "\n",
    "# Set seed\n",
    "set_seed(42)\n",
    "\n",
    "with cProfile.Profile() as pr:\n",
    "    # Only include data within the specified intervals\n",
    "    data_filtered = filter_data_by_intervals(data, intervals)\n",
    "    # pr.print_stats()\n",
    "\n",
    "    # Make sure there are no values outside the given time intervals\n",
    "    assert_time_intervals(data_filtered, intervals)\n",
    "    # pr.print_stats()\n",
    "\n",
    "    # Plot trading volume data\n",
    "    plot_volume(data_filtered, 'Daily Trading Volume of SC2010 Stocks')\n",
    "\n",
    "    # Extract the main contract\n",
    "    # data_filtered = data_filtered.copy() # Avoid SettingWithCopyWarning\n",
    "    # pr.print_stats()\n",
    "    data_filtered.iloc[:, 1:] = data_filtered.iloc[:, 1:].astype(np.float32)\n",
    "    # pr.print_stats()\n",
    "    extended_segment = extract_main_contract(data_filtered)\n",
    "    # pr.print_stats()\n",
    "\n",
    "    # Plot main contract segment\n",
    "    plot_volume(extended_segment, 'Trading Volume of Main Contract Segment')\n",
    "    # pr.print_stats()\n",
    "\n",
    "    # Set index of the resulting dataframe\n",
    "    extended_segment.set_index('datetime', inplace=True)\n",
    "    # pr.print_stats()\n",
    "\n",
    "    # Make sure there are no values outside the given time intervals\n",
    "    assert_time_intervals(extended_segment, intervals)\n",
    "    # pr.print_stats()\n",
    "\n",
    "    # Extract needed hyperparams\n",
    "    input_size = configs['input_size']\n",
    "    num_steps = configs['num_steps']\n",
    "    normalize = configs['normalize']\n",
    "    batch_size = configs['batch_size']\n",
    "\n",
    "    # Add derived features\n",
    "    add_derived_features(extended_segment)\n",
    "\n",
    "    # Add factors\n",
    "    period_labels = np.array(['2.5s', '5s', '7.5s', '10s'])\n",
    "    period = np.array([5, 10, 15, 20])\n",
    "    short_period = period\n",
    "    long_period = period + 10\n",
    "    signal_period = 2 * period\n",
    "\n",
    "    add_factors(extended_segment, short_period, long_period, signal_period, period)\n",
    "    # pr.print_stats()\n",
    "\n",
    "    # Plot closing prices with factors\n",
    "    plot_prices_with_ma(extended_segment, period, period_labels)\n",
    "    plot_prices_with_macd(extended_segment, short_period, long_period, signal_period)\n",
    "\n",
    "    # Normalize data\n",
    "    extended_segment = normalize_data(extended_segment)\n",
    "    assert(extended_segment.isna().sum().sum() == 0)\n",
    "\n",
    "    # Create sequences based on the extended_segment\n",
    "    extended_segment, train_loader, val_loader, test_loader, num_features = create_sequences(extended_segment, num_steps, input_size, normalize, val_split=0.1, test_split=0.1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYSFG2CvRdl3"
   },
   "source": [
    "## RNN Model Definition \n",
    "\n",
    "Include quantized components when Quantized-Aware Training (QAT) enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97w6HrsKqROK"
   },
   "outputs": [],
   "source": [
    "def convert_to_labels(val, flat_gap=0.01):\n",
    "    return 2 if val > flat_gap else 0 if val < -flat_gap else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6Jc4p_Rqguf"
   },
   "outputs": [],
   "source": [
    "def classification_accuracy(y_pred, y_true):\n",
    "    y_pred_labels = torch.tensor([convert_to_labels(y) for y in torch.flatten(y_pred)])\n",
    "    y_true_labels = torch.tensor([convert_to_labels(y) for y in torch.flatten(y_true)])\n",
    "    return torch.sum(y_pred_labels == y_true_labels) / len(y_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for fake quantization\n",
    "def fake_quantize(x, scale, zero_point):\n",
    "    return torch.round(x / scale + zero_point).clamp(0, 255).float() * scale - zero_point * scale\n",
    "    \n",
    "def update_scale_and_zero_point(x, num_bits=8):\n",
    "    qmin, qmax = 0, (1 << num_bits) - 1\n",
    "    min_val, max_val = x.min(), x.max()\n",
    "    \n",
    "    min_val = min_val.item()  # Convert to Python float\n",
    "    max_val = max_val.item()  # Convert to Python float\n",
    "    \n",
    "    if min_val == max_val:  # Avoid division by zero\n",
    "        scale = 1.0\n",
    "        zero_point = 0\n",
    "    else:\n",
    "        scale = (max_val - min_val) / (qmax - qmin)\n",
    "        zero_point = qmin - min_val / scale\n",
    "    \n",
    "    return scale, zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(CustomLayerNorm, self).__init__()\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = 1e-5  # Small value to avoid division by zero\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        return (x - mean) / torch.sqrt(var + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTpoDSrwJxeA"
   },
   "outputs": [],
   "source": [
    "class CustomLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout_rate=0.0, quantize=False):\n",
    "        super(CustomLSTMCell, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.quantize = quantize\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W_i = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_i = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_i = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "\n",
    "        self.W_f = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_f = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_f = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        self.W_c = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_c = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_c = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        self.W_o = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_o = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_o = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        # Layer normalization layers\n",
    "        self.ln_i = CustomLayerNorm(hidden_dim)\n",
    "        self.ln_f = CustomLayerNorm(hidden_dim)\n",
    "        self.ln_c = CustomLayerNorm(hidden_dim)\n",
    "        self.ln_o = CustomLayerNorm(hidden_dim)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Quantization parameters\n",
    "        self.scale_w = None\n",
    "        self.zero_point_w = None\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'W_' in name or 'U_' in name:\n",
    "                nn.init.orthogonal_(param)  # Use orthogonal initialization\n",
    "            elif 'b_' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "            # Check for NaN values\n",
    "            assert(not torch.isnan(param).any())\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        if self.quantize:\n",
    "            # Update scale and zero points for quantization\n",
    "            self.scale_w, self.zero_point_w = update_scale_and_zero_point(self.W_i)\n",
    "            self.scale_w, self.zero_point_w = update_scale_and_zero_point(self.U_i)\n",
    "            self.scale_w, self.zero_point_w = update_scale_and_zero_point(self.b_i)\n",
    "            self.scale_w, self.zero_point_w = update_scale_and_zero_point(self.W_f)\n",
    "            self.scale_w, self.zero_point_w = update_scale_and_zero_point(self.U_f)\n",
    "            self.scale_w, self.zero_point_w = update_scale_and_zero_point(self.b_f)\n",
    "            self.scale_w, self.zero_point_w = update_scale_and_zero_point(self.W_c)\n",
    "            self.scale_w, self.zero_point_w = update_scale_and_zero_point(self.U_c)\n",
    "            self.scale_w, self.zero_point_w = update_scale_and_zero_point(self.b_c)\n",
    "            self.scale_w, self.zero_point_w = update_scale_and_zero_point(self.W_o)\n",
    "            self.scale_w, self.zero_point_w = update_scale_and_zero_point(self.U_o)\n",
    "            self.scale_w, self.zero_point_w = update_scale_and_zero_point(self.b_o)\n",
    "        \n",
    "        def quantize_param(param):\n",
    "            return fake_quantize(param, self.scale_w, self.zero_point_w) if self.quantize else param\n",
    "\n",
    "        W_i = quantize_param(self.W_i)\n",
    "        U_i = quantize_param(self.U_i)\n",
    "        b_i = quantize_param(self.b_i)\n",
    "        W_f = quantize_param(self.W_f)\n",
    "        U_f = quantize_param(self.U_f)\n",
    "        b_f = quantize_param(self.b_f)\n",
    "        W_c = quantize_param(self.W_c)\n",
    "        U_c = quantize_param(self.U_c)\n",
    "        b_c = quantize_param(self.b_c)\n",
    "        W_o = quantize_param(self.W_o)\n",
    "        U_o = quantize_param(self.U_o)\n",
    "        b_o = quantize_param(self.b_o)\n",
    "\n",
    "        i_t = torch.sigmoid(self.ln_i(self.dropout(torch.mm(x, W_i) + torch.mm(h, U_i) + b_i)))\n",
    "        f_t = torch.sigmoid(self.ln_f(self.dropout(torch.mm(x, W_f) + torch.mm(h, U_f) + b_f)))\n",
    "        g_t = torch.tanh(self.ln_c(self.dropout(torch.mm(x, W_c) + torch.mm(h, U_c) + b_c)))\n",
    "        o_t = torch.sigmoid(self.ln_o(self.dropout(torch.mm(x, W_o) + torch.mm(h, U_o) + b_o)))\n",
    "\n",
    "        c_t = f_t * c + i_t * g_t\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "        assert(not torch.isnan(h_t).any())\n",
    "        assert(not torch.isnan(c_t).any())\n",
    "        \n",
    "        return h_t, c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckdaPSaxJxhM"
   },
   "outputs": [],
   "source": [
    "class CustomGRUCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout_rate=0.1, quantize=False):\n",
    "        super(CustomGRUCell, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.quantize = quantize\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W_z = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_z = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_z = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "\n",
    "        self.W_r = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_r = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_r = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        self.W_h = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_h = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_h = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        # Layer normalization layers\n",
    "        self.ln_z = CustomLayerNorm(hidden_dim)\n",
    "        self.ln_r = CustomLayerNorm(hidden_dim)\n",
    "        self.ln_h = CustomLayerNorm(hidden_dim)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Quantization parameters\n",
    "        self.scale_w = None\n",
    "        self.zero_point_w = None\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'W_' in name or 'U_' in name:\n",
    "                nn.init.orthogonal_(param)  # Use orthogonal initialization\n",
    "            elif 'b_' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "            # Check for NaN values\n",
    "            assert(not torch.isnan(param).any())\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        if self.quantize:\n",
    "            # Update scale and zero points for quantization\n",
    "            self.scale_w, self.zero_point_w = update_scale_and_zero_point(self.W_z)\n",
    "            self.scale_w, self.zero_point_w = update_scale_and_zero_point(self.U_z)\n",
    "            self.scale_w, self.zero_point_w = update_scale_and_zero_point(self.b_z)\n",
    "            self.scale_w, self.zero_point_w = update_scale_and_zero_point(self.W_r)\n",
    "            self.scale_w, self.zero_point_w = update_scale_and_zero_point(self.U_r)\n",
    "            self.scale_w, self.zero_point_w = update_scale_and_zero_point(self.b_r)\n",
    "            self.scale_w, self.zero_point_w = update_scale_and_zero_point(self.W_h)\n",
    "            self.scale_w, self.zero_point_w = update_scale_and_zero_point(self.U_h)\n",
    "            self.scale_w, self.zero_point_w = update_scale_and_zero_point(self.b_h)\n",
    "        \n",
    "        def quantize_param(param):\n",
    "            return fake_quantize(param, self.scale_w, self.zero_point_w) if self.quantize else param\n",
    "        \n",
    "        W_z = quantize_param(self.W_z)\n",
    "        U_z = quantize_param(self.U_z)\n",
    "        b_z = quantize_param(self.b_z)\n",
    "        W_r = quantize_param(self.W_r)\n",
    "        U_r = quantize_param(self.U_r)\n",
    "        b_r = quantize_param(self.b_r)\n",
    "        W_h = quantize_param(self.W_h)\n",
    "        U_h = quantize_param(self.U_h)\n",
    "        b_h = quantize_param(self.b_h)\n",
    "\n",
    "        z_t = torch.sigmoid(self.ln_z(self.dropout(torch.mm(x, W_z) + torch.mm(h, U_z) + b_z)))\n",
    "        r_t = torch.sigmoid(self.ln_r(self.dropout(torch.mm(x, W_r) + torch.mm(h, U_r) + b_r)))\n",
    "        h_hat_t = torch.tanh(self.ln_h(self.dropout(torch.mm(x, W_h) + torch.mm(r_t * h, U_h) + b_h)))\n",
    "\n",
    "        h_t = (1 - z_t) * h + z_t * h_hat_t\n",
    "\n",
    "        assert(not torch.isnan(h_t).any())\n",
    "        \n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, quantize=False):\n",
    "        super(CustomLinear, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.quantize = quantize\n",
    "        \n",
    "        if self.quantize:\n",
    "            self.scale_w, self.zero_point_w = None, None  # Initialize as None initially\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.quantize:\n",
    "            # Update scale and zero points for quantization\n",
    "            if self.scale_w is None or self.zero_point_w is None:\n",
    "                self.scale_w, self.zero_point_w = update_scale_and_zero_point(self.linear.weight)\n",
    "            # Apply quantization to weights\n",
    "            weight = fake_quantize(self.linear.weight, self.scale_w, self.zero_point_w)\n",
    "            x = F.linear(x, weight, self.linear.bias)\n",
    "        else:\n",
    "            x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_ZACtaln5Nz"
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, rnn_type='lstm', num_steps=20, input_size=10, hidden_units=128, num_layers=3, dropout_rate=0.1, dense_units=64, quantize=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn_type = rnn_type\n",
    "        self.quantize = quantize\n",
    "        \n",
    "        if rnn_type == 'lstm':\n",
    "            self.rnn = nn.ModuleList([CustomLSTMCell(input_size * num_features, hidden_units, dropout_rate=dropout_rate, quantize=quantize) for _ in range(num_layers)])\n",
    "        elif rnn_type == 'gru':\n",
    "            self.rnn = nn.ModuleList([CustomGRUCell(input_size * num_features, hidden_units, dropout_rate=dropout_rate, quantize=quantize) for _ in range(num_layers)])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported RNN type: {rnn_type}\")\n",
    "        \n",
    "        # Assign names to RNN layers\n",
    "        for layer_idx, layer in enumerate(self.rnn):\n",
    "            for name, param in layer.named_parameters():\n",
    "                self.register_buffer(f'rnn{layer_idx}_{name}', param)\n",
    "        \n",
    "        self.fc = CustomLinear(hidden_units, dense_units, quantize=quantize)\n",
    "        self.out = CustomLinear(dense_units, input_size * num_steps, quantize=quantize)\n",
    "        \n",
    "        # Assign names to linear layers\n",
    "        for name, param in self.fc.named_parameters():\n",
    "            name = name.replace('.', '_')\n",
    "            self.register_buffer(f'fc_{name}', param)\n",
    "        \n",
    "        for name, param in self.out.named_parameters():\n",
    "            name = name.replace('.', '_')\n",
    "            self.register_buffer(f'out_{name}', param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_steps, input_size, num_features = x.shape\n",
    "        x = torch.reshape(x, (batch_size, num_steps, input_size * num_features))\n",
    "\n",
    "        # RNN layers\n",
    "        hidden_size = self.rnn[0].hidden_dim if self.rnn_type == 'lstm' else self.rnn[0].hidden_dim\n",
    "        h_t = torch.zeros(batch_size, hidden_size).to(x.device)\n",
    "        c_t = torch.zeros(batch_size, hidden_size).to(x.device)\n",
    "\n",
    "        for layer_idx, layer in enumerate(self.rnn):\n",
    "            if self.rnn_type == 'lstm':\n",
    "                h_t, c_t = layer(x[:, -1, :], h_t, c_t)\n",
    "            else:\n",
    "                h_t = layer(x[:, -1, :], h_t)\n",
    "\n",
    "        dense_out = self.fc(h_t)\n",
    "        out = self.out(dense_out)\n",
    "        \n",
    "        out = torch.reshape(out, (out.shape[0], num_steps, input_size))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7x14GluDn5Yt"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, config, device, print_freq=10):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['init_learning_rate'])\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=config['learning_rate_decay'])\n",
    "\n",
    "    early_stop_patience = config['early_stop_patience']\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 0\n",
    "\n",
    "    for epoch in tqdm(range(config['max_epochs']), position=0, leave=True):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for i, (x_batch, y_batch) in tqdm(enumerate(train_loader), position=0, leave=True):\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % print_freq == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, config['max_epochs'], i + 1, len(train_loader), loss.item()))\n",
    "                print('Training classification:', classification_accuracy(y_pred, y_batch).item())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, (x_batch, y_batch) in tqdm(enumerate(val_loader), position=0, leave=True):\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                y_pred = model(x_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                if (i + 1) % print_freq == 0:\n",
    "                    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, config['max_epochs'], i + 1, len(val_loader), loss.item()))\n",
    "                    print('Validation classification:', classification_accuracy(y_pred, y_batch).item())\n",
    "\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{config['max_epochs']}, Train Loss: {train_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}\")\n",
    "\n",
    "        if config['use_early_stop']:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= early_stop_patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "        scheduler.step()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noAjxW3sppPC"
   },
   "outputs": [],
   "source": [
    "def load_model(model, rnn_type, device, quantize=False):\n",
    "    model_path = f'models/SC2010_{rnn_type}_model_quantized.pth' if quantize else f'models/SC2010_{rnn_type}_model.pth' \n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFfMKnlIPjmo"
   },
   "outputs": [],
   "source": [
    "def train_or_load_model(configs, train_loader, val_loader, rnn_type, device, quantize=False):\n",
    "    # Define model & load into device\n",
    "    model = RNNModel(rnn_type=rnn_type, num_steps=configs['num_steps'], input_size=configs['input_size'], hidden_units=configs[rnn_type][f'{rnn_type}_units'], num_layers=configs[rnn_type][f'{rnn_type}_layers'], dropout_rate=configs[rnn_type]['dropout_rate'], dense_units=configs[rnn_type]['dense_units'], quantize=quantize)\n",
    "    model.to(device)\n",
    "    \n",
    "    if configs[rnn_type]['pretrain']:\n",
    "        # Train the model\n",
    "        model = train_model(model, train_loader, val_loader, configs[rnn_type], device)\n",
    "\n",
    "        # Save model\n",
    "        model_path = f'models/SC2010_{rnn_type}_model_quantized.pth' if quantize else f'models/SC2010_{rnn_type}_model.pth' \n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    else:\n",
    "        # Load the model from a pre-saved file\n",
    "        model = load_model(model, rnn_type, device, quantize)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1-42q_WQxoq"
   },
   "outputs": [],
   "source": [
    "def mean_percent_diff(outputs, targets):\n",
    "    total_abs_diff = 0.0\n",
    "    total_elements = 0\n",
    "    \n",
    "    # Iterate through each batch in the list\n",
    "    for i in range(len(test_preds_lstm_noquant)):\n",
    "        noquant = test_preds_lstm_noquant[i]\n",
    "        dequant = test_preds_lstm_dequant[i].cpu().numpy()\n",
    "        \n",
    "        # Ensure shapes match\n",
    "        if noquant.shape != dequant.shape:\n",
    "            raise ValueError(f\"Shape mismatch: {noquant.shape} vs {dequant.shape}\")\n",
    "        \n",
    "        # Calculate absolute differences\n",
    "        abs_diff = np.abs(noquant - dequant)\n",
    "        \n",
    "        # Calculate percent differences\n",
    "        percent_diff = (abs_diff / (np.abs(noquant) + 1e-8)) * 100\n",
    "        \n",
    "        # Aggregate differences\n",
    "        total_abs_diff += np.sum(percent_diff)\n",
    "        total_elements += np.size(percent_diff)\n",
    "    \n",
    "    # Calculate mean percent difference\n",
    "    percent_diff = total_abs_diff / total_elements   \n",
    "    return percent_diff\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0.0\n",
    "    test_accuracy = 0.0\n",
    "\n",
    "\n",
    "    total = 0 \n",
    "    \n",
    "    preds = []\n",
    "\n",
    "    # Disable gradient computation during evaluation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over test set data\n",
    "        for i, (x_batch, y_batch) in tqdm(enumerate(test_loader), position=0, leave=True):\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(x_batch)\n",
    "            loss = nn.MSELoss()(y_pred, y_batch)\n",
    "            preds.append(y_pred.numpy())\n",
    "            test_loss += loss.item()\n",
    "            test_accuracy += classification_accuracy(y_pred, y_batch).item()\n",
    "\n",
    "            total += y_batch.size(0)\n",
    "    \n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_accuracy = test_accuracy / total\n",
    "\n",
    "    print(\"Test Loss:\", test_loss)\n",
    "    print('Test classification:', test_accuracy)\n",
    "    \n",
    "    return preds, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtfCK8cen5dZ"
   },
   "outputs": [],
   "source": [
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset configs\n",
    "configs = load_configs('configs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXCWMY1ERPb4"
   },
   "source": [
    "## Training + Evaluation Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "95eca35a46a14aa4abf7c91b1ccee065",
      "01289697a46a4749808002f2a6eaaf73",
      "9ff6b3cf75b746f5a11dc7a7eadc2009",
      "e86b3864d880446081ae53689493f24e",
      "c275aca6f2c84125923cbd7246786078",
      "b1f95063a92b48e0b5ca86e7a6716236",
      "abf1a93ee34843578018f72eeb64a9ea",
      "18429f5771a24ae1a72150c16e7f97d3",
      "a583468ab883499e9d8cc9dadac9c597",
      "497e7dd3069a4b5dbc883e4695332963",
      "5ab32519140341eebad1e15228deb3e2",
      "6b7700e80d854914ad4f75c34e338d3e",
      "77b06e361d74438fad7a17c8f34c0aa1",
      "c2ba33ed9a314536bffb427470ee1957",
      "b15dd251352142d39fb0c1a46cf9b388",
      "2ee881decbf04d1cbc933c20ab8bc714",
      "e879cb6793ff45e4886866587f71db62",
      "08f0ea7c403040968f079e0e694286b7",
      "04aac2c8d09544f88e1b50f7ac048ca7",
      "22f0476c8b2f48a8868bd2216ed4d7cf",
      "abc2dd3740504f31bcc05d4ca007e658",
      "d0ab580d14614370b83ce50e456b1e75"
     ]
    },
    "id": "O3dmAEtuQ9l5",
    "outputId": "b4634db3-939c-4a67-c6f8-8975af1ed448",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rnn_type = 'lstm'\n",
    "\n",
    "# Training\n",
    "model_lstm_noquant = train_or_load_model(configs, train_loader, val_loader, rnn_type, device, quantize=False)\n",
    "\n",
    "# Evaluation\n",
    "test_preds_lstm_noquant, test_loss_lstm_noquant, test_accuracy_lstm_noquant = evaluate_model(model_lstm_noquant, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "model_lstm_quant = train_or_load_model(configs, train_loader, val_loader, rnn_type, device, quantize=True)\n",
    "\n",
    "# Evaluation\n",
    "test_preds_lstm_quant, test_loss_lstm_quant, test_accuracy_lstm_quant = evaluate_model(model_lstm_quant, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percent Difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results\n",
    "percent_diff_lstm = mean_percent_diff(test_preds_lstm_quant, test_preds_lstm_noquant)\n",
    "percent_diff_in_test_accuracy_lstm = 100 * abs(test_accuracy_lstm_quant - test_accuracy_lstm_noquant) / test_accuracy_lstm_noquant\n",
    "percent_diff_in_test_loss_lstm = 100 * abs(test_loss_lstm_quant - test_loss_lstm_noquant) / test_loss_lstm_noquant\n",
    "print(f\"Mean Percent Difference between outputs (non-quantized vs quantized) LSTM: {percent_diff_lstm:.2f}%\")\n",
    "print(f'Mean Percent Difference between accuracy (non-quantized vs quantized) LSTM: {percent_diff_in_test_accuracy_lstm:.2f}%')\n",
    "print(f'Mean Percent Difference between loss (non-quantized vs quantized) LSTM: {percent_diff_in_test_loss_lstm:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bliBob-oRT-A"
   },
   "source": [
    "### GRU:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yItI-2tcPEZf"
   },
   "outputs": [],
   "source": [
    "rnn_type = 'gru'\n",
    "\n",
    "# Training\n",
    "model_gru = train_or_load_model(configs, train_loader, val_loader, rnn_type, device)\n",
    "\n",
    "# Evaluation\n",
    "test_preds_gru_noquant, test_loss_gru_noquant, test_accuracy_gru_noquant = evaluate_model(model_gru, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model_gru_quant = train_or_load_model(configs, train_loader, val_loader, rnn_type, device, quantize=True)\n",
    " \n",
    "# Evaluation\n",
    "test_preds_gru_quant, test_loss_gru_quant, test_accuracy_gru_quant = evaluate_model(model_gru_quant, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percent Difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results\n",
    "percent_diff_gru = mean_percent_diff(test_preds_gru_quant, test_preds_gru_noquant)\n",
    "percent_diff_in_test_accuracy_gru = 100 * abs(test_accuracy_gru_quant - test_accuracy_gru_noquant) / test_accuracy_gru_noquant\n",
    "percent_diff_in_test_loss_gru = 100 * abs(test_loss_gru_quant - test_loss_gru_noquant) / test_loss_gru_noquant\n",
    "print(f\"Mean Percent Difference between outputs (non-quantized vs quantized): {percent_diff_gru:.2f}%\")\n",
    "print(f'Mean Percent Difference between accuracy (non-quantized vs quantized): {percent_diff_in_test_accuracy_gru:.2f}%')\n",
    "print(f'Mean Percent Difference between loss (non-quantized vs quantized): {percent_diff_in_test_loss_gru:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01289697a46a4749808002f2a6eaaf73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1f95063a92b48e0b5ca86e7a6716236",
      "placeholder": "​",
      "style": "IPY_MODEL_abf1a93ee34843578018f72eeb64a9ea",
      "value": "  0%"
     }
    },
    "04aac2c8d09544f88e1b50f7ac048ca7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "08f0ea7c403040968f079e0e694286b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "18429f5771a24ae1a72150c16e7f97d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22f0476c8b2f48a8868bd2216ed4d7cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2ee881decbf04d1cbc933c20ab8bc714": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "497e7dd3069a4b5dbc883e4695332963": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ab32519140341eebad1e15228deb3e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6b7700e80d854914ad4f75c34e338d3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_77b06e361d74438fad7a17c8f34c0aa1",
       "IPY_MODEL_c2ba33ed9a314536bffb427470ee1957",
       "IPY_MODEL_b15dd251352142d39fb0c1a46cf9b388"
      ],
      "layout": "IPY_MODEL_2ee881decbf04d1cbc933c20ab8bc714"
     }
    },
    "77b06e361d74438fad7a17c8f34c0aa1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e879cb6793ff45e4886866587f71db62",
      "placeholder": "​",
      "style": "IPY_MODEL_08f0ea7c403040968f079e0e694286b7",
      "value": ""
     }
    },
    "95eca35a46a14aa4abf7c91b1ccee065": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_01289697a46a4749808002f2a6eaaf73",
       "IPY_MODEL_9ff6b3cf75b746f5a11dc7a7eadc2009",
       "IPY_MODEL_e86b3864d880446081ae53689493f24e"
      ],
      "layout": "IPY_MODEL_c275aca6f2c84125923cbd7246786078"
     }
    },
    "9ff6b3cf75b746f5a11dc7a7eadc2009": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18429f5771a24ae1a72150c16e7f97d3",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a583468ab883499e9d8cc9dadac9c597",
      "value": 0
     }
    },
    "a583468ab883499e9d8cc9dadac9c597": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "abc2dd3740504f31bcc05d4ca007e658": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abf1a93ee34843578018f72eeb64a9ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b15dd251352142d39fb0c1a46cf9b388": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_abc2dd3740504f31bcc05d4ca007e658",
      "placeholder": "​",
      "style": "IPY_MODEL_d0ab580d14614370b83ce50e456b1e75",
      "value": " 0/? [00:00&lt;?, ?it/s]"
     }
    },
    "b1f95063a92b48e0b5ca86e7a6716236": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c275aca6f2c84125923cbd7246786078": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2ba33ed9a314536bffb427470ee1957": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04aac2c8d09544f88e1b50f7ac048ca7",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_22f0476c8b2f48a8868bd2216ed4d7cf",
      "value": 0
     }
    },
    "d0ab580d14614370b83ce50e456b1e75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e86b3864d880446081ae53689493f24e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_497e7dd3069a4b5dbc883e4695332963",
      "placeholder": "​",
      "style": "IPY_MODEL_5ab32519140341eebad1e15228deb3e2",
      "value": " 0/50 [00:00&lt;?, ?it/s]"
     }
    },
    "e879cb6793ff45e4886866587f71db62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
