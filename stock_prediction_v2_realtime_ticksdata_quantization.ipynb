{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a816d605-de9a-4ca5-90f5-1842d223e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import json\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a9a0bd-e4c8-4780-a42a-0bc04d864aae",
   "metadata": {},
   "source": [
    "## Data Preparation & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0515e674-b12e-4960-96bb-58feb7018379",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparation():\n",
    "    def __init__(self, dataset_filename, num_steps=None, input_size=None, configs_filename='configs', seed=42):\n",
    "        self.num_steps = num_steps\n",
    "        self.input_size = input_size\n",
    "        self.set_seed(seed)\n",
    "        self.load_configs(configs_filename)\n",
    "        self.load_data(dataset_filename)\n",
    "        \n",
    "    def set_seed(self, seed):\n",
    "        np.random.seed(seed)  # Set seed for numpy\n",
    "        random.seed(seed)  # Set seed for random\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.manual_seed(seed)  # Set seed for PyTorch CPU\n",
    "        \n",
    "        torch.cuda.manual_seed(seed)  # Set seed for PyTorch GPU\n",
    "        torch.cuda.manual_seed_all(seed)  # Set seed for all GPUs\n",
    "        torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior for CUDA\n",
    "        torch.backends.cudnn.benchmark = False  # Disable the auto-tuner for GPUs\n",
    "    \n",
    "    def load_configs(self, configs_filename):\n",
    "        # Load hyperparameters\n",
    "        with open(f'configs/{configs_filename}.json', 'r') as file:\n",
    "            self.hyperparams = json.load(file)\n",
    "            \n",
    "    def load_data(self, dataset_filename):\n",
    "        self.data = pd.read_csv('data/' + dataset_filename + '.csv')\n",
    "        # self.data = self.data.resample(period).mean()  # Resample data based on the period\n",
    "        \n",
    "        # Extract attributes\n",
    "        if self.num_steps is None:\n",
    "            self.num_steps = self.hyperparams['num_steps']  # Extract number of steps\n",
    "        \n",
    "        if self.input_size is None:\n",
    "            self.input_size = self.hyperparams['input_size']  # Extract input size\n",
    "        \n",
    "        self.val_split = self.hyperparams['val_split']  # Extract validation split\n",
    "        self.test_split = self.hyperparams['test_split']  # Extract test split\n",
    "        self.batch_size = self.hyperparams['batch_size'] # Extract batch size\n",
    "        self.num_workers = self.hyperparams['num_workers'] # Extract number of workers (for GPU)\n",
    "        self.volume_threshold = self.hyperparams['volume_threshold'] # Extract volume threshold (for main contract)\n",
    "        self.open_interest_threshold = self.hyperparams['open_interest_threshold'] # Extract open interest threshold (for main contract)\n",
    "        self.time_interval = self.hyperparams['time_interval'] # Extract time interval of data (in seconds)\n",
    "\n",
    "        # Extract main contract\n",
    "        self.data = extract_high_frequency_trading(self.data, volume_threshold=self.volume_threshold, open_interest_threshold=self.open_interest_threshold, time_interval=self.time_interval)\n",
    "        \n",
    "        # Normalize data\n",
    "        self.data = self.normalize_data(self.data)\n",
    "\n",
    "        # Assert no NaN values\n",
    "        assert(self.data.isna().sum().sum() == 0)\n",
    "\n",
    "        # Create sequences (with sliding windows)\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = self.create_sequences(self.data, self.input_size, self.num_steps, self.val_split, self.test_split)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "        \n",
    "        assert(not torch.isnan(X_train_tensor).any())\n",
    "        assert(not torch.isnan(y_train_tensor).any())\n",
    "        assert(not torch.isnan(X_val_tensor).any())\n",
    "        assert(not torch.isnan(y_val_tensor).any())\n",
    "        assert(not torch.isnan(X_test_tensor).any())\n",
    "        assert(not torch.isnan(y_test_tensor).any())\n",
    "        \n",
    "        # Create DataLoader instances\n",
    "        self.train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, pin_memory=True)\n",
    "        self.val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, pin_memory=True)\n",
    "        self.test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, pin_memory=True)\n",
    "\n",
    "    def normalize_data(self, data):\n",
    "        # Apply MinMaxScaler to normalize data\n",
    "        scaler = MinMaxScaler()\n",
    "        normalized_data = scaler.fit_transform(data)\n",
    "        return pd.DataFrame(normalized_data, index=data.index, columns=data.columns)\n",
    "    \n",
    "    def create_sequences(self, data, input_size, num_steps, val_split=0.1, test_split=0.1):\n",
    "        # Convert data to numpy array\n",
    "        data = np.array(data)\n",
    "        \n",
    "        # Roll data\n",
    "        data = [np.array(data[i * input_size: (i + 1) * input_size]) \n",
    "           for i in range(len(data) // input_size)]\n",
    "\n",
    "        # Convert data to numpy array\n",
    "        data = np.array(data)\n",
    "        \n",
    "        # Check if data has any NaNs\n",
    "        if np.isnan(data).any():\n",
    "            raise ValueError(\"Data contains NaN values. Please clean the data before proceeding.\")\n",
    "    \n",
    "        # Replace zeros in the last price with a small value to avoid division by zero\n",
    "        last_price_index = 0\n",
    "        data[:, :, last_price_index] = np.where(data[:, :, last_price_index] == 0, 1e-6, data[:, :, last_price_index])  # Small constant to avoid division by zero\n",
    "        \n",
    "        # Normalize the last_price (predict relative change rates)\n",
    "        first_price = data[0, :, last_price_index]\n",
    "        data[:, :, last_price_index] = (data[:, :, last_price_index] / first_price) - 1.0\n",
    "    \n",
    "        # Check for remaining NaNs after normalization\n",
    "        if np.isnan(data[:, :, last_price_index]).any():\n",
    "            raise ValueError(\"Data contains NaN values after normalization. Please check the normalization process.\")\n",
    "    \n",
    "        # Split into groups of `num_steps`\n",
    "        X = np.array([data[i: i + num_steps] for i in range(len(data) - num_steps)])\n",
    "        assert(X.shape == (len(X), num_steps, input_size, data.shape[-1]))\n",
    "        y = np.array([data[i + num_steps, :, last_price_index] for i in range(len(data) - num_steps)])\n",
    "        assert(y.shape == (len(X), input_size))\n",
    "        \n",
    "        # Ensure X and y have the same number of samples after splitting\n",
    "        assert len(X) == len(y), \"Number of samples in X and y must be equal\"\n",
    "    \n",
    "        # Split into train, validation, and test sets\n",
    "        total_len = len(X)\n",
    "        test_start = int(total_len * (1 - test_split))\n",
    "        val_start = int(total_len * (1 - test_split - val_split))\n",
    "    \n",
    "        X_train, X_val, X_test = X[:val_start], X[val_start:test_start], X[test_start:]\n",
    "        y_train, y_val, y_test = y[:val_start], y[val_start:test_start], y[test_start:]\n",
    "    \n",
    "        return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "def extract_continuous_segment(data, volume_threshold, open_interest_threshold, time_interval):\n",
    "    \"\"\"\n",
    "    Extracts the largest continuous segment from high-frequency data based on given thresholds and time interval.\n",
    "    \n",
    "    Args:\n",
    "    data (pd.DataFrame): High-frequency data containing 'Timestamp', 'Volume', and 'OpenInterest' columns.\n",
    "    volume_threshold (int): Minimum volume threshold.\n",
    "    open_interest_threshold (int): Minimum open interest threshold.\n",
    "    time_interval (float): Maximum allowable time difference between consecutive data points (in seconds).\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Continuous segment of the main contract data.\n",
    "    \"\"\"\n",
    "    # Step 1: Remove rows with NaN values\n",
    "    cleaned_data = data.dropna()\n",
    "\n",
    "    # Step 2: Apply volume and open interest thresholds\n",
    "    filtered_data = cleaned_data[(cleaned_data['volume'] >= volume_threshold) & \n",
    "                                 (cleaned_data['open_interest'] >= open_interest_threshold)]\n",
    "    \n",
    "    # Step 3: Identify continuous segments\n",
    "    filtered_data = filtered_data.sort_values(by='datetime')\n",
    "\n",
    "    # Calculate time differences between consecutive rows\n",
    "    filtered_data['datetime_diff'] = filtered_data['datetime'].diff().dt.total_seconds()\n",
    "\n",
    "    # Identify segments where the time difference is larger than the time_interval threshold\n",
    "    filtered_data['segment'] = (filtered_data['datetime_diff'] > time_interval).cumsum()\n",
    "\n",
    "    # Find the length of each segment\n",
    "    segment_lengths = filtered_data['segment'].value_counts()\n",
    "\n",
    "    # Extract the largest segment\n",
    "    largest_segment = segment_lengths.idxmax()\n",
    "    continuous_data = filtered_data[filtered_data['segment'] == largest_segment]\n",
    "\n",
    "    # Drop the temporary columns\n",
    "    continuous_data = continuous_data.drop(columns=['datetime_diff', 'segment'])\n",
    "\n",
    "    return continuous_data\n",
    "\n",
    "\n",
    "def extract_high_frequency_trading(data, volume_threshold=1000, open_interest_threshold=500, time_interval=1):\n",
    "    # Convert datetime column to pandas datetime type\n",
    "    data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "    \n",
    "    # Identify high-frequency trading periods\n",
    "    # Assuming a significant increase in volume and consistent bid/ask prices indicates high-frequency trading\n",
    "    # Adjust the thresholds as necessary for your dataset\n",
    "    high_freq_trading = data[\n",
    "        (data['volume'] > volume_threshold) & \n",
    "        (data['open_interest'] > open_interest_threshold) & \n",
    "        (data['last_price'].notna()) & \n",
    "        (data['highest'].notna()) & \n",
    "        (data['volume'].notna()) & \n",
    "        (data['amount'].notna()) & \n",
    "        (data['bid_price1'].notna()) & \n",
    "        (data['ask_price1'].notna())\n",
    "    ]\n",
    "\n",
    "    # Additional filter to ensure high-frequency (optional)\n",
    "    # Extract largest continuous segment of high-frequency trading data\n",
    "    high_freq_trading = extract_continuous_segment(high_freq_trading, volume_threshold, open_interest_threshold, time_interval)\n",
    "    high_freq_trading = high_freq_trading.set_index('datetime')\n",
    "    \n",
    "    # Ensure No NaN values remaining\n",
    "    assert(high_freq_trading.isna().sum().sum() == 0)\n",
    "    \n",
    "    # Return the filtered dataset\n",
    "    return high_freq_trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "23afb5de-029d-407c-a3da-6f8b75f4fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'INE.sc2010'\n",
    "data_preparation = DataPreparation(dataset_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d10c93-2fa4-41f2-9b68-24606ba802c8",
   "metadata": {},
   "source": [
    "## Model Definition: LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "273efa4b-f195-4cf0-8c8e-d9ce9b501300",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CustomLSTMCell, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W_i = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_i = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_i = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "\n",
    "        self.W_f = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_f = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_f = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        self.W_c = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_c = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_c = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        self.W_o = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_o = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_o = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        # Layer normalization layers\n",
    "        self.ln_i = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_f = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_c = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_o = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'W_' in name or 'U_' in name:\n",
    "                nn.init.orthogonal_(param)  # Use orthogonal initialization\n",
    "            elif 'b_' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "            # Check for NaN values\n",
    "            assert(not torch.isnan(param).any())\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        print(f\"x shape: {x.shape}, h shape: {h.shape}\")\n",
    "        print(f\"W_i shape: {self.W_i.shape}, U_i shape: {self.U_i.shape}, b_i shape: {self.b_i.shape}\")\n",
    "        \n",
    "        i_t = torch.sigmoid(self.ln_i(torch.mm(x, self.W_i) + torch.mm(h, self.U_i) + self.b_i))\n",
    "        f_t = torch.sigmoid(self.ln_f(torch.mm(x, self.W_f) + torch.mm(h, self.U_f) + self.b_f))\n",
    "        g_t = torch.tanh(self.ln_c(torch.mm(x, self.W_c) + torch.mm(h, self.U_c) + self.b_c))\n",
    "        o_t = torch.sigmoid(self.ln_o(torch.mm(x, self.W_o) + torch.mm(h, self.U_o) + self.b_o))\n",
    "\n",
    "        c_t = f_t * c + i_t * g_t\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        \n",
    "        assert(not torch.isnan(h_t).any())\n",
    "        assert(not torch.isnan(c_t).any())\n",
    "        \n",
    "        return h_t, c_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c23a8c4-e52f-4cd4-9d48-79a1d23b8de6",
   "metadata": {},
   "source": [
    "## Model Definition: GRU Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ad7c0b79-da6e-4bbb-8c6c-e9066b42e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGRUCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CustomGRUCell, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W_z = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_z = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_z = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "\n",
    "        self.W_r = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_r = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_r = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        self.W_h = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_h = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_h = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        # Layer normalization layers\n",
    "        self.ln_z = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_r = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_h = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'W_' in name or 'U_' in name:\n",
    "                nn.init.xavier_uniform_(param)  # Use orthogonal initialization\n",
    "            elif 'b_' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "            # Check for NaN values\n",
    "            assert(not torch.isnan(param).any())\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        z_t = torch.sigmoid(self.ln_z(torch.mm(x, self.W_z) + torch.mm(h, self.U_z) + self.b_z))\n",
    "        r_t = torch.sigmoid(self.ln_r(torch.mm(x, self.W_r) + torch.mm(h, self.U_r) + self.b_r))\n",
    "        h_hat_t = torch.tanh(self.ln_h(torch.mm(x, self.W_h) + torch.mm(r_t * h, self.U_h) + self.b_h))\n",
    "\n",
    "        h_t = (1 - z_t) * h + z_t * h_hat_t\n",
    "        \n",
    "        assert(not torch.isnan(h_t).any())\n",
    "    \n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d56002-e909-4460-a087-92d9f4f32749",
   "metadata": {},
   "source": [
    "## Model Definition: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ab261366-d0cd-4995-b9c1-6a51d242543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout_rate, dense_units, cell_type='lstm'):\n",
    "        super(CustomRNNModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "        if cell_type == 'lstm':\n",
    "            self.layers = nn.ModuleList([CustomLSTMCell(input_dim, hidden_dim)])\n",
    "            self.layers.extend([CustomLSTMCell(hidden_dim, hidden_dim) for _ in range(num_layers - 1)])\n",
    "        elif cell_type == 'gru':\n",
    "            self.layers = nn.ModuleList([CustomGRUCell(input_dim, hidden_dim)])\n",
    "            self.layers.extend([CustomGRUCell(hidden_dim, hidden_dim) for _ in range(num_layers - 1)])\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported cell type\")\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden_dim, dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        h_t = [torch.zeros(batch_size, self.hidden_dim, device=x.device) for _ in range(self.num_layers)]\n",
    "        c_t = [torch.zeros(batch_size, self.hidden_dim, device=x.device) for _ in range(self.num_layers)]\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]\n",
    "            for layer in range(self.num_layers):\n",
    "                if self.cell_type == 'lstm':\n",
    "                    h_t[layer], c_t[layer] = self.layers[layer](x_t, h_t[layer], c_t[layer])\n",
    "                else:\n",
    "                    h_t[layer] = self.layers[layer](x_t, h_t[layer])\n",
    "                x_t = h_t[layer]\n",
    "\n",
    "        x = self.dropout(x_t)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74bacfb-2f29-4dd9-98fb-378abee23041",
   "metadata": {},
   "source": [
    "## RNN Model Training + Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1ba6949c-3daf-49d5-a5f9-ab13c518cd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss definitions \n",
    "def quantile_loss(outputs, targets, quantile=0.5):\n",
    "    errors = targets - outputs\n",
    "    loss = torch.max((quantile - 1) * errors, quantile * errors)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "class HingeLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HingeLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        return torch.mean(torch.clamp(1 - targets * outputs, min=0))\n",
    "\n",
    "def directional_loss(outputs, targets):\n",
    "    return torch.mean(torch.abs(torch.sign(outputs) - torch.sign(targets)))\n",
    "\n",
    "def choose_loss_function(loss, quantile=None):\n",
    "    if loss == 'huber':  \n",
    "        criterion = nn.SmoothL1Loss() \n",
    "    elif loss == 'mse': \n",
    "        criterion = nn.MSELoss()\n",
    "    elif loss == 'quantile' and quantile is not None:\n",
    "        criterion = quantile_loss(quantile=quantile)\n",
    "    elif loss == 'hinge':\n",
    "        criterion = HingeLoss()\n",
    "    elif loss == 'directional':\n",
    "        criterion = directional_loss()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported loss function\")\n",
    "\n",
    "    return criterion\n",
    "\n",
    "class TrainAndPredict():\n",
    "    def __init__(self, data_preparation, cell_type, train_needed=None):\n",
    "        self.train_needed = train_needed\n",
    "        self.test_loss, self.classification_accuracy, self.duration = self.train_and_predict(data_preparation, cell_type)\n",
    "\n",
    "    # Extract hyperparameters\n",
    "    def extract_hyperparams(self, dp, cell_type):\n",
    "        num_units = dp.hyperparams[cell_type][cell_type + '_units']\n",
    "        num_layers = dp.hyperparams[cell_type][cell_type + '_layers']\n",
    "        dropout_rate = dp.hyperparams[cell_type]['dropout_rate']\n",
    "        dense_units = dp.hyperparams[cell_type]['dense_units']\n",
    "        init_learning_rate = dp.hyperparams[cell_type]['init_learning_rate']\n",
    "        learning_rate_decay = dp.hyperparams[cell_type]['learning_rate_decay']\n",
    "        init_epochs = dp.hyperparams[cell_type]['init_epochs']\n",
    "        max_epochs = dp.hyperparams[cell_type]['max_epochs']\n",
    "        early_stop_patience = dp.hyperparams[cell_type].get('early_stop_patience', None)\n",
    "        \n",
    "        if self.train_needed is None:\n",
    "            self.train_needed = dp.hyperparams[cell_type]['pretrain'] # Whether to train the model\n",
    "        \n",
    "        return num_units, num_layers, dropout_rate, dense_units, init_learning_rate, learning_rate_decay, init_epochs, max_epochs, early_stop_patience, self.train_needed  \n",
    "\n",
    "    # Train the model\n",
    "    def train_model(self, model, train_loader, val_loader, criterion, init_epochs, num_epochs, init_learning_rate, learning_rate_decay, device, early_stop_patience=None, cell_type='lstm'):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        best_val_loss = float('inf')\n",
    "        early_stop_counter = 0\n",
    "    \n",
    "        optimizer = optim.Adam(model.parameters(), lr=init_learning_rate)\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0.0)\n",
    "    \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            current_lr = init_learning_rate * (learning_rate_decay ** max(float(epoch + 1 - init_epochs), 0.0))\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = current_lr\n",
    "    \n",
    "            train_loss = 0.0\n",
    "            for X_batch, y_batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                # Reshape X_batch to (batch_size, num_steps, input_size, data.shape[-1])\n",
    "                X_batch = X_batch.view(-1, X_batch.size(1), X_batch.size(2) * X_batch.size(3))\n",
    "                outputs = model(X_batch)\n",
    "                # Reshape outputs to match y_batch shape (batch_size, input_size)\n",
    "                outputs = outputs.view(-1, y_batch.size(1))\n",
    "                \n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                train_loss += loss.item()\n",
    "    \n",
    "            train_loss /= len(train_loader)\n",
    "            train_losses.append(train_loss)\n",
    "    \n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    # Reshape X_batch to (batch_size, num_steps, input_size, data.shape[-1])\n",
    "                    X_batch = X_batch.view(-1, X_batch.size(1), X_batch.size(2) * X_batch.size(3))\n",
    "                    outputs = model(X_batch)\n",
    "                    # Reshape outputs to match y_batch shape (batch_size, input_size)\n",
    "                    outputs = outputs.view(-1, y_batch.size(1))\n",
    "                    \n",
    "                    val_loss += criterion(outputs, y_batch).item()\n",
    "    \n",
    "            val_loss /= len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "    \n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "            scheduler.step()\n",
    "    \n",
    "            if early_stop_patience is not None:\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    early_stop_counter = 0\n",
    "                    torch.save(model.state_dict(), f'models/SP500_{cell_type}_model.pth')\n",
    "                else:\n",
    "                    early_stop_counter += 1\n",
    "                    if early_stop_counter >= early_stop_patience:\n",
    "                        print(\"Early stopping triggered\")\n",
    "                        break\n",
    "    \n",
    "        return train_losses, val_losses\n",
    "\n",
    "    def make_predictions(self, model, data_loader, device):\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in data_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                # Reshape X_batch to (batch_size, num_steps, input_size, data.shape[-1])\n",
    "                X_batch = X_batch.view(-1, X_batch.size(1), X_batch.size(2) * X_batch.size(3))\n",
    "                outputs = model(X_batch)\n",
    "                # Reshape outputs to match y_batch shape (batch_size, input_size)\n",
    "                outputs = outputs.view(-1, y_batch.size(1))\n",
    "                \n",
    "                predictions.extend(outputs.cpu().numpy())\n",
    "                actuals.extend(y_batch.cpu().numpy())\n",
    "                test_loss += criterion(outputs, y_batch).item()\n",
    "        return np.array(predictions).flatten(), np.array(actuals).flatten(), test_loss\n",
    "\n",
    "    # Function to load the model\n",
    "    def load_model(self, model, cell_type, device):\n",
    "        model_path = f'models/SP500_{cell_type}_model.pth'\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.to(device)\n",
    "        return model\n",
    "\n",
    "    # Plot the predictions against the actual values\n",
    "    def plot_results(self, data_preparation, actuals, predictions):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        x = np.arange(len(actuals))\n",
    "        plt.bar(x - 0.2, actuals, label='Actual Prices')\n",
    "        plt.bar(x + 0.2, predictions, label='Predicted Prices')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Normalized Price')\n",
    "        plt.ylim(-0.1, 0.1)\n",
    "        plt.title(f'Predicted vs Actual Prices for input_size={data_preparation.input_size}, num_steps={data_preparation.num_steps}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    # Check for overfitting/underfitting\n",
    "    def plot_losses(self, train_losses, val_losses):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def train_and_predict(self, data_preparation, cell_type):\n",
    "        # Initialize model\n",
    "        num_units, num_layers, dropout_rate, dense_units, init_learning_rate, learning_rate_decay, init_epochs, max_epochs, early_stop_patience, train_needed = self.extract_hyperparams(data_preparation, cell_type)\n",
    "        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        model = CustomRNNModel(input_dim=data_preparation.input_size, hidden_dim=num_units, output_dim=data_preparation.input_size, num_layers=num_layers, dropout_rate=dropout_rate, dense_units=dense_units, cell_type=cell_type).to(device)\n",
    "        \n",
    "        # Train or load the model\n",
    "        if train_needed:\n",
    "            # Train the model\n",
    "            criterion = choose_loss_function('huber')\n",
    "            t1 = time.time()\n",
    "            train_losses, val_losses = self.train_model(model, data_preparation.train_loader, data_preparation.val_loader, criterion, init_epochs, max_epochs, init_learning_rate, learning_rate_decay, device, early_stop_patience, cell_type=cell_type)\n",
    "            duration = time.time() - t1\n",
    "            \n",
    "            torch.save(model.state_dict(), f'models/SP500_{cell_type}_model.pth')\n",
    "        else:\n",
    "            # Load the model\n",
    "            model = self.load_model(model, cell_type, device)\n",
    "            print(\"Model loaded successfully\")\n",
    "        \n",
    "        # Get a model summary\n",
    "        print(model)\n",
    "        \n",
    "        # Get predictions and actual values\n",
    "        predictions, actuals, test_loss = self.make_predictions(model, data_preparation.test_loader, device)\n",
    "        \n",
    "        # Plot the predictions against the actual values\n",
    "        self.plot_results(data_preparation, actuals, predictions)\n",
    "        \n",
    "        # Check for overfitting/underfitting\n",
    "        self.plot_losses(train_losses, val_losses)\n",
    "        \n",
    "        # Classification rate\n",
    "        classification_accuracy = np.mean((predictions > 0) == (actuals > 0))\n",
    "    \n",
    "        return test_loss, classification_accuracy, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e40914a2-017e-4747-a9b2-cc17aa976ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                                         | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([64, 52]), h shape: torch.Size([64, 64])\n",
      "W_i shape: torch.Size([2, 64]), U_i shape: torch.Size([64, 64]), b_i shape: torch.Size([64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x52 and 2x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m cell_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m evaluation_results \u001b[38;5;241m=\u001b[39m \u001b[43mTrainAndPredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_preparation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_needed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[130], line 36\u001b[0m, in \u001b[0;36mTrainAndPredict.__init__\u001b[0;34m(self, data_preparation, cell_type, train_needed)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_preparation, cell_type, train_needed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_needed \u001b[38;5;241m=\u001b[39m train_needed\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassification_accuracy, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_preparation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[130], line 184\u001b[0m, in \u001b[0;36mTrainAndPredict.train_and_predict\u001b[0;34m(self, data_preparation, cell_type)\u001b[0m\n\u001b[1;32m    182\u001b[0m criterion \u001b[38;5;241m=\u001b[39m choose_loss_function(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuber\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    183\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 184\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_preparation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_preparation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_learning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop_patience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcell_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t1\n\u001b[1;32m    187\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/SP500_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcell_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[130], line 78\u001b[0m, in \u001b[0;36mTrainAndPredict.train_model\u001b[0;34m(self, model, train_loader, val_loader, criterion, init_epochs, num_epochs, init_learning_rate, learning_rate_decay, device, early_stop_patience, cell_type)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Reshape X_batch to (batch_size, num_steps, input_size, data.shape[-1])\u001b[39;00m\n\u001b[1;32m     77\u001b[0m X_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, X_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), X_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m X_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m---> 78\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Reshape outputs to match y_batch shape (batch_size, input_size)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, y_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[129], line 31\u001b[0m, in \u001b[0;36mCustomRNNModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 31\u001b[0m         h_t[layer], c_t[layer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_t\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_t\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m         h_t[layer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[layer](x_t, h_t[layer])\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_m1/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[127], line 45\u001b[0m, in \u001b[0;36mCustomLSTMCell.forward\u001b[0;34m(self, x, h, c)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, h shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW_i shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_i\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, U_i shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU_i\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, b_i shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_i\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m i_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_i(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_i\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(h, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU_i) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_i))\n\u001b[1;32m     46\u001b[0m f_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(torch\u001b[38;5;241m.\u001b[39mmm(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_f) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(h, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU_f) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_f))\n\u001b[1;32m     47\u001b[0m g_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_c(torch\u001b[38;5;241m.\u001b[39mmm(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_c) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(h, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU_c) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_c))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x52 and 2x64)"
     ]
    }
   ],
   "source": [
    "cell_type = 'lstm'\n",
    "evaluation_results = TrainAndPredict(data_preparation, cell_type, train_needed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5fa42f-e2d0-4e2c-8d8e-a940282be4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type = 'gru'\n",
    "evaluation_results = TrainAndPredict(data_preparation, cell_type, train_needed=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b090a4e-768d-4f17-a712-e6777d506fe5",
   "metadata": {},
   "source": [
    "## Hyperparameter Search over `input_size` and `num_steps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e915769c-f17f-4104-8fc2-7c11e0617632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(cell_type, input_size_values, num_steps_values):\n",
    "    # Define dataframe\n",
    "    df = pd.DataFrame(columns=['Input Size', 'Number of Steps', 'Test Loss', 'Classification Accuracy', 'Duration'])\n",
    "    \n",
    "    # Define test losses & classification accuracies\n",
    "    input_size_combination_values = []\n",
    "    num_steps_combination_values = []\n",
    "\n",
    "    # Define hyperparam combinations\n",
    "    test_losses = []\n",
    "    classification_accuracies = []\n",
    "    durations = []\n",
    "    \n",
    "    # Loop over the hyperparam values\n",
    "    for input_size in input_size_values:\n",
    "        for num_steps in num_steps_values:\n",
    "            # Add to hyperparam combination\n",
    "            input_size_combination_values.append(input_size)\n",
    "            num_steps_combination_values.append(num_steps)\n",
    "            \n",
    "            # Modify the input_size and num_steps attributes\n",
    "            data_preparation = DataPreparation(dataset_name, num_steps=num_steps, input_size=input_size)\n",
    "            \n",
    "            # Train & Evaluate model\n",
    "            evaluation_results = TrainAndPredict(data_preparation, cell_type, train_needed=True)\n",
    "            test_losses.append(evaluation_results.test_loss)\n",
    "            classification_accuracies.append(evaluation_results.classification_accuracy)   \n",
    "            durations.append(evaluation_results.duration)\n",
    "    \n",
    "    df['Input Size'] = input_size_combination_values\n",
    "    df['Number of Steps'] = num_steps_combination_values\n",
    "    df['Test Loss'] = test_losses\n",
    "    df['Classification Accuracy'] = classification_accuracies\n",
    "    df['Duration'] = durations\n",
    "    \n",
    "    df.to_csv(f'results/{cell_type}_evaluation_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf0929d-6959-47e3-aea3-ef307d20779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(cell_type, input_size_values, num_steps_values, SEARCH_NEEDED=False):\n",
    "    # Perform grid search if needed (setting SEARCH_NEEDED = TRUE will RUN LONG TIME!!!)\n",
    "    if SEARCH_NEEDED:\n",
    "        grid_search(cell_type, input_size_values, num_steps_values)\n",
    "    \n",
    "    # Get best model\n",
    "    df = pd.read_csv(f'results/{cell_type}_evaluation_results.csv')\n",
    "    idx = np.argmin(df['Test Loss'])\n",
    "    df_best_model = df.iloc[idx, :]\n",
    "    return df_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec64117-eef8-4ec1-93fa-08aab4760824",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size_values = [2, 3, 5, 10, 20]\n",
    "num_steps_values = [3, 10, 20, 30, 40, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a622b-c8f5-479c-b921-5692cb4682a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM (setting SEARCH_NEEDED = TRUE will RUN LONG TIME!!!)\n",
    "cell_type = 'lstm'\n",
    "SEARCH_NEEDED_LSTM = False\n",
    "df_best_model = get_best_model(cell_type, input_size_values, num_steps_values, SEARCH_NEEDED=SEARCH_NEEDED_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0518331-2739-4344-bf18-d35ee9eb49db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb045fd-b9b2-480d-8dc2-ab0109f024e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the input_size and num_steps attributes\n",
    "data_preparation = DataPreparation(dataset_name, num_steps=int(df_best_model['Number of Steps']), input_size=int(df_best_model['Input Size']))\n",
    "\n",
    "# Train & Evaluate model\n",
    "evaluation_results = TrainAndPredict(data_preparation, cell_type, train_needed=True)\n",
    "print(\"Test loss of best model:\", evaluation_results.test_loss)\n",
    "print(\"Classification accuracy of best model:\", evaluation_results.classification_accuracy)\n",
    "print(\"Duration of best model:\", evaluation_results.duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29927b0-71bb-4c9d-acae-f7d06ccd2a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU (setting SEARCH_NEEDED = TRUE will RUN LONG TIME!!!)\n",
    "cell_type = 'gru'\n",
    "SEARCH_NEEDED_GRU = True\n",
    "df_best_model = get_best_model(cell_type, input_size_values, num_steps_values, SEARCH_NEEDED=SEARCH_NEEDED_GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80a94f7-7f3b-4881-9148-c6b4e8d410ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e47baf4-4392-4c4e-9ccf-83ab2ccaecf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b10df20-68dc-4170-9ba0-0ad0c4d3990c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05432447-658e-4b8d-9fa4-c0cc10612f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7254237d-ce87-489c-9d2a-3d4ef17e7d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4600a91-ae44-4e47-9b38-9439bb11702f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de33b4a4-9a04-4027-8815-4711ad8037d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232d58c2-0fc3-44b0-8311-56220c75d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.special import kl_div\n",
    "\n",
    "# # Sample data for demonstration purposes\n",
    "# # In practice, you would use actual activation data from the model\n",
    "# def get_fp32_activation_data():\n",
    "#     # Replace this with actual data collection\n",
    "#     return np.array([0.5, 1.0, 2.0, -1.0, -0.5, 3.0, -3.0, 0.1])\n",
    "\n",
    "# # **1. Calibration: Collect Data and Compute Scale Factor**\n",
    "\n",
    "# # Collect activation data (replace with actual FP32 data)\n",
    "# activation_data_fp32 = get_fp32_activation_data()\n",
    "\n",
    "# # Step 1: Determine the maximum absolute value\n",
    "# max_abs_value = np.max(np.abs(activation_data_fp32))\n",
    "\n",
    "# # Step 2: Compute the scale factor for symmetric quantization\n",
    "# def compute_scale(max_abs_value):\n",
    "#     return max_abs_value / 127  # 127 because 8-bit quantization uses values from -128 to 127\n",
    "\n",
    "# scale = compute_scale(max_abs_value)\n",
    "\n",
    "# # Step 3: Perform preliminary quantization of FP32 data\n",
    "# def preliminary_quantize(fp32_data, scale):\n",
    "#     return np.clip(np.round(fp32_data / scale), -128, 127).astype(np.int8)\n",
    "\n",
    "# activation_data_int8 = preliminary_quantize(activation_data_fp32, scale)\n",
    "\n",
    "# # Step 4: Compute histograms for FP32 and INT8\n",
    "# def compute_histograms(fp32_data, int8_data):\n",
    "#     hist_fp32, _ = np.histogram(fp32_data, bins=2048, range=(-128, 127), density=True)\n",
    "#     hist_int8, _ = np.histogram(int8_data, bins=2048, range=(-128, 127), density=True)\n",
    "#     return hist_fp32, hist_int8\n",
    "\n",
    "# hist_fp32, hist_int8 = compute_histograms(activation_data_fp32, activation_data_int8)\n",
    "\n",
    "# # Step 5: Compute KL divergence\n",
    "# def compute_kl_divergence(hist_fp32, hist_int8):\n",
    "#     # Adding a small constant to avoid log(0)\n",
    "#     return np.sum(kl_div(hist_fp32 + 1e-8, hist_int8 + 1e-8))\n",
    "\n",
    "# kl_divergence = compute_kl_divergence(hist_fp32, hist_int8)\n",
    "# print(f\"KL Divergence: {kl_divergence}\")\n",
    "\n",
    "# # **2. Quantization: Convert FP32 to INT8**\n",
    "\n",
    "# # Final quantization of FP32 data\n",
    "# def quantize_to_int8(fp32_data, scale):\n",
    "#     return np.clip(np.round(fp32_data / scale), -128, 127).astype(np.int8)\n",
    "\n",
    "# activation_data_int8_final = quantize_to_int8(activation_data_fp32, scale)\n",
    "\n",
    "# # **3. INT32 Computations: Perform Layer Operations**\n",
    "\n",
    "# # Example INT32 computation function\n",
    "# def int32_computations(weights, activations, bias):\n",
    "#     # Perform INT32 matrix multiplication and add bias\n",
    "#     int32_result = np.dot(weights, activations) + bias\n",
    "#     return int32_result\n",
    "\n",
    "# # Sample weights and bias for demonstration\n",
    "# weights = np.array([[1, -1], [2, 3]])\n",
    "# bias = np.array([1, -1])\n",
    "\n",
    "# # Perform INT32 computations\n",
    "# int32_result = int32_computations(weights, activation_data_int8_final, bias)\n",
    "# print(f\"INT32 Computation Result: {int32_result}\")\n",
    "\n",
    "# # **4. Re-Quantization: Convert INT32 to INT8**\n",
    "\n",
    "# # Re-quantization process\n",
    "# def requantize(int32_activations, scale, zero_point, bias):\n",
    "#     # Add bias and then requantize\n",
    "#     int32_activations_with_bias = int32_activations + bias\n",
    "#     return np.clip(np.round(int32_activations_with_bias * scale) + zero_point, -128, 127).astype(np.int8)\n",
    "\n",
    "# # Assuming zero_point = 0 for symmetric quantization\n",
    "# zero_point = 0\n",
    "\n",
    "# # Re-quantize INT32 results to INT8\n",
    "# activation_data_int8_requantized = requantize(int32_result, scale, zero_point, bias)\n",
    "# print(f\"Re-Quantized INT8 Data: {activation_data_int8_requantized}\")\n",
    "\n",
    "# # **5. De-Quantization: Convert INT8 Back to FP32**\n",
    "\n",
    "# # De-quantization process\n",
    "# def dequantize_to_fp32(int8_data, scale, zero_point):\n",
    "#     return (int8_data - zero_point) * scale\n",
    "\n",
    "# # Convert INT8 results back to FP32\n",
    "# fp32_reconstructed_data = dequantize_to_fp32(activation_data_int8_requantized, scale, zero_point)\n",
    "# print(f\"De-Quantized FP32 Data: {fp32_reconstructed_data}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
