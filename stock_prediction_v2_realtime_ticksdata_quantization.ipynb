{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "a816d605-de9a-4ca5-90f5-1842d223e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import json\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a9a0bd-e4c8-4780-a42a-0bc04d864aae",
   "metadata": {},
   "source": [
    "## Data Preparation & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "0515e674-b12e-4960-96bb-58feb7018379",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparation():\n",
    "    def __init__(self, dataset_filename, num_steps=None, input_size=None, configs_filename='configs', seed=42):\n",
    "        self.num_steps = num_steps\n",
    "        self.input_size = input_size\n",
    "        self.set_seed(seed)\n",
    "        self.load_configs(configs_filename)\n",
    "        self.load_data(dataset_filename)\n",
    "        \n",
    "    def set_seed(self, seed):\n",
    "        np.random.seed(seed)  # Set seed for numpy\n",
    "        random.seed(seed)  # Set seed for random\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.manual_seed(seed)  # Set seed for PyTorch CPU\n",
    "        \n",
    "        torch.cuda.manual_seed(seed)  # Set seed for PyTorch GPU\n",
    "        torch.cuda.manual_seed_all(seed)  # Set seed for all GPUs\n",
    "        torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior for CUDA\n",
    "        torch.backends.cudnn.benchmark = False  # Disable the auto-tuner for GPUs\n",
    "    \n",
    "    def load_configs(self, configs_filename):\n",
    "        # Load hyperparameters\n",
    "        with open(f'configs/{configs_filename}.json', 'r') as file:\n",
    "            self.hyperparams = json.load(file)\n",
    "            \n",
    "    def load_data(self, dataset_filename):\n",
    "        self.data = pd.read_csv('data/' + dataset_filename + '.csv')\n",
    "        # self.data = self.data.resample(period).mean()  # Resample data based on the period\n",
    "        \n",
    "        # Extract attributes\n",
    "        if self.num_steps is None:\n",
    "            self.num_steps = self.hyperparams['num_steps']  # Extract number of steps\n",
    "        \n",
    "        if self.input_size is None:\n",
    "            self.input_size = self.hyperparams['input_size']  # Extract input size\n",
    "        \n",
    "        self.val_split = self.hyperparams['val_split']  # Extract validation split\n",
    "        self.test_split = self.hyperparams['test_split']  # Extract test split\n",
    "        self.batch_size = self.hyperparams['batch_size'] # Extract batch size\n",
    "        self.num_workers = self.hyperparams['num_workers'] # Extract number of workers (for GPU)\n",
    "        self.volume_threshold = self.hyperparams['volume_threshold'] # Extract volume threshold (for main contract)\n",
    "        self.open_interest_threshold = self.hyperparams['open_interest_threshold'] # Extract open interest threshold (for main contract)\n",
    "        self.time_interval = self.hyperparams['time_interval'] # Extract time interval of data (in seconds)\n",
    "\n",
    "        # Extract main contract\n",
    "        self.data = extract_high_frequency_trading(self.data, volume_threshold=self.volume_threshold, open_interest_threshold=self.open_interest_threshold, time_interval=self.time_interval)\n",
    "        \n",
    "        # Normalize data\n",
    "        self.data = self.normalize_data(self.data)\n",
    "\n",
    "        # Assert no NaN values\n",
    "        assert(self.data.isna().sum().sum() == 0)\n",
    "\n",
    "        # Create sequences (with sliding windows)\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = self.create_sequences(self.data, self.input_size, self.num_steps, self.val_split, self.test_split)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "        \n",
    "        assert(not torch.isnan(X_train_tensor).any())\n",
    "        assert(not torch.isnan(y_train_tensor).any())\n",
    "        assert(not torch.isnan(X_val_tensor).any())\n",
    "        assert(not torch.isnan(y_val_tensor).any())\n",
    "        assert(not torch.isnan(X_test_tensor).any())\n",
    "        assert(not torch.isnan(y_test_tensor).any())\n",
    "        \n",
    "        # Create DataLoader instances\n",
    "        self.train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, pin_memory=True)\n",
    "        self.val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, pin_memory=True)\n",
    "        self.test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, pin_memory=True)\n",
    "\n",
    "    def normalize_data(self, data):\n",
    "        # Apply MinMaxScaler to normalize data\n",
    "        scaler = MinMaxScaler()\n",
    "        normalized_data = scaler.fit_transform(data)\n",
    "        return pd.DataFrame(normalized_data, index=data.index, columns=data.columns)\n",
    "    \n",
    "    def create_sequences(self, data, input_size, num_steps, val_split=0.1, test_split=0.1):\n",
    "        # Convert data to numpy array\n",
    "        data = np.array(data)\n",
    "        \n",
    "        # Roll data\n",
    "        data = [np.array(data[i * input_size: (i + 1) * input_size]) \n",
    "           for i in range(len(data) // input_size)]\n",
    "\n",
    "        # Convert data to numpy array\n",
    "        data = np.array(data)\n",
    "        \n",
    "        # Check if data has any NaNs\n",
    "        if np.isnan(data).any():\n",
    "            raise ValueError(\"Data contains NaN values. Please clean the data before proceeding.\")\n",
    "    \n",
    "        # Replace zeros in the last price with a small value to avoid division by zero\n",
    "        last_price_index = 0\n",
    "        data[:, :, last_price_index] = np.where(data[:, :, last_price_index] == 0, 1e-6, data[:, :, last_price_index])  # Small constant to avoid division by zero\n",
    "        \n",
    "        # Normalize the last_price (predict relative change rates)\n",
    "        first_price = data[0, :, last_price_index]\n",
    "        data[:, :, last_price_index] = (data[:, :, last_price_index] / first_price) - 1.0\n",
    "    \n",
    "        # Check for remaining NaNs after normalization\n",
    "        if np.isnan(data[:, :, last_price_index]).any():\n",
    "            raise ValueError(\"Data contains NaN values after normalization. Please check the normalization process.\")\n",
    "    \n",
    "        # Split into groups of `num_steps`\n",
    "        X = np.array([data[i: i + num_steps] for i in range(len(data) - num_steps)])\n",
    "        assert(X.shape == (len(X), num_steps, input_size, data.shape[-1]))\n",
    "        y = np.array([data[i + num_steps, :, last_price_index] for i in range(len(data) - num_steps)])\n",
    "        assert(y.shape == (len(X), input_size))\n",
    "\n",
    "        self.num_features = data.shape[-1]\n",
    "        \n",
    "        # Ensure X and y have the same number of samples after splitting\n",
    "        assert len(X) == len(y), \"Number of samples in X and y must be equal\"\n",
    "    \n",
    "        # Split into train, validation, and test sets\n",
    "        total_len = len(X)\n",
    "        test_start = int(total_len * (1 - test_split))\n",
    "        val_start = int(total_len * (1 - test_split - val_split))\n",
    "    \n",
    "        X_train, X_val, X_test = X[:val_start], X[val_start:test_start], X[test_start:]\n",
    "        y_train, y_val, y_test = y[:val_start], y[val_start:test_start], y[test_start:]\n",
    "    \n",
    "        return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "def extract_continuous_segment(data, volume_threshold, open_interest_threshold, time_interval):\n",
    "    \"\"\"\n",
    "    Extracts the largest continuous segment from high-frequency data based on given thresholds and time interval.\n",
    "    \n",
    "    Args:\n",
    "    data (pd.DataFrame): High-frequency data containing 'Timestamp', 'Volume', and 'OpenInterest' columns.\n",
    "    volume_threshold (int): Minimum volume threshold.\n",
    "    open_interest_threshold (int): Minimum open interest threshold.\n",
    "    time_interval (float): Maximum allowable time difference between consecutive data points (in seconds).\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Continuous segment of the main contract data.\n",
    "    \"\"\"\n",
    "    # Step 1: Remove rows with NaN values\n",
    "    cleaned_data = data.dropna()\n",
    "\n",
    "    # Step 2: Apply volume and open interest thresholds\n",
    "    filtered_data = cleaned_data[(cleaned_data['volume'] >= volume_threshold) & \n",
    "                                 (cleaned_data['open_interest'] >= open_interest_threshold)]\n",
    "    \n",
    "    # Step 3: Identify continuous segments\n",
    "    filtered_data = filtered_data.sort_values(by='datetime')\n",
    "\n",
    "    # Calculate time differences between consecutive rows\n",
    "    filtered_data['datetime_diff'] = filtered_data['datetime'].diff().dt.total_seconds()\n",
    "\n",
    "    # Identify segments where the time difference is larger than the time_interval threshold\n",
    "    filtered_data['segment'] = (filtered_data['datetime_diff'] > time_interval).cumsum()\n",
    "\n",
    "    # Find the length of each segment\n",
    "    segment_lengths = filtered_data['segment'].value_counts()\n",
    "\n",
    "    # Extract the largest segment\n",
    "    largest_segment = segment_lengths.idxmax()\n",
    "    continuous_data = filtered_data[filtered_data['segment'] == largest_segment]\n",
    "\n",
    "    # Drop the temporary columns\n",
    "    continuous_data = continuous_data.drop(columns=['datetime_diff', 'segment'])\n",
    "\n",
    "    return continuous_data\n",
    "\n",
    "\n",
    "def extract_high_frequency_trading(data, volume_threshold=1000, open_interest_threshold=500, time_interval=1):\n",
    "    # Convert datetime column to pandas datetime type\n",
    "    data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "    \n",
    "    # Identify high-frequency trading periods\n",
    "    # Assuming a significant increase in volume and consistent bid/ask prices indicates high-frequency trading\n",
    "    # Adjust the thresholds as necessary for your dataset\n",
    "    high_freq_trading = data[\n",
    "        (data['volume'] > volume_threshold) & \n",
    "        (data['open_interest'] > open_interest_threshold) & \n",
    "        (data['last_price'].notna()) & \n",
    "        (data['highest'].notna()) & \n",
    "        (data['volume'].notna()) & \n",
    "        (data['amount'].notna()) & \n",
    "        (data['bid_price1'].notna()) & \n",
    "        (data['ask_price1'].notna())\n",
    "    ]\n",
    "\n",
    "    # Additional filter to ensure high-frequency (optional)\n",
    "    # Extract largest continuous segment of high-frequency trading data\n",
    "    high_freq_trading = extract_continuous_segment(high_freq_trading, volume_threshold, open_interest_threshold, time_interval)\n",
    "    high_freq_trading = high_freq_trading.set_index('datetime')\n",
    "    \n",
    "    # Ensure No NaN values remaining\n",
    "    assert(high_freq_trading.isna().sum().sum() == 0)\n",
    "    \n",
    "    # Return the filtered dataset\n",
    "    return high_freq_trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "23afb5de-029d-407c-a3da-6f8b75f4fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'INE.sc2010'\n",
    "data_preparation = DataPreparation(dataset_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d10c93-2fa4-41f2-9b68-24606ba802c8",
   "metadata": {},
   "source": [
    "## Model Definition: LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "273efa4b-f195-4cf0-8c8e-d9ce9b501300",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim):\n",
    "        super(CustomLSTMCell, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W_i = nn.Parameter(torch.randn(input_size, hidden_dim).float())\n",
    "        self.U_i = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_i = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "\n",
    "        self.W_f = nn.Parameter(torch.randn(input_size, hidden_dim).float())\n",
    "        self.U_f = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_f = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        self.W_c = nn.Parameter(torch.randn(input_size, hidden_dim).float())\n",
    "        self.U_c = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_c = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        self.W_o = nn.Parameter(torch.randn(input_size, hidden_dim).float())\n",
    "        self.U_o = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_o = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        # Layer normalization layers\n",
    "        self.ln_i = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_f = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_c = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_o = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'W_' in name or 'U_' in name:\n",
    "                nn.init.orthogonal_(param)  # Use orthogonal initialization\n",
    "            elif 'b_' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "            # Check for NaN values\n",
    "            assert(not torch.isnan(param).any())\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        # Compute gates\n",
    "        i_t = torch.sigmoid(self.ln_i(torch.matmul(x, self.W_i) + torch.matmul(h, self.U_i) + self.b_i))\n",
    "        f_t = torch.sigmoid(self.ln_f(torch.matmul(x, self.W_f) + torch.matmul(h, self.U_f) + self.b_f))\n",
    "        g_t = torch.tanh(self.ln_c(torch.matmul(x, self.W_c) + torch.matmul(h, self.U_c) + self.b_c))\n",
    "        o_t = torch.sigmoid(self.ln_o(torch.matmul(x, self.W_o) + torch.matmul(h, self.U_o) + self.b_o))\n",
    "\n",
    "        # Compute cell and hidden state\n",
    "        c_t = f_t * c + i_t * g_t\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        \n",
    "        assert(not torch.isnan(h_t).any())\n",
    "        assert(not torch.isnan(c_t).any())\n",
    "        \n",
    "        return h_t, c_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c23a8c4-e52f-4cd4-9d48-79a1d23b8de6",
   "metadata": {},
   "source": [
    "## Model Definition: GRU Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "ad7c0b79-da6e-4bbb-8c6c-e9066b42e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim):\n",
    "        super(CustomGRUCell, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W_z = nn.Parameter(torch.randn(input_size, hidden_dim).float())\n",
    "        self.U_z = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_z = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "\n",
    "        self.W_r = nn.Parameter(torch.randn(input_size, hidden_dim).float())\n",
    "        self.U_r = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_r = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        self.W_h = nn.Parameter(torch.randn(input_size, hidden_dim).float())\n",
    "        self.U_h = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_h = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        # Layer normalization layers\n",
    "        self.ln_z = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_r = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_h = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'W_' in name or 'U_' in name:\n",
    "                nn.init.xavier_uniform_(param)  # Use orthogonal initialization\n",
    "            elif 'b_' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "            # Check for NaN values\n",
    "            assert(not torch.isnan(param).any())\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        z_t = torch.sigmoid(self.ln_z(torch.mm(x, self.W_z) + torch.mm(h, self.U_z) + self.b_z))\n",
    "        r_t = torch.sigmoid(self.ln_r(torch.mm(x, self.W_r) + torch.mm(h, self.U_r) + self.b_r))\n",
    "        h_hat_t = torch.tanh(self.ln_h(torch.mm(x, self.W_h) + torch.mm(r_t * h, self.U_h) + self.b_h))\n",
    "\n",
    "        h_t = (1 - z_t) * h + z_t * h_hat_t\n",
    "        \n",
    "        assert(not torch.isnan(h_t).any())\n",
    "    \n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d56002-e909-4460-a087-92d9f4f32749",
   "metadata": {},
   "source": [
    "## Model Definition: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "ab261366-d0cd-4995-b9c1-6a51d242543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNNModel(nn.Module):\n",
    "    def __init__(self, input_size, num_features, hidden_dim, output_dim, num_layers, dropout_rate, dense_units, cell_type='lstm'):\n",
    "        super(CustomRNNModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.cell_type = cell_type\n",
    "        self.input_dim = input_size * num_features\n",
    "\n",
    "        if cell_type == 'lstm':\n",
    "            self.layers = nn.ModuleList([CustomLSTMCell(self.input_dim, hidden_dim)])\n",
    "            self.layers.extend([CustomLSTMCell(hidden_dim, hidden_dim) for _ in range(num_layers - 1)])\n",
    "        elif cell_type == 'gru':\n",
    "            self.layers = nn.ModuleList([CustomGRUCell(self.input_dim, hidden_dim)])\n",
    "            self.layers.extend([CustomGRUCell(hidden_dim, hidden_dim) for _ in range(num_layers - 1)])\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported cell type\")\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden_dim, dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_steps, input_size, num_features = x.size()\n",
    "        x = x.view(batch_size, num_steps, -1)  # Flatten the input dimensions to (batch_size, num_steps, input_size * num_features)\n",
    "\n",
    "        h_t = [torch.zeros(batch_size, self.hidden_dim, device=x.device) for _ in range(self.num_layers)]\n",
    "        if self.cell_type == 'lstm':\n",
    "            c_t = [torch.zeros(batch_size, self.hidden_dim, device=x.device) for _ in range(self.num_layers)]\n",
    "\n",
    "        for t in range(num_steps):\n",
    "            x_t = x[:, t, :]\n",
    "            for layer in range(self.num_layers):\n",
    "                if self.cell_type == 'lstm':\n",
    "                    h_t[layer], c_t[layer] = self.layers[layer](x_t, h_t[layer], c_t[layer])\n",
    "                else:\n",
    "                    h_t[layer] = self.layers[layer](x_t, h_t[layer])\n",
    "                x_t = h_t[layer]\n",
    "\n",
    "        x = self.dropout(x_t)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74bacfb-2f29-4dd9-98fb-378abee23041",
   "metadata": {},
   "source": [
    "## RNN Model Training + Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "03c7e1ac-08cd-44f9-9a27-6f7ba34f0236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss definitions\n",
    "def quantile_loss(outputs, targets, quantile=0.5):\n",
    "    errors = targets - outputs\n",
    "    loss = torch.max((quantile - 1) * errors, quantile * errors)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "\n",
    "class HingeLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HingeLoss, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        return torch.mean(torch.clamp(1 - targets * outputs, min=0))\n",
    "\n",
    "\n",
    "def directional_loss(outputs, targets):\n",
    "    return torch.mean(torch.abs(torch.sign(outputs) - torch.sign(targets)))\n",
    "\n",
    "\n",
    "def choose_loss_function(loss, quantile=None):\n",
    "    if loss == 'huber':\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "    elif loss == 'mse':\n",
    "        criterion = nn.MSELoss()\n",
    "    elif loss == 'quantile' and quantile is not None:\n",
    "        criterion = lambda outputs, targets: quantile_loss(outputs, targets, quantile=quantile)\n",
    "    elif loss == 'hinge':\n",
    "        criterion = HingeLoss()\n",
    "    elif loss == 'directional':\n",
    "        criterion = directional_loss\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported loss function\")\n",
    "\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "1ba6949c-3daf-49d5-a5f9-ab13c518cd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, data_preparation, cell_type):\n",
    "        self.data_preparation = data_preparation\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "    def extract_hyperparams(self):\n",
    "        dp = self.data_preparation\n",
    "        cell_type = self.cell_type\n",
    "        num_units = dp.hyperparams[cell_type][cell_type + '_units']\n",
    "        num_layers = dp.hyperparams[cell_type][cell_type + '_layers']\n",
    "        dropout_rate = dp.hyperparams[cell_type]['dropout_rate']\n",
    "        dense_units = dp.hyperparams[cell_type]['dense_units']\n",
    "        init_learning_rate = dp.hyperparams[cell_type]['init_learning_rate']\n",
    "        learning_rate_decay = dp.hyperparams[cell_type]['learning_rate_decay']\n",
    "        init_epochs = dp.hyperparams[cell_type]['init_epochs']\n",
    "        max_epochs = dp.hyperparams[cell_type]['max_epochs']\n",
    "        early_stop_patience = dp.hyperparams[cell_type].get('early_stop_patience', None)\n",
    "        train_needed = dp.hyperparams[cell_type]['pretrain']\n",
    "        \n",
    "        return num_units, num_layers, dropout_rate, dense_units, init_learning_rate, learning_rate_decay, init_epochs, max_epochs, early_stop_patience, train_needed\n",
    "\n",
    "    def train_model(self, model, train_loader, val_loader, criterion, init_epochs, num_epochs, init_learning_rate, learning_rate_decay, device, early_stop_patience=None):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        best_val_loss = float('inf')\n",
    "        early_stop_counter = 0\n",
    "    \n",
    "        optimizer = optim.Adam(model.parameters(), lr=init_learning_rate)\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0.0)\n",
    "    \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            current_lr = init_learning_rate * (learning_rate_decay ** max(float(epoch + 1 - init_epochs), 0.0))\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = current_lr\n",
    "                \n",
    "            train_loss = 0.0\n",
    "            for X_batch, y_batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False):\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                train_loss += loss.item()\n",
    "    \n",
    "            train_loss /= len(train_loader)\n",
    "            train_losses.append(train_loss)\n",
    "    \n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    outputs = model(X_batch)\n",
    "                    val_loss += criterion(outputs, y_batch).item()\n",
    "    \n",
    "            val_loss /= len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "    \n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "            scheduler.step()\n",
    "    \n",
    "            if early_stop_patience is not None:\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    early_stop_counter = 0\n",
    "                    torch.save(model.state_dict(), f'models/SP500_{self.cell_type}_model.pth')\n",
    "                else:\n",
    "                    early_stop_counter += 1\n",
    "                    if early_stop_counter >= early_stop_patience:\n",
    "                        print(\"Early stopping triggered\")\n",
    "                        break\n",
    "    \n",
    "        return train_losses, val_losses\n",
    "\n",
    "    def train(self):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        num_units, num_layers, dropout_rate, dense_units, init_learning_rate, learning_rate_decay, init_epochs, max_epochs, early_stop_patience, train_needed = self.extract_hyperparams()\n",
    "        num_features = self.data_preparation.num_features\n",
    "        input_size = self.data_preparation.input_size\n",
    "\n",
    "        model = CustomRNNModel(input_size=input_size, num_features=num_features, hidden_dim=num_units, output_dim=input_size, num_layers=num_layers, dropout_rate=dropout_rate, dense_units=dense_units, cell_type=self.cell_type)\n",
    "        model.to(device)\n",
    "\n",
    "        criterion = choose_loss_function('mse')\n",
    "\n",
    "        start_time = time.time()\n",
    "        train_losses, val_losses = self.train_model(model, self.data_preparation.train_loader, self.data_preparation.val_loader, criterion, init_epochs, max_epochs, init_learning_rate, learning_rate_decay, device, early_stop_patience)\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Loss Curves')\n",
    "        plt.show()\n",
    "\n",
    "        return model, duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "756b7f9f-26d6-4457-9dea-32489c801588",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPredictor:\n",
    "    def __init__(self, model, data_preparation, cell_type):\n",
    "        self.model = model\n",
    "        self.data_preparation = data_preparation\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "    def make_predictions(self, model, data_loader, device):\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        test_loss = 0\n",
    "        criterion = choose_loss_function('mse')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in data_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                predictions.append(outputs.cpu().numpy())\n",
    "                actuals.append(y_batch.cpu().numpy())\n",
    "                test_loss += criterion(outputs, y_batch).item()\n",
    "\n",
    "        test_loss /= len(data_loader)\n",
    "        predictions = np.concatenate(predictions)\n",
    "        actuals = np.concatenate(actuals)\n",
    "        return predictions, actuals, test_loss\n",
    "\n",
    "    def calculate_accuracy(self, predictions, actuals):\n",
    "        correct = np.sum(np.sign(predictions) == np.sign(actuals))\n",
    "        total = len(actuals)\n",
    "        accuracy = correct / total\n",
    "        return accuracy\n",
    "\n",
    "    def predict(self):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        \n",
    "        predictions, actuals, test_loss = self.make_predictions(self.model, self.data_preparation.test_loader, device)\n",
    "        classification_accuracy = self.calculate_accuracy(predictions, actuals)\n",
    "        \n",
    "        return test_loss, classification_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "e40914a2-017e-4747-a9b2-cc17aa976ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type = 'lstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383e9f15-3d16-4e5a-9114-367324b4c97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.5715, Val Loss: 1.0156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Train Loss: 0.5265, Val Loss: 0.9456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Train Loss: 0.4879, Val Loss: 0.8852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Train Loss: 0.4550, Val Loss: 0.8370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Train Loss: 0.4202, Val Loss: 0.7933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, Train Loss: 0.3886, Val Loss: 0.7501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, Train Loss: 0.3600, Val Loss: 0.7071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, Train Loss: 0.3324, Val Loss: 0.6641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Train Loss: 0.3057, Val Loss: 0.6208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Train Loss: 0.2774, Val Loss: 0.5773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100, Train Loss: 0.2510, Val Loss: 0.5347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100, Train Loss: 0.2214, Val Loss: 0.4917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Train Loss: 0.1926, Val Loss: 0.4391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100, Train Loss: 0.1637, Val Loss: 0.3807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100, Train Loss: 0.1320, Val Loss: 0.3147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100, Train Loss: 0.1027, Val Loss: 0.2569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100, Train Loss: 0.0834, Val Loss: 0.2116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100, Train Loss: 0.0727, Val Loss: 0.1762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100, Train Loss: 0.0602, Val Loss: 0.1491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Train Loss: 0.0531, Val Loss: 0.1285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100, Train Loss: 0.0438, Val Loss: 0.1116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100, Train Loss: 0.0404, Val Loss: 0.0981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100, Train Loss: 0.0378, Val Loss: 0.0877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100, Train Loss: 0.0313, Val Loss: 0.0790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100, Train Loss: 0.0302, Val Loss: 0.0713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100, Train Loss: 0.0291, Val Loss: 0.0649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100, Train Loss: 0.0245, Val Loss: 0.0591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100, Train Loss: 0.0260, Val Loss: 0.0538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100:   0%|                                       | 0/7 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "trainer = ModelTrainer(data_preparation, cell_type)\n",
    "trained_model, training_duration = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b11b1-9e06-44b8-ab0b-525c50ca406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "predictor = ModelPredictor(trained_model, data_preparation, cell_type)\n",
    "test_loss, classification_accuracy = predictor.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fc2119-fdc4-4826-be67-2d7eb6faf8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type = 'gru'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5fa42f-e2d0-4e2c-8d8e-a940282be4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "trainer = ModelTrainer(data_preparation, cell_type)\n",
    "trained_model, training_duration = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fb4bb2-fb44-4b53-8e6b-c79ca9cf3948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "predictor = ModelPredictor(trained_model, data_preparation, cell_type)\n",
    "test_loss, classification_accuracy = predictor.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b090a4e-768d-4f17-a712-e6777d506fe5",
   "metadata": {},
   "source": [
    "## Hyperparameter Search over `input_size` and `num_steps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e915769c-f17f-4104-8fc2-7c11e0617632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(cell_type, input_size_values, num_steps_values):\n",
    "    # Define dataframe\n",
    "    df = pd.DataFrame(columns=['Input Size', 'Number of Steps', 'Test Loss', 'Classification Accuracy', 'Duration'])\n",
    "    \n",
    "    # Define test losses & classification accuracies\n",
    "    input_size_combination_values = []\n",
    "    num_steps_combination_values = []\n",
    "\n",
    "    # Define hyperparam combinations\n",
    "    test_losses = []\n",
    "    classification_accuracies = []\n",
    "    durations = []\n",
    "    \n",
    "    # Loop over the hyperparam values\n",
    "    for input_size in input_size_values:\n",
    "        for num_steps in num_steps_values:\n",
    "            # Add to hyperparam combination\n",
    "            input_size_combination_values.append(input_size)\n",
    "            num_steps_combination_values.append(num_steps)\n",
    "            \n",
    "            # Modify the input_size and num_steps attributes\n",
    "            data_preparation = DataPreparation(dataset_name, num_steps=num_steps, input_size=input_size)\n",
    "            \n",
    "            # Training\n",
    "            trainer = ModelTrainer(data_preparation, cell_type)\n",
    "            trained_model, training_duration = trainer.train()\n",
    "\n",
    "            # Prediction\n",
    "            predictor = ModelPredictor(trained_model, data_preparation, cell_type)\n",
    "            test_loss, classification_accuracy = predictor.predict()\n",
    "            \n",
    "            test_losses.append(test_loss)\n",
    "            classification_accuracies.append(classification_accuracy)   \n",
    "            durations.append(training_duration)\n",
    "    \n",
    "    df['Input Size'] = input_size_combination_values\n",
    "    df['Number of Steps'] = num_steps_combination_values\n",
    "    df['Test Loss'] = test_losses\n",
    "    df['Classification Accuracy'] = classification_accuracies\n",
    "    df['Duration'] = durations\n",
    "    \n",
    "    df.to_csv(f'results/{cell_type}_evaluation_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf0929d-6959-47e3-aea3-ef307d20779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(cell_type, input_size_values, num_steps_values, SEARCH_NEEDED=False):\n",
    "    # Perform grid search if needed (setting SEARCH_NEEDED = TRUE will RUN LONG TIME!!!)\n",
    "    if SEARCH_NEEDED:\n",
    "        grid_search(cell_type, input_size_values, num_steps_values)\n",
    "    \n",
    "    # Get best model\n",
    "    df = pd.read_csv(f'results/{cell_type}_evaluation_results.csv')\n",
    "    idx = np.argmin(df['Test Loss'])\n",
    "    df_best_model = df.iloc[idx, :]\n",
    "    return df_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec64117-eef8-4ec1-93fa-08aab4760824",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size_values = [2, 3, 5, 10, 20]\n",
    "num_steps_values = [3, 10, 20, 30, 40, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a622b-c8f5-479c-b921-5692cb4682a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM (setting SEARCH_NEEDED = TRUE will RUN LONG TIME!!!)\n",
    "cell_type = 'lstm'\n",
    "SEARCH_NEEDED_LSTM = False\n",
    "df_best_model = get_best_model(cell_type, input_size_values, num_steps_values, SEARCH_NEEDED=SEARCH_NEEDED_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0518331-2739-4344-bf18-d35ee9eb49db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb045fd-b9b2-480d-8dc2-ab0109f024e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the input_size and num_steps attributes\n",
    "data_preparation = DataPreparation(dataset_name, num_steps=int(df_best_model['Number of Steps']), input_size=int(df_best_model['Input Size']))\n",
    "\n",
    "# Training\n",
    "trainer = ModelTrainer(data_preparation, cell_type)\n",
    "trained_model, training_duration = trainer.train()\n",
    "\n",
    "# Prediction\n",
    "predictor = ModelPredictor(trained_model, data_preparation, cell_type)\n",
    "test_loss, classification_accuracy = predictor.predict()\n",
    "\n",
    "print(\"Test loss of best model:\", test_loss)\n",
    "print(\"Classification accuracy of best model:\", classification_accuracy)\n",
    "print(\"Duration of best model:\", training_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29927b0-71bb-4c9d-acae-f7d06ccd2a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU (setting SEARCH_NEEDED = TRUE will RUN LONG TIME!!!)\n",
    "cell_type = 'gru'\n",
    "SEARCH_NEEDED_GRU = True\n",
    "df_best_model = get_best_model(cell_type, input_size_values, num_steps_values, SEARCH_NEEDED=SEARCH_NEEDED_GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80a94f7-7f3b-4881-9148-c6b4e8d410ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e47baf4-4392-4c4e-9ccf-83ab2ccaecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the input_size and num_steps attributes\n",
    "data_preparation = DataPreparation(dataset_name, num_steps=int(df_best_model['Number of Steps']), input_size=int(df_best_model['Input Size']))\n",
    "\n",
    "# Training\n",
    "trainer = ModelTrainer(data_preparation, cell_type)\n",
    "trained_model, training_duration = trainer.train()\n",
    "\n",
    "# Prediction\n",
    "predictor = ModelPredictor(trained_model, data_preparation, cell_type)\n",
    "test_loss, classification_accuracy = predictor.predict()\n",
    "\n",
    "print(\"Test loss of best model:\", test_loss)\n",
    "print(\"Classification accuracy of best model:\", classification_accuracy)\n",
    "print(\"Duration of best model:\", training_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b10df20-68dc-4170-9ba0-0ad0c4d3990c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05432447-658e-4b8d-9fa4-c0cc10612f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7254237d-ce87-489c-9d2a-3d4ef17e7d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4600a91-ae44-4e47-9b38-9439bb11702f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de33b4a4-9a04-4027-8815-4711ad8037d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232d58c2-0fc3-44b0-8311-56220c75d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.special import kl_div\n",
    "\n",
    "# # Sample data for demonstration purposes\n",
    "# # In practice, you would use actual activation data from the model\n",
    "# def get_fp32_activation_data():\n",
    "#     # Replace this with actual data collection\n",
    "#     return np.array([0.5, 1.0, 2.0, -1.0, -0.5, 3.0, -3.0, 0.1])\n",
    "\n",
    "# # **1. Calibration: Collect Data and Compute Scale Factor**\n",
    "\n",
    "# # Collect activation data (replace with actual FP32 data)\n",
    "# activation_data_fp32 = get_fp32_activation_data()\n",
    "\n",
    "# # Step 1: Determine the maximum absolute value\n",
    "# max_abs_value = np.max(np.abs(activation_data_fp32))\n",
    "\n",
    "# # Step 2: Compute the scale factor for symmetric quantization\n",
    "# def compute_scale(max_abs_value):\n",
    "#     return max_abs_value / 127  # 127 because 8-bit quantization uses values from -128 to 127\n",
    "\n",
    "# scale = compute_scale(max_abs_value)\n",
    "\n",
    "# # Step 3: Perform preliminary quantization of FP32 data\n",
    "# def preliminary_quantize(fp32_data, scale):\n",
    "#     return np.clip(np.round(fp32_data / scale), -128, 127).astype(np.int8)\n",
    "\n",
    "# activation_data_int8 = preliminary_quantize(activation_data_fp32, scale)\n",
    "\n",
    "# # Step 4: Compute histograms for FP32 and INT8\n",
    "# def compute_histograms(fp32_data, int8_data):\n",
    "#     hist_fp32, _ = np.histogram(fp32_data, bins=2048, range=(-128, 127), density=True)\n",
    "#     hist_int8, _ = np.histogram(int8_data, bins=2048, range=(-128, 127), density=True)\n",
    "#     return hist_fp32, hist_int8\n",
    "\n",
    "# hist_fp32, hist_int8 = compute_histograms(activation_data_fp32, activation_data_int8)\n",
    "\n",
    "# # Step 5: Compute KL divergence\n",
    "# def compute_kl_divergence(hist_fp32, hist_int8):\n",
    "#     # Adding a small constant to avoid log(0)\n",
    "#     return np.sum(kl_div(hist_fp32 + 1e-8, hist_int8 + 1e-8))\n",
    "\n",
    "# kl_divergence = compute_kl_divergence(hist_fp32, hist_int8)\n",
    "# print(f\"KL Divergence: {kl_divergence}\")\n",
    "\n",
    "# # **2. Quantization: Convert FP32 to INT8**\n",
    "\n",
    "# # Final quantization of FP32 data\n",
    "# def quantize_to_int8(fp32_data, scale):\n",
    "#     return np.clip(np.round(fp32_data / scale), -128, 127).astype(np.int8)\n",
    "\n",
    "# activation_data_int8_final = quantize_to_int8(activation_data_fp32, scale)\n",
    "\n",
    "# # **3. INT32 Computations: Perform Layer Operations**\n",
    "\n",
    "# # Example INT32 computation function\n",
    "# def int32_computations(weights, activations, bias):\n",
    "#     # Perform INT32 matrix multiplication and add bias\n",
    "#     int32_result = np.dot(weights, activations) + bias\n",
    "#     return int32_result\n",
    "\n",
    "# # Sample weights and bias for demonstration\n",
    "# weights = np.array([[1, -1], [2, 3]])\n",
    "# bias = np.array([1, -1])\n",
    "\n",
    "# # Perform INT32 computations\n",
    "# int32_result = int32_computations(weights, activation_data_int8_final, bias)\n",
    "# print(f\"INT32 Computation Result: {int32_result}\")\n",
    "\n",
    "# # **4. Re-Quantization: Convert INT32 to INT8**\n",
    "\n",
    "# # Re-quantization process\n",
    "# def requantize(int32_activations, scale, zero_point, bias):\n",
    "#     # Add bias and then requantize\n",
    "#     int32_activations_with_bias = int32_activations + bias\n",
    "#     return np.clip(np.round(int32_activations_with_bias * scale) + zero_point, -128, 127).astype(np.int8)\n",
    "\n",
    "# # Assuming zero_point = 0 for symmetric quantization\n",
    "# zero_point = 0\n",
    "\n",
    "# # Re-quantize INT32 results to INT8\n",
    "# activation_data_int8_requantized = requantize(int32_result, scale, zero_point, bias)\n",
    "# print(f\"Re-Quantized INT8 Data: {activation_data_int8_requantized}\")\n",
    "\n",
    "# # **5. De-Quantization: Convert INT8 Back to FP32**\n",
    "\n",
    "# # De-quantization process\n",
    "# def dequantize_to_fp32(int8_data, scale, zero_point):\n",
    "#     return (int8_data - zero_point) * scale\n",
    "\n",
    "# # Convert INT8 results back to FP32\n",
    "# fp32_reconstructed_data = dequantize_to_fp32(activation_data_int8_requantized, scale, zero_point)\n",
    "# print(f\"De-Quantized FP32 Data: {fp32_reconstructed_data}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
