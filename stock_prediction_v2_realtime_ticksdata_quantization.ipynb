{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a816d605-de9a-4ca5-90f5-1842d223e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import json\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a9a0bd-e4c8-4780-a42a-0bc04d864aae",
   "metadata": {},
   "source": [
    "## Data Preparation & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0515e674-b12e-4960-96bb-58feb7018379",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparation():\n",
    "    def __init__(self, dataset_filename, num_steps=None, input_size=None, configs_filename='configs', seed=42, period='1S'):\n",
    "        self.num_steps = num_steps\n",
    "        self.input_size = input_size\n",
    "        self.set_seed(seed)\n",
    "        self.load_configs(configs_filename)\n",
    "        self.load_data(dataset_filename, period)\n",
    "        \n",
    "    def set_seed(self, seed):\n",
    "        np.random.seed(seed)  # Set seed for numpy\n",
    "        random.seed(seed)  # Set seed for random\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.manual_seed(seed)  # Set seed for PyTorch CPU\n",
    "        \n",
    "        torch.cuda.manual_seed(seed)  # Set seed for PyTorch GPU\n",
    "        torch.cuda.manual_seed_all(seed)  # Set seed for all GPUs\n",
    "        torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior for CUDA\n",
    "        torch.backends.cudnn.benchmark = False  # Disable the auto-tuner for GPUs\n",
    "    \n",
    "    def load_configs(self, configs_filename):\n",
    "        # Load hyperparameters\n",
    "        with open(f'configs/{configs_filename}.json', 'r') as file:\n",
    "            self.hyperparams = json.load(file)\n",
    "            \n",
    "    def load_data(self, dataset_filename, period='1S'):\n",
    "        self.data = pd.read_csv('data/' + dataset_filename + '.csv')\n",
    "        self.data = extract_high_frequency_trading(self.data, period)\n",
    "        # self.data = self.data.resample(period).mean()  # Resample data based on the period\n",
    "        \n",
    "        # Extract attributes\n",
    "        if self.num_steps is None:\n",
    "            self.num_steps = self.hyperparams['num_steps']  # Extract number of steps\n",
    "        \n",
    "        if self.input_size is None:\n",
    "            self.input_size = self.hyperparams['input_size']  # Extract input size\n",
    "        \n",
    "        self.val_split = self.hyperparams['val_split']  # Extract validation split\n",
    "        self.test_split = self.hyperparams['test_split']  # Extract test split\n",
    "        self.batch_size = self.hyperparams['batch_size'] # Extract batch size\n",
    "        self.num_workers = self.hyperparams['num_workers'] # Extract number of workers (for GPU)\n",
    "        \n",
    "        # Normalize data\n",
    "        self.data = self.normalize_data(self.data)\n",
    "\n",
    "        # Assert no NaN values\n",
    "        assert(self.data.isna().sum().sum() == 0)\n",
    "\n",
    "        # Create sequences (with sliding windows)\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = self.create_sequences(self.data, self.input_size, self.num_steps, self.val_split, self.test_split)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "        \n",
    "        assert(not torch.isnan(X_train_tensor).any())\n",
    "        assert(not torch.isnan(y_train_tensor).any())\n",
    "        assert(not torch.isnan(X_val_tensor).any())\n",
    "        assert(not torch.isnan(y_val_tensor).any())\n",
    "        assert(not torch.isnan(X_test_tensor).any())\n",
    "        assert(not torch.isnan(y_test_tensor).any())\n",
    "        \n",
    "        # Create DataLoader instances\n",
    "        self.train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, pin_memory=True)\n",
    "        self.val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, pin_memory=True)\n",
    "        self.test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, pin_memory=True)\n",
    "\n",
    "    def normalize_data(self, data):\n",
    "        # Apply MinMaxScaler to normalize data\n",
    "        scaler = MinMaxScaler()\n",
    "        normalized_data = scaler.fit_transform(data)\n",
    "        return pd.DataFrame(normalized_data, index=data.index, columns=data.columns)\n",
    "    \n",
    "    def create_sequences(self, data, input_size, num_steps, val_split=0.1, test_split=0.1):\n",
    "        # Convert data to numpy array\n",
    "        data = np.array(data)\n",
    "    \n",
    "        # Check if data has any NaNs\n",
    "        if np.isnan(data).any():\n",
    "            raise ValueError(\"Data contains NaN values. Please clean the data before proceeding.\")\n",
    "    \n",
    "        # Normalize data\n",
    "        # Replace zeros in the last price with a small value to avoid division by zero\n",
    "        data = np.where(data[:, -1] == 0, 1e-6, data[:, -1])  # Small constant to avoid division by zero\n",
    "        data = [data[i * input_size: (i + 1) * input_size] \n",
    "               for i in range(len(data) // input_size)]\n",
    "        \n",
    "        # Normalize the data\n",
    "        data = [data[0] / data[0][0] - 1.0] + [\n",
    "                    curr / data[i][-1] - 1.0 for i, curr in enumerate(data[1:])]\n",
    "        \n",
    "        # Check for remaining NaNs after normalization\n",
    "        if np.isnan(data).any():\n",
    "            raise ValueError(\"Data contains NaN values after normalization. Please check the normalization process.\")\n",
    "        \n",
    "        # Split into groups of `num_steps`\n",
    "        X = np.array([data[i: i + num_steps] for i in range(len(data) - num_steps)])\n",
    "        y = np.array([data[i + num_steps] for i in range(len(data) - num_steps)])\n",
    "    \n",
    "        # Drop all rows with NaN values\n",
    "        mask = ~np.isnan(X).any(axis=(1, 2)) & ~np.isnan(y).any(axis=1)\n",
    "        X = X[mask]\n",
    "        y = y[mask]\n",
    "        \n",
    "        # Reshape X to have shape (N, num_steps, input_size)\n",
    "        X = X.reshape(-1, num_steps, input_size)\n",
    "        y = y.reshape(-1, input_size)  # Reshape y to match the output shape\n",
    "        \n",
    "        # Split into train, validation, and test sets\n",
    "        total_len = len(X)\n",
    "        test_start = int(total_len * (1 - test_split))\n",
    "        val_start = int(total_len * (1 - test_split - val_split))\n",
    "        \n",
    "        X_train, X_val, X_test = X[:val_start], X[val_start:test_start], X[test_start:]\n",
    "        y_train, y_val, y_test = y[:val_start], y[val_start:test_start], y[test_start:]\n",
    "        \n",
    "        return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "def extract_high_frequency_trading(data, period='1S'):\n",
    "    # Convert datetime column to pandas datetime type\n",
    "    data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "    \n",
    "    # # Add additional diffs info\n",
    "    # data_pre = data.copy().shift(1)\n",
    "    # data_pre['price_diff'] = data['last_price'] - data_pre['last_price']\n",
    "    # data_pre['oi_diff'] = data['open_interest'] - data_pre['open_interest']\n",
    "    # data_pre['vol_diff'] = data['volume'] - data_pre['volume']\n",
    "    \n",
    "    # data_pre[\"pc\"] = np.where(\n",
    "    #     data[\"last_price\"] <= data_pre[\"bid_price1\"], -1,\n",
    "    #     np.where(data[\"last_price\"] >= data_pre[\"ask_price1\"], 1,\n",
    "    #              np.sign(data_pre[\"price_diff\"])))\n",
    "    # pc_g = data_pre[\"pc\"] > 0\n",
    "    \n",
    "    \n",
    "    # condition_buy = (data[\"last_price\"] >= data_pre[\"ask_price1\"]) | (data[\"last_price\"] >= data[\"ask_price1\"])\n",
    "    # condition_sell = (data['last_price'] <= data_pre['bid_price1']) | (data['last_price'] <= data['bid_price1'])\n",
    "    # direction values = np.where(condition_buy,np.where(condition_sell,\"主动卖！\", \" 其他，\"))data_pre['direction'] = pd.Series (direction _values, index=data.index)\n",
    "    \n",
    "    # Identify high-frequency trading periods\n",
    "    # Assuming a significant increase in volume and consistent bid/ask prices indicates high-frequency trading\n",
    "    # Adjust the thresholds as necessary for your dataset\n",
    "    high_freq_trading = data[\n",
    "        (data['volume'] > 0) & \n",
    "        (data['open_interest'] > 0) & \n",
    "        (data['last_price'].notna()) & \n",
    "        (data['highest'].notna()) & \n",
    "        (data['volume'].notna()) & \n",
    "        (data['amount'].notna()) & \n",
    "        (data['bid_price1'].notna()) & \n",
    "        (data['ask_price1'].notna())\n",
    "    ]\n",
    "    \n",
    "    # # Calculate the difference between consecutive rows for open_interest and volume\n",
    "    # high_freq_trading['open_interest_diff'] = high_freq_trading['open_interest'].diff()\n",
    "    # high_freq_trading['volume_diff'] = high_freq_trading['volume'].diff()\n",
    "    \n",
    "    # # Find the row with the maximum increase in open_interest\n",
    "    # max_open_interest_idx = high_freq_trading['open_interest_diff'].idxmax()\n",
    "    # max_open_interest_segment = high_freq_trading.loc[max_open_interest_idx - 1 : max_open_interest_idx + 1]\n",
    "    \n",
    "    # # Find the row with the maximum increase in volume\n",
    "    # max_volume_idx = high_freq_trading['volume_diff'].idxmax()\n",
    "    # max_volume_segment = high_freq_trading.loc[max_volume_idx - 1 : max_volume_idx + 1]\n",
    "    \n",
    "    # Additional filter to ensure high-frequency (optional)\n",
    "    # You can adjust the frequency threshold as needed\n",
    "    high_freq_trading = high_freq_trading.set_index('datetime')\n",
    "    high_freq_trading = high_freq_trading.resample(period).ffill().dropna()  # Resample to `period` intervals and forward fill\n",
    "\n",
    "    # Ensure No NaN values remaining\n",
    "    assert(high_freq_trading.isna().sum().sum() == 0)\n",
    "    \n",
    "    # Return the filtered dataset\n",
    "    return high_freq_trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23afb5de-029d-407c-a3da-6f8b75f4fc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last_price</th>\n",
       "      <th>highest</th>\n",
       "      <th>lowest</th>\n",
       "      <th>bid_price1</th>\n",
       "      <th>bid_volume1</th>\n",
       "      <th>bid_price2</th>\n",
       "      <th>bid_volume2</th>\n",
       "      <th>bid_price3</th>\n",
       "      <th>bid_volume3</th>\n",
       "      <th>bid_price4</th>\n",
       "      <th>...</th>\n",
       "      <th>ask_volume2</th>\n",
       "      <th>ask_price3</th>\n",
       "      <th>ask_volume3</th>\n",
       "      <th>ask_price4</th>\n",
       "      <th>ask_volume4</th>\n",
       "      <th>ask_price5</th>\n",
       "      <th>ask_volume5</th>\n",
       "      <th>volume</th>\n",
       "      <th>amount</th>\n",
       "      <th>open_interest</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-10-30 09:10:46</th>\n",
       "      <td>447.2</td>\n",
       "      <td>447.2</td>\n",
       "      <td>447.2</td>\n",
       "      <td>430.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>424.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>422.4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>447.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>464.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>503.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>447200.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-30 09:10:47</th>\n",
       "      <td>447.2</td>\n",
       "      <td>447.2</td>\n",
       "      <td>447.2</td>\n",
       "      <td>430.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>424.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>422.4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>447.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>464.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>503.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>447200.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-30 09:10:48</th>\n",
       "      <td>447.2</td>\n",
       "      <td>447.2</td>\n",
       "      <td>447.2</td>\n",
       "      <td>430.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>424.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>422.4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>447.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>464.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>503.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>447200.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-30 09:10:49</th>\n",
       "      <td>447.2</td>\n",
       "      <td>447.2</td>\n",
       "      <td>447.2</td>\n",
       "      <td>430.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>424.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>422.4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>447.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>464.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>503.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>447200.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-30 09:10:51</th>\n",
       "      <td>447.2</td>\n",
       "      <td>447.2</td>\n",
       "      <td>447.2</td>\n",
       "      <td>431.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>431.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>424.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>422.4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>447.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>464.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>503.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>447200.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-23 14:22:28</th>\n",
       "      <td>250.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>234.5</td>\n",
       "      <td>248.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>242.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>231.1</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>279.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>735.0</td>\n",
       "      <td>183112800.0</td>\n",
       "      <td>7305.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-23 14:22:29</th>\n",
       "      <td>250.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>234.5</td>\n",
       "      <td>248.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>242.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>231.1</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>279.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>735.0</td>\n",
       "      <td>183112800.0</td>\n",
       "      <td>7305.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-23 14:22:30</th>\n",
       "      <td>250.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>234.5</td>\n",
       "      <td>248.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>242.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>231.1</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>279.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>735.0</td>\n",
       "      <td>183112800.0</td>\n",
       "      <td>7305.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-23 14:22:31</th>\n",
       "      <td>250.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>234.5</td>\n",
       "      <td>248.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>242.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>231.1</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>279.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>735.0</td>\n",
       "      <td>183112800.0</td>\n",
       "      <td>7305.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-23 14:22:32</th>\n",
       "      <td>250.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>234.5</td>\n",
       "      <td>248.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>242.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>231.1</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>279.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>735.0</td>\n",
       "      <td>183112800.0</td>\n",
       "      <td>7305.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15255630 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     last_price  highest  lowest  bid_price1  bid_volume1  \\\n",
       "datetime                                                                    \n",
       "2019-10-30 09:10:46       447.2    447.2   447.2       430.1          1.0   \n",
       "2019-10-30 09:10:47       447.2    447.2   447.2       430.1          1.0   \n",
       "2019-10-30 09:10:48       447.2    447.2   447.2       430.1          1.0   \n",
       "2019-10-30 09:10:49       447.2    447.2   447.2       430.1          1.0   \n",
       "2019-10-30 09:10:51       447.2    447.2   447.2       431.1          1.0   \n",
       "...                         ...      ...     ...         ...          ...   \n",
       "2020-09-23 14:22:28       250.0    260.0   234.5       248.2          5.0   \n",
       "2020-09-23 14:22:29       250.0    260.0   234.5       248.2          5.0   \n",
       "2020-09-23 14:22:30       250.0    260.0   234.5       248.2          5.0   \n",
       "2020-09-23 14:22:31       250.0    260.0   234.5       248.2          5.0   \n",
       "2020-09-23 14:22:32       250.0    260.0   234.5       248.2          5.0   \n",
       "\n",
       "                     bid_price2  bid_volume2  bid_price3  bid_volume3  \\\n",
       "datetime                                                                \n",
       "2019-10-30 09:10:46       430.0          1.0       424.4          1.0   \n",
       "2019-10-30 09:10:47       430.0          1.0       424.4          1.0   \n",
       "2019-10-30 09:10:48       430.0          1.0       424.4          1.0   \n",
       "2019-10-30 09:10:49       430.0          1.0       424.4          1.0   \n",
       "2019-10-30 09:10:51       431.0          1.0       424.4          1.0   \n",
       "...                         ...          ...         ...          ...   \n",
       "2020-09-23 14:22:28       242.2          6.0       242.0         10.0   \n",
       "2020-09-23 14:22:29       242.2          6.0       242.0         10.0   \n",
       "2020-09-23 14:22:30       242.2          6.0       242.0         10.0   \n",
       "2020-09-23 14:22:31       242.2          6.0       242.0         10.0   \n",
       "2020-09-23 14:22:32       242.2          6.0       242.0         10.0   \n",
       "\n",
       "                     bid_price4  ...  ask_volume2  ask_price3  ask_volume3  \\\n",
       "datetime                         ...                                         \n",
       "2019-10-30 09:10:46       422.4  ...          1.0       447.3          1.0   \n",
       "2019-10-30 09:10:47       422.4  ...          1.0       447.3          1.0   \n",
       "2019-10-30 09:10:48       422.4  ...          1.0       447.3          1.0   \n",
       "2019-10-30 09:10:49       422.4  ...          1.0       447.3          1.0   \n",
       "2019-10-30 09:10:51       422.4  ...          1.0       447.3          1.0   \n",
       "...                         ...  ...          ...         ...          ...   \n",
       "2020-09-23 14:22:28       231.1  ...          5.0       278.0          8.0   \n",
       "2020-09-23 14:22:29       231.1  ...          5.0       278.0          8.0   \n",
       "2020-09-23 14:22:30       231.1  ...          5.0       278.0          8.0   \n",
       "2020-09-23 14:22:31       231.1  ...          5.0       278.0          8.0   \n",
       "2020-09-23 14:22:32       231.1  ...          5.0       278.0          8.0   \n",
       "\n",
       "                     ask_price4  ask_volume4  ask_price5  ask_volume5  volume  \\\n",
       "datetime                                                                        \n",
       "2019-10-30 09:10:46       464.6          1.0       503.9          1.0     1.0   \n",
       "2019-10-30 09:10:47       464.6          1.0       503.9          1.0     1.0   \n",
       "2019-10-30 09:10:48       464.6          1.0       503.9          1.0     1.0   \n",
       "2019-10-30 09:10:49       464.6          1.0       503.9          1.0     1.0   \n",
       "2019-10-30 09:10:51       464.6          1.0       503.9          1.0     1.0   \n",
       "...                         ...          ...         ...          ...     ...   \n",
       "2020-09-23 14:22:28       279.0          2.0       279.1         15.0   735.0   \n",
       "2020-09-23 14:22:29       279.0          2.0       279.1         15.0   735.0   \n",
       "2020-09-23 14:22:30       279.0          2.0       279.1         15.0   735.0   \n",
       "2020-09-23 14:22:31       279.0          2.0       279.1         15.0   735.0   \n",
       "2020-09-23 14:22:32       279.0          2.0       279.1         15.0   735.0   \n",
       "\n",
       "                          amount  open_interest  \n",
       "datetime                                         \n",
       "2019-10-30 09:10:46     447200.0            1.0  \n",
       "2019-10-30 09:10:47     447200.0            1.0  \n",
       "2019-10-30 09:10:48     447200.0            1.0  \n",
       "2019-10-30 09:10:49     447200.0            1.0  \n",
       "2019-10-30 09:10:51     447200.0            1.0  \n",
       "...                          ...            ...  \n",
       "2020-09-23 14:22:28  183112800.0         7305.0  \n",
       "2020-09-23 14:22:29  183112800.0         7305.0  \n",
       "2020-09-23 14:22:30  183112800.0         7305.0  \n",
       "2020-09-23 14:22:31  183112800.0         7305.0  \n",
       "2020-09-23 14:22:32  183112800.0         7305.0  \n",
       "\n",
       "[15255630 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/INE.sc2010.csv')\n",
    "data = extract_high_frequency_trading(data, period='1S')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d10c93-2fa4-41f2-9b68-24606ba802c8",
   "metadata": {},
   "source": [
    "## Model Definition: LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "273efa4b-f195-4cf0-8c8e-d9ce9b501300",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CustomLSTMCell, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W_i = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_i = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_i = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "\n",
    "        self.W_f = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_f = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_f = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        self.W_c = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_c = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_c = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        self.W_o = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_o = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_o = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        # Layer normalization layers\n",
    "        self.ln_i = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_f = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_c = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_o = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'W_' in name or 'U_' in name:\n",
    "                nn.init.orthogonal_(param)  # Use orthogonal initialization\n",
    "            elif 'b_' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "            # Check for NaN values\n",
    "            assert(not torch.isnan(param).any())\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        i_t = torch.sigmoid(self.ln_i(torch.mm(x, self.W_i) + torch.mm(h, self.U_i) + self.b_i))\n",
    "        f_t = torch.sigmoid(self.ln_f(torch.mm(x, self.W_f) + torch.mm(h, self.U_f) + self.b_f))\n",
    "        g_t = torch.tanh(self.ln_c(torch.mm(x, self.W_c) + torch.mm(h, self.U_c) + self.b_c))\n",
    "        o_t = torch.sigmoid(self.ln_o(torch.mm(x, self.W_o) + torch.mm(h, self.U_o) + self.b_o))\n",
    "\n",
    "        c_t = f_t * c + i_t * g_t\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        \n",
    "        assert(not torch.isnan(h_t).any())\n",
    "        assert(not torch.isnan(c_t).any())\n",
    "    \n",
    "        return h_t, c_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c23a8c4-e52f-4cd4-9d48-79a1d23b8de6",
   "metadata": {},
   "source": [
    "## Model Definition: GRU Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad7c0b79-da6e-4bbb-8c6c-e9066b42e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGRUCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CustomGRUCell, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W_z = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_z = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_z = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "\n",
    "        self.W_r = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_r = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_r = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        self.W_h = nn.Parameter(torch.randn(input_dim, hidden_dim).float())\n",
    "        self.U_h = nn.Parameter(torch.randn(hidden_dim, hidden_dim).float())\n",
    "        self.b_h = nn.Parameter(torch.zeros(hidden_dim).float())\n",
    "        \n",
    "        # Layer normalization layers\n",
    "        self.ln_z = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_r = nn.LayerNorm(hidden_dim)\n",
    "        self.ln_h = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'W_' in name or 'U_' in name:\n",
    "                nn.init.xavier_uniform_(param)  # Use orthogonal initialization\n",
    "            elif 'b_' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "            # Check for NaN values\n",
    "            assert(not torch.isnan(param).any())\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        z_t = torch.sigmoid(self.ln_z(torch.mm(x, self.W_z) + torch.mm(h, self.U_z) + self.b_z))\n",
    "        r_t = torch.sigmoid(self.ln_r(torch.mm(x, self.W_r) + torch.mm(h, self.U_r) + self.b_r))\n",
    "        h_hat_t = torch.tanh(self.ln_h(torch.mm(x, self.W_h) + torch.mm(r_t * h, self.U_h) + self.b_h))\n",
    "\n",
    "        h_t = (1 - z_t) * h + z_t * h_hat_t\n",
    "        \n",
    "        assert(not torch.isnan(h_t).any())\n",
    "    \n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d56002-e909-4460-a087-92d9f4f32749",
   "metadata": {},
   "source": [
    "## Model Definition: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab261366-d0cd-4995-b9c1-6a51d242543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout_rate, dense_units, cell_type='lstm'):\n",
    "        super(CustomRNNModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "        if cell_type == 'lstm':\n",
    "            self.layers = nn.ModuleList([CustomLSTMCell(input_dim, hidden_dim)])\n",
    "            self.layers.extend([CustomLSTMCell(hidden_dim, hidden_dim) for _ in range(num_layers - 1)])\n",
    "        elif cell_type == 'gru':\n",
    "            self.layers = nn.ModuleList([CustomGRUCell(input_dim, hidden_dim)])\n",
    "            self.layers.extend([CustomGRUCell(hidden_dim, hidden_dim) for _ in range(num_layers - 1)])\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported cell type\")\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden_dim, dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        h_t = [torch.zeros(batch_size, self.hidden_dim, device=x.device) for _ in range(self.num_layers)]\n",
    "        c_t = [torch.zeros(batch_size, self.hidden_dim, device=x.device) for _ in range(self.num_layers)]\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]\n",
    "            for layer in range(self.num_layers):\n",
    "                if self.cell_type == 'lstm':\n",
    "                    h_t[layer], c_t[layer] = self.layers[layer](x_t, h_t[layer], c_t[layer])\n",
    "                else:\n",
    "                    h_t[layer] = self.layers[layer](x_t, h_t[layer])\n",
    "                x_t = h_t[layer]\n",
    "\n",
    "        x = self.dropout(x_t)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74bacfb-2f29-4dd9-98fb-378abee23041",
   "metadata": {},
   "source": [
    "## RNN Model Training + Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ba6949c-3daf-49d5-a5f9-ab13c518cd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss definitions \n",
    "def quantile_loss(outputs, targets, quantile=0.5):\n",
    "    errors = targets - outputs\n",
    "    loss = torch.max((quantile - 1) * errors, quantile * errors)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "class HingeLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HingeLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        return torch.mean(torch.clamp(1 - targets * outputs, min=0))\n",
    "\n",
    "def directional_loss(outputs, targets):\n",
    "    return torch.mean(torch.abs(torch.sign(outputs) - torch.sign(targets)))\n",
    "\n",
    "def choose_loss_function(loss):\n",
    "    if loss == 'huber':  \n",
    "        criterion = nn.SmoothL1Loss() \n",
    "    elif loss == 'mse': \n",
    "        criterion = nn.MSELoss()\n",
    "    elif loss == 'quantile':\n",
    "        criterion = quantile_loss(quantile=quantile)\n",
    "    elif loss == 'hinge':\n",
    "        criterion = HingeLoss()\n",
    "    elif loss == 'directional':\n",
    "        criterion = directional_loss()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported loss function\")\n",
    "\n",
    "    return criterion\n",
    "\n",
    "class TrainAndPredict():\n",
    "    def __init__(self, data_preparation, cell_type, train_needed=None):\n",
    "        self.train_needed = train_needed\n",
    "        self.test_loss, self.classification_accuracy, self.duration = self.train_and_predict(data_preparation, cell_type)\n",
    "\n",
    "    # Extract hyperparameters\n",
    "    def extract_hyperparams(self, dp, cell_type):\n",
    "        num_units = dp.hyperparams[cell_type][cell_type + '_units']\n",
    "        num_layers = dp.hyperparams[cell_type][cell_type + '_layers']\n",
    "        dropout_rate = dp.hyperparams[cell_type]['dropout_rate']\n",
    "        dense_units = dp.hyperparams[cell_type]['dense_units']\n",
    "        init_learning_rate = dp.hyperparams[cell_type]['init_learning_rate']\n",
    "        learning_rate_decay = dp.hyperparams[cell_type]['learning_rate_decay']\n",
    "        init_epochs = dp.hyperparams[cell_type]['init_epochs']\n",
    "        max_epochs = dp.hyperparams[cell_type]['max_epochs']\n",
    "        early_stop_patience = dp.hyperparams[cell_type].get('early_stop_patience', None)\n",
    "        \n",
    "        if self.train_needed is None:\n",
    "            self.train_needed = dp.hyperparams[cell_type]['pretrain'] # Whether to train the model\n",
    "        \n",
    "        return num_units, num_layers, dropout_rate, dense_units, init_learning_rate, learning_rate_decay, init_epochs, max_epochs, early_stop_patience, self.train_needed  \n",
    "\n",
    "\n",
    "    def train_model(self, model, train_loader, val_loader, criterion, init_epochs, num_epochs, init_learning_rate, learning_rate_decay, device, early_stop_patience=None, cell_type='lstm'):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        best_val_loss = float('inf')\n",
    "        early_stop_counter = 0\n",
    "    \n",
    "        # Initialize optimizer and scheduler\n",
    "        optimizer = optim.Adam(model.parameters(), lr=init_learning_rate)\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0.0)\n",
    "    \n",
    "        # Initialize gradient scaler for mixed precision training\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            current_lr = init_learning_rate * (learning_rate_decay ** max(float(epoch + 1 - init_epochs), 0.0))\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = current_lr\n",
    "    \n",
    "            train_loss = 0.0\n",
    "            with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], profile_memory=True) as prof:\n",
    "                for X_batch, y_batch in tqdm(train_loader):\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = model(X_batch)\n",
    "                        loss = criterion(outputs, y_batch)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "                    train_loss += loss.item()\n",
    "    \n",
    "            train_loss /= len(train_loader)\n",
    "            train_losses.append(train_loss)\n",
    "    \n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    outputs = model(X_batch)\n",
    "                    loss = criterion(outputs, y_batch)\n",
    "                    val_loss += loss.item()\n",
    "    \n",
    "            val_loss /= len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "    \n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "            # Update the learning rate\n",
    "            scheduler.step()\n",
    "    \n",
    "            # Print profiling data\n",
    "            print(prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=10))\n",
    "    \n",
    "            # Early stopping\n",
    "            if early_stop_patience is not None:\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    early_stop_counter = 0\n",
    "                    # Save best model\n",
    "                    torch.save(model.state_dict(), f'models/SP500_{cell_type}_model.pth')\n",
    "                else:\n",
    "                    early_stop_counter += 1\n",
    "                    if early_stop_counter >= early_stop_patience:\n",
    "                        print(\"Early stopping triggered\")\n",
    "                        break\n",
    "    \n",
    "        return train_losses, val_losses\n",
    "\n",
    "    \n",
    "        \n",
    "    # Training loop\n",
    "    def train_model(self, model, train_loader, val_loader, criterion, init_epochs, num_epochs, init_learning_rate, learning_rate_decay, device, early_stop_patience=None, cell_type='lstm'):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        best_val_loss = float('inf')\n",
    "        early_stop_counter = 0\n",
    "    \n",
    "        learning_rates_to_use = [\n",
    "        init_learning_rate * (\n",
    "            learning_rate_decay ** max(float(i + 1 - init_epochs), 0.0)\n",
    "        ) for i in range(num_epochs)]\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            current_lr = learning_rates_to_use[epoch]\n",
    "            optimizer = optim.Adam(model.parameters(), lr=current_lr)\n",
    "            train_loss = 0.0\n",
    "            for X_batch, y_batch in tqdm(train_loader):\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "    \n",
    "            train_loss /= len(train_loader)\n",
    "            train_losses.append(train_loss)\n",
    "    \n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    outputs = model(X_batch)\n",
    "                    loss = criterion(outputs, y_batch)\n",
    "                    val_loss += loss.item()\n",
    "    \n",
    "            val_loss /= len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "    \n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "            # Early stopping\n",
    "            if early_stop_patience is not None:\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    early_stop_counter = 0\n",
    "                    # Save best model\n",
    "                    torch.save(model.state_dict(), f'models/SP500_{cell_type}_model.pth')\n",
    "                else:\n",
    "                    early_stop_counter += 1\n",
    "                    if early_stop_counter >= early_stop_patience:\n",
    "                        print(\"Early stopping triggered\")\n",
    "                        break\n",
    "        \n",
    "        return train_losses, val_losses\n",
    "    \n",
    "    # Function to load the model\n",
    "    def load_model(self, model, cell_type, device):\n",
    "        model_path = f'models/SP500_{cell_type}_model.pth'\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.to(device)\n",
    "        return model\n",
    "\n",
    "    # Function to make predictions\n",
    "    def make_predictions(self, model, data_loader, device):\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in data_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                predictions.extend(outputs.cpu().numpy())\n",
    "                actuals.extend(y_batch.cpu().numpy())\n",
    "                criterion = choose_loss_function('huber')\n",
    "                test_loss += criterion(outputs, y_batch)\n",
    "        return np.array(predictions).flatten(), np.array(actuals).flatten(), test_loss.item()\n",
    "    \n",
    "    # Plot the predictions against the actual values\n",
    "    def plot_results(self, data_preparation, actuals, predictions):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        x = np.arange(len(actuals))\n",
    "        plt.bar(x - 0.2, actuals, label='Actual Prices')\n",
    "        plt.bar(x + 0.2, predictions, label='Predicted Prices')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Normalized Price')\n",
    "        plt.ylim(-0.1, 0.1)\n",
    "        plt.title(f'Predicted vs Actual Prices for input_size={data_preparation.input_size}, num_steps={data_preparation.num_steps}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    # Check for overfitting/underfitting\n",
    "    def plot_losses(self, train_losses, val_losses):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def train_and_predict(self, data_preparation, cell_type):\n",
    "        # Initialize model\n",
    "        num_units, num_layers, dropout_rate, dense_units, init_learning_rate, learning_rate_decay, init_epochs, max_epochs, early_stop_patience, train_needed = self.extract_hyperparams(data_preparation, cell_type)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = CustomRNNModel(input_dim=data_preparation.input_size, hidden_dim=num_units, output_dim=data_preparation.input_size, num_layers=num_layers, dropout_rate=dropout_rate, dense_units=dense_units, cell_type=cell_type).to(device)\n",
    "        \n",
    "        # Train or load the model\n",
    "        if train_needed:\n",
    "            # Train the model\n",
    "            criterion = choose_loss_function('huber')\n",
    "            t1 = time.time()\n",
    "            train_losses, val_losses = self.train_model(model, data_preparation.train_loader, data_preparation.val_loader, criterion, init_epochs, max_epochs, init_learning_rate, learning_rate_decay, device, early_stop_patience, cell_type=cell_type)\n",
    "            duration = time.time() - t1\n",
    "            \n",
    "            torch.save(model.state_dict(), f'models/SP500_{cell_type}_model.pth')\n",
    "        else:\n",
    "            # Load the model\n",
    "            model = self.load_model(model, cell_type, device)\n",
    "            print(\"Model loaded successfully\")\n",
    "        \n",
    "        # Get a model summary\n",
    "        print(model)\n",
    "        \n",
    "        # Get predictions and actual values\n",
    "        predictions, actuals, test_loss = self.make_predictions(model, data_preparation.test_loader, device)\n",
    "        \n",
    "        # Plot the predictions against the actual values\n",
    "        self.plot_results(data_preparation, actuals, predictions)\n",
    "        \n",
    "        # Check for overfitting/underfitting\n",
    "        self.plot_losses(train_losses, val_losses)\n",
    "        \n",
    "        # Classification rate\n",
    "        classification_accuracy = np.mean((predictions > 0) == (actuals > 0))\n",
    "    \n",
    "        return test_loss, classification_accuracy, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f1746fb-4044-4a74-a7be-cc9e1d7d1970",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'INE.sc2010'\n",
    "period = '1S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40914a2-017e-4747-a9b2-cc17aa976ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|███████████████████████████████▍     | 81031/95348 [16:42<02:46, 86.05it/s]"
     ]
    }
   ],
   "source": [
    "data_preparation = DataPreparation(dataset_name, period=period)  # Adjust num_steps and input_size as needed\n",
    "cell_type = 'lstm'\n",
    "evaluation_results = TrainAndPredict(data_preparation, cell_type, train_needed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5fa42f-e2d0-4e2c-8d8e-a940282be4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preparation = DataPreparation(dataset_name, period=period)\n",
    "cell_type = 'gru'\n",
    "evaluation_results = TrainAndPredict(data_preparation, cell_type, train_needed=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b090a4e-768d-4f17-a712-e6777d506fe5",
   "metadata": {},
   "source": [
    "## Hyperparameter Search over `input_size` and `num_steps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e915769c-f17f-4104-8fc2-7c11e0617632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(cell_type, input_size_values, num_steps_values):\n",
    "    # Define dataframe\n",
    "    df = pd.DataFrame(columns=['Input Size', 'Number of Steps', 'Test Loss', 'Classification Accuracy', 'Duration'])\n",
    "    \n",
    "    # Define test losses & classification accuracies\n",
    "    input_size_combination_values = []\n",
    "    num_steps_combination_values = []\n",
    "\n",
    "    # Define hyperparam combinations\n",
    "    test_losses = []\n",
    "    classification_accuracies = []\n",
    "    durations = []\n",
    "    \n",
    "    # Loop over the hyperparam values\n",
    "    for input_size in input_size_values:\n",
    "        for num_steps in num_steps_values:\n",
    "            # Add to hyperparam combination\n",
    "            input_size_combination_values.append(input_size)\n",
    "            num_steps_combination_values.append(num_steps)\n",
    "            \n",
    "            # Modify the input_size and num_steps attributes\n",
    "            data_preparation = DataPreparation(dataset_name, num_steps=num_steps, input_size=input_size)\n",
    "            \n",
    "            # Train & Evaluate model\n",
    "            evaluation_results = TrainAndPredict(data_preparation, cell_type, train_needed=True)\n",
    "            test_losses.append(evaluation_results.test_loss)\n",
    "            classification_accuracies.append(evaluation_results.classification_accuracy)   \n",
    "            durations.append(evaluation_results.duration)\n",
    "    \n",
    "    df['Input Size'] = input_size_combination_values\n",
    "    df['Number of Steps'] = num_steps_combination_values\n",
    "    df['Test Loss'] = test_losses\n",
    "    df['Classification Accuracy'] = classification_accuracies\n",
    "    df['Duration'] = durations\n",
    "    \n",
    "    df.to_csv(f'results/{cell_type}_evaluation_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf0929d-6959-47e3-aea3-ef307d20779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(cell_type, input_size_values, num_steps_values, SEARCH_NEEDED=False):\n",
    "    # Perform grid search if needed (setting SEARCH_NEEDED = TRUE will RUN LONG TIME!!!)\n",
    "    if SEARCH_NEEDED:\n",
    "        grid_search(cell_type, input_size_values, num_steps_values)\n",
    "    \n",
    "    # Get best model\n",
    "    df = pd.read_csv(f'results/{cell_type}_evaluation_results.csv')\n",
    "    idx = np.argmin(df['Test Loss'])\n",
    "    df_best_model = df.iloc[idx, :]\n",
    "    return df_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec64117-eef8-4ec1-93fa-08aab4760824",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size_values = [2, 3, 5, 10, 20]\n",
    "num_steps_values = [3, 10, 20, 30, 40, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a622b-c8f5-479c-b921-5692cb4682a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM (setting SEARCH_NEEDED = TRUE will RUN LONG TIME!!!)\n",
    "cell_type = 'lstm'\n",
    "SEARCH_NEEDED_LSTM = False\n",
    "df_best_model = get_best_model(cell_type, input_size_values, num_steps_values, SEARCH_NEEDED=SEARCH_NEEDED_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0518331-2739-4344-bf18-d35ee9eb49db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb045fd-b9b2-480d-8dc2-ab0109f024e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the input_size and num_steps attributes\n",
    "data_preparation = DataPreparation(dataset_name, num_steps=int(df_best_model['Number of Steps']), input_size=int(df_best_model['Input Size']))\n",
    "\n",
    "# Train & Evaluate model\n",
    "evaluation_results = TrainAndPredict(data_preparation, cell_type, train_needed=True)\n",
    "print(\"Test loss of best model:\", evaluation_results.test_loss)\n",
    "print(\"Classification accuracy of best model:\", evaluation_results.classification_accuracy)\n",
    "print(\"Duration of best model:\", evaluation_results.duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29927b0-71bb-4c9d-acae-f7d06ccd2a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU (setting SEARCH_NEEDED = TRUE will RUN LONG TIME!!!)\n",
    "cell_type = 'gru'\n",
    "SEARCH_NEEDED_GRU = True\n",
    "df_best_model = get_best_model(cell_type, input_size_values, num_steps_values, SEARCH_NEEDED=SEARCH_NEEDED_GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80a94f7-7f3b-4881-9148-c6b4e8d410ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e47baf4-4392-4c4e-9ccf-83ab2ccaecf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b10df20-68dc-4170-9ba0-0ad0c4d3990c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05432447-658e-4b8d-9fa4-c0cc10612f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7254237d-ce87-489c-9d2a-3d4ef17e7d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4600a91-ae44-4e47-9b38-9439bb11702f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de33b4a4-9a04-4027-8815-4711ad8037d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232d58c2-0fc3-44b0-8311-56220c75d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.special import kl_div\n",
    "\n",
    "# # Sample data for demonstration purposes\n",
    "# # In practice, you would use actual activation data from the model\n",
    "# def get_fp32_activation_data():\n",
    "#     # Replace this with actual data collection\n",
    "#     return np.array([0.5, 1.0, 2.0, -1.0, -0.5, 3.0, -3.0, 0.1])\n",
    "\n",
    "# # **1. Calibration: Collect Data and Compute Scale Factor**\n",
    "\n",
    "# # Collect activation data (replace with actual FP32 data)\n",
    "# activation_data_fp32 = get_fp32_activation_data()\n",
    "\n",
    "# # Step 1: Determine the maximum absolute value\n",
    "# max_abs_value = np.max(np.abs(activation_data_fp32))\n",
    "\n",
    "# # Step 2: Compute the scale factor for symmetric quantization\n",
    "# def compute_scale(max_abs_value):\n",
    "#     return max_abs_value / 127  # 127 because 8-bit quantization uses values from -128 to 127\n",
    "\n",
    "# scale = compute_scale(max_abs_value)\n",
    "\n",
    "# # Step 3: Perform preliminary quantization of FP32 data\n",
    "# def preliminary_quantize(fp32_data, scale):\n",
    "#     return np.clip(np.round(fp32_data / scale), -128, 127).astype(np.int8)\n",
    "\n",
    "# activation_data_int8 = preliminary_quantize(activation_data_fp32, scale)\n",
    "\n",
    "# # Step 4: Compute histograms for FP32 and INT8\n",
    "# def compute_histograms(fp32_data, int8_data):\n",
    "#     hist_fp32, _ = np.histogram(fp32_data, bins=2048, range=(-128, 127), density=True)\n",
    "#     hist_int8, _ = np.histogram(int8_data, bins=2048, range=(-128, 127), density=True)\n",
    "#     return hist_fp32, hist_int8\n",
    "\n",
    "# hist_fp32, hist_int8 = compute_histograms(activation_data_fp32, activation_data_int8)\n",
    "\n",
    "# # Step 5: Compute KL divergence\n",
    "# def compute_kl_divergence(hist_fp32, hist_int8):\n",
    "#     # Adding a small constant to avoid log(0)\n",
    "#     return np.sum(kl_div(hist_fp32 + 1e-8, hist_int8 + 1e-8))\n",
    "\n",
    "# kl_divergence = compute_kl_divergence(hist_fp32, hist_int8)\n",
    "# print(f\"KL Divergence: {kl_divergence}\")\n",
    "\n",
    "# # **2. Quantization: Convert FP32 to INT8**\n",
    "\n",
    "# # Final quantization of FP32 data\n",
    "# def quantize_to_int8(fp32_data, scale):\n",
    "#     return np.clip(np.round(fp32_data / scale), -128, 127).astype(np.int8)\n",
    "\n",
    "# activation_data_int8_final = quantize_to_int8(activation_data_fp32, scale)\n",
    "\n",
    "# # **3. INT32 Computations: Perform Layer Operations**\n",
    "\n",
    "# # Example INT32 computation function\n",
    "# def int32_computations(weights, activations, bias):\n",
    "#     # Perform INT32 matrix multiplication and add bias\n",
    "#     int32_result = np.dot(weights, activations) + bias\n",
    "#     return int32_result\n",
    "\n",
    "# # Sample weights and bias for demonstration\n",
    "# weights = np.array([[1, -1], [2, 3]])\n",
    "# bias = np.array([1, -1])\n",
    "\n",
    "# # Perform INT32 computations\n",
    "# int32_result = int32_computations(weights, activation_data_int8_final, bias)\n",
    "# print(f\"INT32 Computation Result: {int32_result}\")\n",
    "\n",
    "# # **4. Re-Quantization: Convert INT32 to INT8**\n",
    "\n",
    "# # Re-quantization process\n",
    "# def requantize(int32_activations, scale, zero_point, bias):\n",
    "#     # Add bias and then requantize\n",
    "#     int32_activations_with_bias = int32_activations + bias\n",
    "#     return np.clip(np.round(int32_activations_with_bias * scale) + zero_point, -128, 127).astype(np.int8)\n",
    "\n",
    "# # Assuming zero_point = 0 for symmetric quantization\n",
    "# zero_point = 0\n",
    "\n",
    "# # Re-quantize INT32 results to INT8\n",
    "# activation_data_int8_requantized = requantize(int32_result, scale, zero_point, bias)\n",
    "# print(f\"Re-Quantized INT8 Data: {activation_data_int8_requantized}\")\n",
    "\n",
    "# # **5. De-Quantization: Convert INT8 Back to FP32**\n",
    "\n",
    "# # De-quantization process\n",
    "# def dequantize_to_fp32(int8_data, scale, zero_point):\n",
    "#     return (int8_data - zero_point) * scale\n",
    "\n",
    "# # Convert INT8 results back to FP32\n",
    "# fp32_reconstructed_data = dequantize_to_fp32(activation_data_int8_requantized, scale, zero_point)\n",
    "# print(f\"De-Quantized FP32 Data: {fp32_reconstructed_data}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
